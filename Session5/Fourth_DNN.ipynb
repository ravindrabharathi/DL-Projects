{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Normalization_and_Regularization.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ravindrabharathi/Project1/blob/master/Session5/Fourth_DNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBzDI9HEsAmu",
        "colab_type": "text"
      },
      "source": [
        "# Build a Convolutional Neural Network with less than 15000 parameters to achieve a validation accuracy of 99.4 or more for MNIST dataset \n",
        "\n",
        "\n",
        "The target is to build a deep learning CNN model with as little parameters as possible and at the same time achieve a high validation accuracy of 99.4 or more . The low parameter count becomes important when deploying the model in memory constrained devices used in edge computing . MNIST is one of the more popular (and simpler) datasets to begin your journey in Vision based Deep learning. We will use this MNIST dataset for this exercise. \n",
        "\n",
        "We will build the model step by step . Broadly speaking we will follow these steps \n",
        "\n",
        "1. Decide on the basic architecture for the network \n",
        "2. Fine tune parameters to comply with the 15000 limit\n",
        "3. Add improvements to the network using Batch Normalization \n",
        "4. See if we can converge faster while learning by using Dropouts to overcome overfitting ,higher   learning rates, etc \n",
        "\n",
        "We are now at step 4 .\n",
        "\n",
        "In the [first iteration](https://github.com/ravindrabharathi/Project1/blob/master/Session4/First_DNN.ipynb)  we fixed the basic architecture without bothering too much about the number of parameters . \n",
        "\n",
        "In the [second iteration](https://colab.research.google.com/github/ravindrabharathi/Project1/blob/master/Session4/Second_DNN.ipynb) we brought down the number of parameters to within the required limit to around 11k .\n",
        "\n",
        "In the [third iteration](https://github.com/ravindrabharathi/Project1/blob/master/Session4/Third_DNN.ipynb) we added BatchNormalization and brought the validation accuracy to 99.41 \n",
        "\n",
        "In this iteration we will tune the performance of the model by adding regularization via Dropouts.\n",
        "\n",
        "Basically Dropout helps in regularizing a network against overrfitting by randomly dropping a proportion of the signals while training. The absence of some of the units force the rest of the active units to learn better . The concept of Dropout was first present in this paper titled [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](http://jmlr.org/papers/v15/srivastava14a.html). \n",
        "\n",
        "![dropouts](https://raw.githubusercontent.com/ravindrabharathi/eip3/master/images/dropouts.png)\n",
        "\n",
        "We will drop about 10% of the signals after each convolution layer (ecluding the transition blocks and the last layer) .\n",
        "\n",
        "Since Dropouts are being used , we could also try higher learning rates . We will start with a higher learning rate than the default of 0.001 for Adam and then use a learning rate scheduler to reduce the learning rate progressively \n",
        "\n",
        "We will also try an increase in Batch size (128 from 32 ) since it would be better for the model to see a larger sampling of images per training step "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0uvqzIGp5zc",
        "colab_type": "text"
      },
      "source": [
        "###Import necessary libraries / modules\n",
        "Import numpy library for array/ matrix operations\n",
        "\n",
        "Import Sequential Model from keras/models for building the model\n",
        "\n",
        "Import Conv2D , Activation , Flatten , BatchNormalization, MaxPooling2D from keras/layers \n",
        "\n",
        "Import np_utils module from keras/utils for numpy related helper functions\n",
        "\n",
        "Import mnist dataset containing hand-written digits images from keras.datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eso6UHE080D4",
        "colab_type": "code",
        "outputId": "02a54876-29b0-4a5c-9734-e102bd42911b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential,load_model\n",
        "from keras.layers import Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Dropout\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zByEi95J86RD",
        "colab_type": "text"
      },
      "source": [
        "### Load pre-shuffled MNIST data into train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eRM0QWN83PV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db3hDvFDqpS9",
        "colab_type": "text"
      },
      "source": [
        "###print the shape of training data and also inspect the first image using matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a4Be72j8-ZC",
        "colab_type": "code",
        "outputId": "40373117-7572-4544-82ca-ce789191d31a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "source": [
        "print (X_train.shape)\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.imshow(X_train[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7ff0bd58c0f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7BHSzLjq6nT",
        "colab_type": "text"
      },
      "source": [
        "####Reshape the training and test dataset to include the channel information.In this case it is a greyscale image and so there is 1 channel . the image data was read in as a 28x28 numpy array and is now reshaped to 28x28x1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkmprriw9AnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3aPHupgrF5a",
        "colab_type": "text"
      },
      "source": [
        "###Cast training data as float32 and normalize/re-scale the values such that they are between 0 and 1 instead of 0 and 255"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2m4YS4E9CRh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmUE1nCXrMua",
        "colab_type": "text"
      },
      "source": [
        "###inspect the first 10 training class labels . They will be some number between 0 and 9 representing the hand-written digit in the corresponding Training data. Each of 0 to 9 represents a class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Mn0vAYD9DvB",
        "colab_type": "code",
        "outputId": "b97c8cfc-9150-4223-98c6-50d4a31cf031",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_train[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dld_th_rU9s",
        "colab_type": "text"
      },
      "source": [
        "####One hot encoding of training and test class labels : Convert 1-dimensional class arrays to 10-dimensional class matrices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG8JiXR39FHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYlFRvKS9HMB",
        "colab_type": "code",
        "outputId": "6540d423-95a2-46b8-a7a8-76c3e4e063f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "Y_train[:10]\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNM1NgZdtQm8",
        "colab_type": "text"
      },
      "source": [
        "###Define a ModelCheckPoint callback which will be called at the end of every training epoch . We will use this callback function to save the model whenever vallidation accuracy improves . We do this so that we can load and use the best model for further predictions after training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Yr6tsrzcSce",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "  \n",
        "#chkpoint_model=ModelCheckpoint(\"/gdrive/My Drive/EVA/Session3/model_customv1_mnist_best.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='max') \n",
        "\n",
        "chkpoint_model=ModelCheckpoint(\"model_custom_v1_mnist_best.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='max') \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmyGmS8oy2qE",
        "colab_type": "text"
      },
      "source": [
        "#Building the model version 4\n",
        "\n",
        "In version 4 of the model we will try and improve the performance of the model by adding Dropout at the end of each convolution block . We will use a dropout of 10%.\n",
        "\n",
        "We will also use an increased initial learning rate that is progressively reduced using a scheduler \n",
        "\n",
        "We will increase training batch size to 128\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rdn9MvWmVOtA",
        "colab_type": "code",
        "outputId": "b552f106-de35-4637-817a-05975afc42a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# to get a certain degeree of predictability when generating random numbers,\n",
        "# set a random seed to initialize the pseudo-random number generator \n",
        "np.random.seed(seed=42)  \n",
        "\n",
        "# instantiate a sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# add the first convolution layer - 10 numbers of 3x3 filters , \n",
        "#This layer sees the input image of 28x28 x 1 channel . \n",
        "\n",
        "model.add(Conv2D(10, (3, 3), input_shape=(28,28,1), use_bias=False))  # remove bias param by setting it to false \n",
        "model.add(BatchNormalization())  #add Batch normalization  \n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "model.add(Dropout(0.1))  #add dropout\n",
        "\n",
        "# Now the global receptive field is 3 x 3 \n",
        "\n",
        "# First Convolution Block\n",
        "# Block1 conv layer 1 - 12 filters of shape  3x3x10 \n",
        "# input from previous layer is 26 x 26 x 10 . \n",
        "\n",
        "## Block 1\n",
        "\n",
        "model.add(Conv2D(12, 3, use_bias=False))  # remove bias param by setting it to false \n",
        "model.add(BatchNormalization())  #add Batch normalization \n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "model.add(Dropout(0.1))   #add dropout\n",
        "\n",
        "#Global receptive field is 5x5\n",
        "\n",
        "# Add convolution layer - 16 filters of shape  3x3x12 \n",
        "#input from previous layer is 24 x 24 x 12 . \n",
        "\n",
        "model.add(Conv2D(16, 3, use_bias=False))  # remove bias param by setting it to false \n",
        "model.add(BatchNormalization())  #add Batch normalization\n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "model.add(Dropout(0.1))    #add dropout\n",
        "\n",
        "#Global receptive field is 7x7\n",
        "\n",
        "##  Transition block \n",
        "\n",
        "# Perform 2x2 max pooling  . \n",
        "#Input from previous layer is 22 x 22 X 16 \n",
        "\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "\n",
        "# After max pooling , 2D spatial dimension reduces by half , i.e it becomes 11 x 11 . \n",
        "# Maxpooling (withpool size 2 and stride 1) doubles receptive field . \n",
        "# So global receptive field after max pooling is 14 x 14\n",
        "\n",
        "# add 1x1 convolution to reduce the channel numbers to 10 \n",
        "\n",
        "model.add(Conv2D(10, 1, use_bias=False))  # remove bias param by setting it to false \n",
        "model.add(BatchNormalization())  #add Batch normalization \n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "\n",
        "\n",
        "#Convolution Block 2\n",
        "\n",
        "# Add convolution layer - 12 filters of shape 3x3x10\n",
        "#input from transition layer is 11 x 11 x 10 .  \n",
        "\n",
        "model.add(Conv2D(12, 3,  use_bias=False))  # remove bias param by setting it to false \n",
        "model.add(BatchNormalization())  #add Batch normalization\n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "model.add(Dropout(0.1))   #add dropout\n",
        "\n",
        "#Global receptive field is now 16 x 16 \n",
        "\n",
        "# Add convolution layer - 16 filters of shape 3x3x12 \n",
        "#input coming from previous layer is 9 x 9 x 12 . \n",
        "\n",
        "model.add(Conv2D(16, 3,  use_bias=False)) # remove bias param by setting it to false \n",
        "model.add(BatchNormalization())  #add Batch normalization \n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "#  Global receptive field is now 18 x 18 \n",
        "\n",
        "\n",
        "# Add 1x1 convolution to reduce number of channels to 10  \n",
        "#input coming from previous layer is 7 x 7 x 16 .\n",
        "\n",
        "model.add(Conv2D(10, 1,  use_bias=False))  # remove bias param by setting it to false \n",
        "model.add(BatchNormalization())  #add Batch normalization \n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "\n",
        "\n",
        "#Global receptive field is now 18 x 18 \n",
        "\n",
        "# Last layer :  Add convolution layer - 10 filters of shape 7x7x10 \n",
        "#input coming from previous layer is 7 x 7 x 10 .\n",
        "\n",
        "model.add(Conv2D(10, 7,  use_bias=False))  # remove bias param by setting it to false \n",
        "model.add(BatchNormalization())  #add Batch normalization\n",
        "# Note absence of ReLU activation here \n",
        "\n",
        "\n",
        "model.add(Flatten())  # Flatten the 2d array to 1d input for the softmax activation \n",
        "model.add(Activation('softmax'))   # Softmax activation to out class probabilities "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTdVH726xhN3",
        "colab_type": "code",
        "outputId": "b9c74023-bd45-4d34-d53f-f97d263e58dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1190
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 26, 26, 10)        90        \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 26, 26, 10)        40        \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 26, 26, 10)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 26, 26, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 24, 24, 12)        1080      \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 24, 24, 12)        48        \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 24, 24, 12)        0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 24, 24, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 22, 22, 16)        1728      \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 22, 22, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 22, 22, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 22, 22, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 11, 11, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 11, 11, 10)        160       \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 11, 11, 10)        40        \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 11, 11, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 9, 9, 12)          1080      \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 9, 9, 12)          48        \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 9, 9, 12)          0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 9, 9, 12)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 7, 7, 16)          1728      \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 7, 7, 16)          64        \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 7, 7, 16)          0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 7, 7, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 7, 7, 10)          160       \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 7, 7, 10)          40        \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 7, 7, 10)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 1, 1, 10)          4900      \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 1, 1, 10)          40        \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 11,310\n",
            "Trainable params: 11,118\n",
            "Non-trainable params: 192\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsmonadDDC5o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define a learning rate scheduler . We will use a simple scheduler that reduces the lr by 10% every 3 epochs subject to a minimum lr of 0.0005 \n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "def scheduler(epoch, lr):\n",
        "  if (epoch%3==0 and epoch):\n",
        "    new_lr = max(0.9*lr,0.0005) \n",
        "  else:\n",
        "    new_lr=lr\n",
        "  \n",
        "  return round(new_lr, 10)\n",
        "  \n",
        " \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7A0AhG_xrjw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#start with a higher lr of 0.003 \n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.003), metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqappyT1x7sf",
        "colab_type": "code",
        "outputId": "30af3a13-4d55-4e4e-b018-694b309fd81a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6242
        }
      },
      "source": [
        "#increase batch size to 128 and traun for 60 epochs \n",
        "model.fit(X_train, Y_train, validation_data=(X_test,Y_test),batch_size=128, epochs=60, verbose=1, callbacks=[chkpoint_model,LearningRateScheduler(scheduler, verbose=1)])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/60\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 9s 147us/step - loss: 0.3796 - acc: 0.9341 - val_loss: 0.1291 - val_acc: 0.9807\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.98070, saving model to model_custom_v1_mnist_best.h5\n",
            "Epoch 2/60\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 7s 112us/step - loss: 0.1190 - acc: 0.9780 - val_loss: 0.0830 - val_acc: 0.9858\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.98070 to 0.98580, saving model to model_custom_v1_mnist_best.h5\n",
            "Epoch 3/60\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 6s 103us/step - loss: 0.0781 - acc: 0.9832 - val_loss: 0.0394 - val_acc: 0.9902\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.98580 to 0.99020, saving model to model_custom_v1_mnist_best.h5\n",
            "Epoch 4/60\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0027.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0628 - acc: 0.9855 - val_loss: 0.0353 - val_acc: 0.9918\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.99020 to 0.99180, saving model to model_custom_v1_mnist_best.h5\n",
            "Epoch 5/60\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0027000001.\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0527 - acc: 0.9870 - val_loss: 0.0363 - val_acc: 0.9895\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.99180\n",
            "Epoch 6/60\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0027000001.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0479 - acc: 0.9878 - val_loss: 0.0344 - val_acc: 0.9905\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.99180\n",
            "Epoch 7/60\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0024300001.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0435 - acc: 0.9879 - val_loss: 0.0273 - val_acc: 0.9918\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.99180\n",
            "Epoch 8/60\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0024300001.\n",
            "60000/60000 [==============================] - 7s 112us/step - loss: 0.0412 - acc: 0.9885 - val_loss: 0.0254 - val_acc: 0.9931\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.99180 to 0.99310, saving model to model_custom_v1_mnist_best.h5\n",
            "Epoch 9/60\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0024300001.\n",
            "60000/60000 [==============================] - 7s 112us/step - loss: 0.0378 - acc: 0.9892 - val_loss: 0.0406 - val_acc: 0.9886\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.99310\n",
            "Epoch 10/60\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0021870001.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0352 - acc: 0.9903 - val_loss: 0.0235 - val_acc: 0.9929\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.99310\n",
            "Epoch 11/60\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0021870001.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0335 - acc: 0.9905 - val_loss: 0.0300 - val_acc: 0.9922\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.99310\n",
            "Epoch 12/60\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.0021870001.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0337 - acc: 0.9901 - val_loss: 0.0262 - val_acc: 0.9921\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.99310\n",
            "Epoch 13/60\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0019683001.\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0299 - acc: 0.9914 - val_loss: 0.0236 - val_acc: 0.9932\n",
            "\n",
            "Epoch 00013: val_acc improved from 0.99310 to 0.99320, saving model to model_custom_v1_mnist_best.h5\n",
            "Epoch 14/60\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0019683002.\n",
            "60000/60000 [==============================] - 7s 109us/step - loss: 0.0284 - acc: 0.9916 - val_loss: 0.0216 - val_acc: 0.9941\n",
            "\n",
            "Epoch 00014: val_acc improved from 0.99320 to 0.99410, saving model to model_custom_v1_mnist_best.h5\n",
            "Epoch 15/60\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0019683002.\n",
            "60000/60000 [==============================] - 7s 112us/step - loss: 0.0279 - acc: 0.9916 - val_loss: 0.0216 - val_acc: 0.9934\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.99410\n",
            "Epoch 16/60\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0017714702.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0272 - acc: 0.9920 - val_loss: 0.0219 - val_acc: 0.9936\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.99410\n",
            "Epoch 17/60\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.0017714702.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0248 - acc: 0.9927 - val_loss: 0.0269 - val_acc: 0.9921\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.99410\n",
            "Epoch 18/60\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0017714702.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0248 - acc: 0.9924 - val_loss: 0.0244 - val_acc: 0.9927\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.99410\n",
            "Epoch 19/60\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0015943232.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0249 - acc: 0.9927 - val_loss: 0.0201 - val_acc: 0.9939\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.99410\n",
            "Epoch 20/60\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.0015943232.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0226 - acc: 0.9929 - val_loss: 0.0226 - val_acc: 0.9932\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.99410\n",
            "Epoch 21/60\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.0015943232.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0235 - acc: 0.9931 - val_loss: 0.0196 - val_acc: 0.9934\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.99410\n",
            "Epoch 22/60\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 0.0014348909.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0229 - acc: 0.9930 - val_loss: 0.0214 - val_acc: 0.9928\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.99410\n",
            "Epoch 23/60\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.0014348909.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0220 - acc: 0.9935 - val_loss: 0.0195 - val_acc: 0.9942\n",
            "\n",
            "Epoch 00023: val_acc improved from 0.99410 to 0.99420, saving model to model_custom_v1_mnist_best.h5\n",
            "Epoch 24/60\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 0.0014348909.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0212 - acc: 0.9938 - val_loss: 0.0195 - val_acc: 0.9949\n",
            "\n",
            "Epoch 00024: val_acc improved from 0.99420 to 0.99490, saving model to model_custom_v1_mnist_best.h5\n",
            "Epoch 25/60\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 0.0012914018.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0193 - acc: 0.9943 - val_loss: 0.0195 - val_acc: 0.9936\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.99490\n",
            "Epoch 26/60\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 0.0012914018.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0198 - acc: 0.9944 - val_loss: 0.0194 - val_acc: 0.9946\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.99490\n",
            "Epoch 27/60\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 0.0012914018.\n",
            "60000/60000 [==============================] - 6s 108us/step - loss: 0.0190 - acc: 0.9945 - val_loss: 0.0215 - val_acc: 0.9932\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.99490\n",
            "Epoch 28/60\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 0.0011622616.\n",
            "60000/60000 [==============================] - 7s 112us/step - loss: 0.0186 - acc: 0.9945 - val_loss: 0.0184 - val_acc: 0.9947\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.99490\n",
            "Epoch 29/60\n",
            "\n",
            "Epoch 00029: LearningRateScheduler setting learning rate to 0.0011622616.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0177 - acc: 0.9947 - val_loss: 0.0212 - val_acc: 0.9932\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.99490\n",
            "Epoch 30/60\n",
            "\n",
            "Epoch 00030: LearningRateScheduler setting learning rate to 0.0011622616.\n",
            "60000/60000 [==============================] - 6s 103us/step - loss: 0.0186 - acc: 0.9945 - val_loss: 0.0198 - val_acc: 0.9942\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.99490\n",
            "Epoch 31/60\n",
            "\n",
            "Epoch 00031: LearningRateScheduler setting learning rate to 0.0010460354.\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0177 - acc: 0.9947 - val_loss: 0.0184 - val_acc: 0.9942\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.99490\n",
            "Epoch 32/60\n",
            "\n",
            "Epoch 00032: LearningRateScheduler setting learning rate to 0.0010460354.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0176 - acc: 0.9947 - val_loss: 0.0185 - val_acc: 0.9946\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.99490\n",
            "Epoch 33/60\n",
            "\n",
            "Epoch 00033: LearningRateScheduler setting learning rate to 0.0010460354.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0181 - acc: 0.9945 - val_loss: 0.0195 - val_acc: 0.9937\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.99490\n",
            "Epoch 34/60\n",
            "\n",
            "Epoch 00034: LearningRateScheduler setting learning rate to 0.0009414319.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0158 - acc: 0.9952 - val_loss: 0.0186 - val_acc: 0.9941\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.99490\n",
            "Epoch 35/60\n",
            "\n",
            "Epoch 00035: LearningRateScheduler setting learning rate to 0.0009414319.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0156 - acc: 0.9952 - val_loss: 0.0192 - val_acc: 0.9939\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.99490\n",
            "Epoch 36/60\n",
            "\n",
            "Epoch 00036: LearningRateScheduler setting learning rate to 0.0009414319.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0160 - acc: 0.9951 - val_loss: 0.0200 - val_acc: 0.9939\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.99490\n",
            "Epoch 37/60\n",
            "\n",
            "Epoch 00037: LearningRateScheduler setting learning rate to 0.0008472887.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0154 - acc: 0.9953 - val_loss: 0.0171 - val_acc: 0.9948\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.99490\n",
            "Epoch 38/60\n",
            "\n",
            "Epoch 00038: LearningRateScheduler setting learning rate to 0.0008472887.\n",
            "60000/60000 [==============================] - 6s 103us/step - loss: 0.0154 - acc: 0.9955 - val_loss: 0.0177 - val_acc: 0.9944\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.99490\n",
            "Epoch 39/60\n",
            "\n",
            "Epoch 00039: LearningRateScheduler setting learning rate to 0.0008472887.\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0145 - acc: 0.9956 - val_loss: 0.0194 - val_acc: 0.9937\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.99490\n",
            "Epoch 40/60\n",
            "\n",
            "Epoch 00040: LearningRateScheduler setting learning rate to 0.0007625598.\n",
            "60000/60000 [==============================] - 7s 109us/step - loss: 0.0156 - acc: 0.9949 - val_loss: 0.0200 - val_acc: 0.9939\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.99490\n",
            "Epoch 41/60\n",
            "\n",
            "Epoch 00041: LearningRateScheduler setting learning rate to 0.0007625598.\n",
            "60000/60000 [==============================] - 7s 112us/step - loss: 0.0146 - acc: 0.9956 - val_loss: 0.0194 - val_acc: 0.9941\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.99490\n",
            "Epoch 42/60\n",
            "\n",
            "Epoch 00042: LearningRateScheduler setting learning rate to 0.0007625598.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0145 - acc: 0.9956 - val_loss: 0.0193 - val_acc: 0.9947\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.99490\n",
            "Epoch 43/60\n",
            "\n",
            "Epoch 00043: LearningRateScheduler setting learning rate to 0.0006863038.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0133 - acc: 0.9961 - val_loss: 0.0188 - val_acc: 0.9949\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.99490\n",
            "Epoch 44/60\n",
            "\n",
            "Epoch 00044: LearningRateScheduler setting learning rate to 0.0006863038.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0141 - acc: 0.9957 - val_loss: 0.0192 - val_acc: 0.9947\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.99490\n",
            "Epoch 45/60\n",
            "\n",
            "Epoch 00045: LearningRateScheduler setting learning rate to 0.0006863038.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0137 - acc: 0.9958 - val_loss: 0.0182 - val_acc: 0.9947\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.99490\n",
            "Epoch 46/60\n",
            "\n",
            "Epoch 00046: LearningRateScheduler setting learning rate to 0.0006176734.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0135 - acc: 0.9959 - val_loss: 0.0184 - val_acc: 0.9942\n",
            "\n",
            "Epoch 00046: val_acc did not improve from 0.99490\n",
            "Epoch 47/60\n",
            "\n",
            "Epoch 00047: LearningRateScheduler setting learning rate to 0.0006176734.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0135 - acc: 0.9960 - val_loss: 0.0187 - val_acc: 0.9942\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.99490\n",
            "Epoch 48/60\n",
            "\n",
            "Epoch 00048: LearningRateScheduler setting learning rate to 0.0006176734.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0129 - acc: 0.9962 - val_loss: 0.0192 - val_acc: 0.9947\n",
            "\n",
            "Epoch 00048: val_acc did not improve from 0.99490\n",
            "Epoch 49/60\n",
            "\n",
            "Epoch 00049: LearningRateScheduler setting learning rate to 0.000555906.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0126 - acc: 0.9960 - val_loss: 0.0187 - val_acc: 0.9946\n",
            "\n",
            "Epoch 00049: val_acc did not improve from 0.99490\n",
            "Epoch 50/60\n",
            "\n",
            "Epoch 00050: LearningRateScheduler setting learning rate to 0.000555906.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0133 - acc: 0.9960 - val_loss: 0.0190 - val_acc: 0.9945\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.99490\n",
            "Epoch 51/60\n",
            "\n",
            "Epoch 00051: LearningRateScheduler setting learning rate to 0.000555906.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0127 - acc: 0.9961 - val_loss: 0.0178 - val_acc: 0.9943\n",
            "\n",
            "Epoch 00051: val_acc did not improve from 0.99490\n",
            "Epoch 52/60\n",
            "\n",
            "Epoch 00052: LearningRateScheduler setting learning rate to 0.0005003154.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0134 - acc: 0.9960 - val_loss: 0.0189 - val_acc: 0.9945\n",
            "\n",
            "Epoch 00052: val_acc did not improve from 0.99490\n",
            "Epoch 53/60\n",
            "\n",
            "Epoch 00053: LearningRateScheduler setting learning rate to 0.0005003154.\n",
            "60000/60000 [==============================] - 7s 109us/step - loss: 0.0121 - acc: 0.9966 - val_loss: 0.0184 - val_acc: 0.9947\n",
            "\n",
            "Epoch 00053: val_acc did not improve from 0.99490\n",
            "Epoch 54/60\n",
            "\n",
            "Epoch 00054: LearningRateScheduler setting learning rate to 0.0005003154.\n",
            "60000/60000 [==============================] - 7s 111us/step - loss: 0.0120 - acc: 0.9962 - val_loss: 0.0199 - val_acc: 0.9941\n",
            "\n",
            "Epoch 00054: val_acc did not improve from 0.99490\n",
            "Epoch 55/60\n",
            "\n",
            "Epoch 00055: LearningRateScheduler setting learning rate to 0.0005.\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0121 - acc: 0.9963 - val_loss: 0.0174 - val_acc: 0.9952\n",
            "\n",
            "Epoch 00055: val_acc improved from 0.99490 to 0.99520, saving model to model_custom_v1_mnist_best.h5\n",
            "Epoch 56/60\n",
            "\n",
            "Epoch 00056: LearningRateScheduler setting learning rate to 0.0005.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0122 - acc: 0.9962 - val_loss: 0.0190 - val_acc: 0.9942\n",
            "\n",
            "Epoch 00056: val_acc did not improve from 0.99520\n",
            "Epoch 57/60\n",
            "\n",
            "Epoch 00057: LearningRateScheduler setting learning rate to 0.0005.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0120 - acc: 0.9963 - val_loss: 0.0181 - val_acc: 0.9945\n",
            "\n",
            "Epoch 00057: val_acc did not improve from 0.99520\n",
            "Epoch 58/60\n",
            "\n",
            "Epoch 00058: LearningRateScheduler setting learning rate to 0.0005.\n",
            "60000/60000 [==============================] - 7s 114us/step - loss: 0.0124 - acc: 0.9962 - val_loss: 0.0187 - val_acc: 0.9947\n",
            "\n",
            "Epoch 00058: val_acc did not improve from 0.99520\n",
            "Epoch 59/60\n",
            "\n",
            "Epoch 00059: LearningRateScheduler setting learning rate to 0.0005.\n",
            "60000/60000 [==============================] - 7s 110us/step - loss: 0.0111 - acc: 0.9966 - val_loss: 0.0186 - val_acc: 0.9943\n",
            "\n",
            "Epoch 00059: val_acc did not improve from 0.99520\n",
            "Epoch 60/60\n",
            "\n",
            "Epoch 00060: LearningRateScheduler setting learning rate to 0.0005.\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0121 - acc: 0.9960 - val_loss: 0.0172 - val_acc: 0.9949\n",
            "\n",
            "Epoch 00060: val_acc did not improve from 0.99520\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff09c3746d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAo7lmRA0FIE",
        "colab_type": "text"
      },
      "source": [
        "###We trained the model for 60 epochs and it has a max validation accuracy of 99.52. \n",
        "\n",
        "###We also observe that the due to the dropouts the training accuracy was lower than the validation accuracy in the beginning before rising higher. This is because the the dropped units are only during training and will participate fully in the validation phase. \n",
        "\n",
        "###We also notice that the gap between training and validation accuracy is lesser due to the effect of dropouts \n",
        "\n",
        "### The required 99.4 validation accuracy was reached faster than the previous iteration - in epoch 14.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7fa3y3MszKQ",
        "colab_type": "text"
      },
      "source": [
        "### Let us load the Model with best validation accuracy and print the evaluation score "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvdXCXK2l9KQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model=load_model(\"model_custom_v1_mnist_best.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtsH-lLk-eLb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score = model.evaluate(X_test, Y_test, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkX8JMv79q9r",
        "colab_type": "code",
        "outputId": "511f5615-8517-4661-d9ac-a5eea1a0d73b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(score)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.01742733436343842, 0.9952]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Acfgy31CtWYc",
        "colab_type": "text"
      },
      "source": [
        "### Predict the classes using model.predict and print predicted probabilities and categorical array for True test classes "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCWoJkwE9suh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym7iCFBm9uBs",
        "colab_type": "code",
        "outputId": "92d62370-d8bf-4f4b-aeac-597ca70b0532",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        }
      },
      "source": [
        "print(y_pred[:9])\n",
        "print(Y_test[:9])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.57381589e-06 1.39165422e-05 9.62392164e-07 2.11704969e-06\n",
            "  2.87080047e-06 9.83216296e-07 1.04582654e-07 9.99976993e-01\n",
            "  7.96954893e-08 3.55278502e-07]\n",
            " [2.93556468e-06 1.17223692e-06 9.99987125e-01 2.98503522e-07\n",
            "  4.99096586e-07 1.09032838e-09 7.16115392e-06 2.45385678e-07\n",
            "  5.51291180e-07 2.63949342e-08]\n",
            " [2.73492390e-07 9.99898791e-01 1.28019352e-07 1.34703976e-06\n",
            "  2.62308026e-06 8.33929371e-05 5.30659645e-06 3.39038070e-06\n",
            "  3.75394393e-06 1.05119875e-06]\n",
            " [9.99887228e-01 1.14009947e-06 6.83825817e-07 1.45662069e-08\n",
            "  5.48336686e-07 1.84093807e-07 7.84729782e-05 7.45014540e-06\n",
            "  9.23682251e-07 2.32167768e-05]\n",
            " [1.52993948e-07 1.25076594e-05 3.82066929e-07 4.78601862e-07\n",
            "  9.99984622e-01 6.68860256e-09 4.43562271e-07 2.04720720e-07\n",
            "  7.10572237e-07 5.65000903e-07]\n",
            " [4.52342675e-07 9.99972224e-01 1.11369343e-06 1.55494590e-07\n",
            "  7.03976593e-06 4.42121109e-06 1.22046720e-06 1.23034270e-05\n",
            "  4.50615715e-07 5.30906448e-07]\n",
            " [3.28050892e-10 1.26155066e-06 1.16197718e-07 4.88288610e-09\n",
            "  9.99990582e-01 5.24228341e-08 1.10688674e-07 9.58880708e-09\n",
            "  7.78294634e-06 1.26507402e-07]\n",
            " [4.43877752e-06 7.33248862e-06 9.35117932e-05 1.07101221e-06\n",
            "  3.98312432e-06 5.81478503e-07 2.87370483e-07 1.87681712e-06\n",
            "  1.96003130e-05 9.99867320e-01]\n",
            " [3.53122090e-07 4.54398423e-06 8.89874264e-06 2.90394695e-07\n",
            "  2.33187457e-05 9.92252052e-01 6.22177869e-03 2.98120758e-05\n",
            "  1.45610736e-03 2.79271057e-06]]\n",
            "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYPrkzLvt2do",
        "colab_type": "text"
      },
      "source": [
        "### Let us visualize some of the filters in the first convolution layer 'conv2d_1'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CT--y98_dr2T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# form a layer dictionary {name : layer} of all layers in the model \n",
        "\n",
        "layer_dict = dict([(layer.name, layer) for layer in model.layers])  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GY4Upv4dsUR",
        "colab_type": "code",
        "outputId": "93fcf4d9-3d09-4466-9e03-1cc68b8daea4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 771
        }
      },
      "source": [
        " # use matplotlib to visualize the filter arrays \n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from keras import backend as K\n",
        "%matplotlib inline\n",
        "# util function to convert a tensor into a valid image\n",
        "def deprocess_image(x):\n",
        "    # normalize tensor: center on 0., ensure std is 0.1\n",
        "    x -= x.mean()\n",
        "    x /= (x.std() + 1e-5)\n",
        "    x *= 0.1\n",
        "\n",
        "    # clip to [0, 1]\n",
        "    x += 0.5\n",
        "    x = np.clip(x, 0, 1)\n",
        "\n",
        "    # convert to RGB array\n",
        "    x *= 255\n",
        "    #x = x.transpose((1, 2, 0))\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x\n",
        "\n",
        "def vis_img_in_filter(img = np.array(X_train[2]).reshape((1, 28, 28, 1)).astype(np.float64), \n",
        "                      layer_name = 'conv2d_1'):\n",
        "    layer_output = layer_dict[layer_name].output\n",
        "    img_ascs = list()\n",
        "    for filter_index in range(layer_output.shape[3]):\n",
        "        # build a loss function that maximizes the activation\n",
        "        # of the nth filter of the layer considered\n",
        "        loss = K.mean(layer_output[:, :, :, filter_index])\n",
        "\n",
        "        # compute the gradient of the input picture wrt this loss\n",
        "        grads = K.gradients(loss, model.input)[0]\n",
        "\n",
        "        # normalization trick: we normalize the gradient\n",
        "        grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)\n",
        "\n",
        "        # this function returns the loss and grads given the input picture\n",
        "        iterate = K.function([model.input], [loss, grads])\n",
        "\n",
        "        # step size for gradient ascent\n",
        "        step = 5.\n",
        "\n",
        "        img_asc = np.array(img)\n",
        "        # run gradient ascent for 20 steps\n",
        "        for i in range(20):\n",
        "            loss_value, grads_value = iterate([img_asc])\n",
        "            img_asc += grads_value * step\n",
        "\n",
        "        img_asc = img_asc[0]\n",
        "        img_ascs.append(deprocess_image(img_asc).reshape((28, 28)))\n",
        "        \n",
        "    if layer_output.shape[3] >= 35:\n",
        "        plot_x, plot_y = 6, 6\n",
        "    elif layer_output.shape[3] >= 23:\n",
        "        plot_x, plot_y = 4, 6\n",
        "    elif layer_output.shape[3] >= 11:\n",
        "        plot_x, plot_y = 2, 6\n",
        "    elif layer_output.shape[3] >= 8:\n",
        "        plot_x, plot_y = 2, 4   \n",
        "    else:\n",
        "        \n",
        "        plot_x, plot_y = 2, 2\n",
        "    fig, ax = plt.subplots(plot_x, plot_y, figsize = (12, 12))\n",
        "    \n",
        "    ax[0,0].imshow(img.reshape((28, 28)), cmap = 'gray')\n",
        "    ax[0,0].set_title('Input image')\n",
        "    fig.suptitle('Input image and %s filters' % (layer_name,))\n",
        "    fig.tight_layout(pad = 0.3, rect = [0, 0, 0.9, 0.9])\n",
        "    for (x, y) in [(i, j) for i in range(plot_x) for j in range(plot_y)]:\n",
        "        if x == 0 and y == 0:\n",
        "            continue\n",
        "        ax[x,y].imshow(img_ascs[x * plot_y + y - 1], cmap = 'gray')\n",
        "        ax[x,y].set_title('filter %d' % (x * plot_y + y - 1))\n",
        "\n",
        "vis_img_in_filter()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwcAAALyCAYAAACPcKhRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npfmhXzYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 864x864 with 8 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2q6fJ67qvlsY",
        "colab_type": "text"
      },
      "source": [
        "### We have now completed the fourth step in building our model and we have reached a max validation accuracy of 99.52 in 60 epochs \n",
        "\n",
        "###We used Dropouts , a very simplified learning rate scheduler and a larger batch size in this iteration to improve the model and croosed the 99.4 validation accuracy in the 14th epoch. \n",
        "\n",
        "###We could improve this model further by using a cleaner and more efficient way of sending the input to the softmax layer by using Global Average Pooling . We did not use SGD optimizer as it might have converged slower than Adam optimizer.  A better learning rate scheduler which took into account whether accuracy was increasing before changing the learning rate would have helped more .\n",
        "\n"
      ]
    }
  ]
}
