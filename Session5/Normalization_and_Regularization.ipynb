{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Normalization_and_Regularization.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ravindrabharathi/Project1/blob/master/Session5/Normalization_and_Regularization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBzDI9HEsAmu",
        "colab_type": "text"
      },
      "source": [
        "# Image Normalization and Regularization in Convolution Neural Networks \n",
        "\n",
        "\n",
        "We are now building on top of the our last exercise where we built a CNN with less than 15K parameters in 4 steps ([Step 1](https://github.com/ravindrabharathi/Project1/blob/master/Session4/First_DNN.ipynb) , [Step 2](https://github.com/ravindrabharathi/Project1/blob/master/Session4/Second_DNN.ipynb) , [Step 3](https://github.com/ravindrabharathi/Project1/blob/master/Session4/Third_DNN.ipynb) , [Step 4](https://github.com/ravindrabharathi/Project1/blob/master/Session4/Fourth_DNN.ipynb) and reached an accuracy of 99.52 in about 60 epochs .\n",
        "\n",
        "We will use the model from our 4th iteration as the starting point and add the following \n",
        "\n",
        "1. Image Normalization \n",
        "2. L2 Regularization to the Cost function (applied to kernel weights )\n",
        "3. Additional experiment of what happens when Batch Normalization is added before and after the ReLU activation \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0uvqzIGp5zc",
        "colab_type": "text"
      },
      "source": [
        "###Import necessary libraries / modules\n",
        "Import numpy library for array/ matrix operations\n",
        "\n",
        "Import Sequential Model from keras/models for building the model\n",
        "\n",
        "Import Conv2D , Activation , Flatten , BatchNormalization, MaxPooling2D from keras/layers \n",
        "\n",
        "Import np_utils module from keras/utils for numpy related helper functions\n",
        "\n",
        "Import mnist dataset containing hand-written digits images from keras.datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eso6UHE080D4",
        "colab_type": "code",
        "outputId": "a2b573f8-87df-4dfe-b381-f68542fbca94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential,load_model\n",
        "from keras.layers import Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Dropout\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zByEi95J86RD",
        "colab_type": "text"
      },
      "source": [
        "### Load pre-shuffled MNIST data into train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eRM0QWN83PV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b123cd2d-885c-4158-be13-25c0b33f1a26"
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db3hDvFDqpS9",
        "colab_type": "text"
      },
      "source": [
        "###print the shape of training data and also inspect the first image using matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a4Be72j8-ZC",
        "colab_type": "code",
        "outputId": "4d7847cf-cf1a-4ed1-e3c5-98899fd24c26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "source": [
        "print (X_train.shape)\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.imshow(X_train[0])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f82e6633198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7BHSzLjq6nT",
        "colab_type": "text"
      },
      "source": [
        "####Reshape the training and test dataset to include the channel information.In this case it is a greyscale image and so there is 1 channel . the image data was read in as a 28x28 numpy array and is now reshaped to 28x28x1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkmprriw9AnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3aPHupgrF5a",
        "colab_type": "text"
      },
      "source": [
        "###Cast training data as float32 and normalize/re-scale the values such that they are between 0 and 1 instead of 0 and 255"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2m4YS4E9CRh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmUE1nCXrMua",
        "colab_type": "text"
      },
      "source": [
        "###inspect the first 10 training class labels . They will be some number between 0 and 9 representing the hand-written digit in the corresponding Training data. Each of 0 to 9 represents a class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Mn0vAYD9DvB",
        "colab_type": "code",
        "outputId": "1887ed52-f349-4489-afce-c3ebbed1a496",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_train[:10]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dld_th_rU9s",
        "colab_type": "text"
      },
      "source": [
        "####One hot encoding of training and test class labels : Convert 1-dimensional class arrays to 10-dimensional class matrices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG8JiXR39FHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYlFRvKS9HMB",
        "colab_type": "code",
        "outputId": "d82bce82-ddf8-4d4b-c490-59706306f72e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "Y_train[:10]\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTtRAeaWTjnQ",
        "colab_type": "text"
      },
      "source": [
        "####Model with BatchNormalization after ReLU "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "1ede4e32-54ba-4017-fa1d-44860b0ae182",
        "id": "dNf11H-ctcFw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# to get a certain degeree of predictability when generating random numbers,\n",
        "# set a random seed to initialize the pseudo-random number generator \n",
        "np.random.seed(seed=42)  \n",
        "\n",
        "# instantiate a sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# add the first convolution layer - 10 numbers of 3x3 filters , \n",
        "#This layer sees the input image of 28x28 x 1 channel . \n",
        "\n",
        "model.add(Conv2D(10, (3, 3), input_shape=(28,28,1), use_bias=False))  # remove bias param by setting it to false \n",
        " \n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "model.add(BatchNormalization())  #add Batch normalization \n",
        "model.add(Dropout(0.1))  #add dropout\n",
        "\n",
        "# Now the global receptive field is 3 x 3 \n",
        "\n",
        "# First Convolution Block\n",
        "# Block1 conv layer 1 - 12 filters of shape  3x3x10 \n",
        "# input from previous layer is 26 x 26 x 10 . \n",
        "\n",
        "## Block 1\n",
        "\n",
        "model.add(Conv2D(12, 3, use_bias=False))  # remove bias param by setting it to false \n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "model.add(BatchNormalization())  #add Batch normalization \n",
        "model.add(Dropout(0.1))   #add dropout\n",
        "\n",
        "#Global receptive field is 5x5\n",
        "\n",
        "# Add convolution layer - 16 filters of shape  3x3x12 \n",
        "#input from previous layer is 24 x 24 x 12 . \n",
        "\n",
        "model.add(Conv2D(16, 3, use_bias=False))  # remove bias param by setting it to false \n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "model.add(BatchNormalization())  #add Batch normalization \n",
        "model.add(Dropout(0.1))    #add dropout\n",
        "\n",
        "#Global receptive field is 7x7\n",
        "\n",
        "##  Transition block \n",
        "\n",
        "# Perform 2x2 max pooling  . \n",
        "#Input from previous layer is 22 x 22 X 16 \n",
        "\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "\n",
        "# After max pooling , 2D spatial dimension reduces by half , i.e it becomes 11 x 11 . \n",
        "# Maxpooling (withpool size 2 and stride 1) doubles receptive field . \n",
        "# So global receptive field after max pooling is 14 x 14\n",
        "\n",
        "# add 1x1 convolution to reduce the channel numbers to 10 \n",
        "\n",
        "model.add(Conv2D(10, 1, use_bias=False))  # remove bias param by setting it to false \n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "model.add(BatchNormalization())  #add Batch normalization \n",
        "\n",
        "\n",
        "#Convolution Block 2\n",
        "\n",
        "# Add convolution layer - 12 filters of shape 3x3x10\n",
        "#input from transition layer is 11 x 11 x 10 .  \n",
        "\n",
        "model.add(Conv2D(12, 3,  use_bias=False))  # remove bias param by setting it to false \n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "model.add(BatchNormalization())  #add Batch normalization \n",
        "model.add(Dropout(0.1))   #add dropout\n",
        "\n",
        "#Global receptive field is now 16 x 16 \n",
        "\n",
        "# Add convolution layer - 16 filters of shape 3x3x12 \n",
        "#input coming from previous layer is 9 x 9 x 12 . \n",
        "\n",
        "model.add(Conv2D(16, 3,  use_bias=False)) # remove bias param by setting it to false \n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "model.add(BatchNormalization())  #add Batch normalization \n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "#  Global receptive field is now 18 x 18 \n",
        "\n",
        "\n",
        "# Add 1x1 convolution to reduce number of channels to 10  \n",
        "#input coming from previous layer is 7 x 7 x 16 .\n",
        "\n",
        "model.add(Conv2D(10, 1,  use_bias=False))  # remove bias param by setting it to false \n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "model.add(BatchNormalization())  #add Batch normalization \n",
        "\n",
        "\n",
        "#Global receptive field is now 18 x 18 \n",
        "\n",
        "# Last layer :  Add convolution layer - 10 filters of shape 7x7x10 \n",
        "#input coming from previous layer is 7 x 7 x 10 .\n",
        "\n",
        "model.add(Conv2D(10, 7,  use_bias=False))  # remove bias param by setting it to false \n",
        "#No Batch normalization before prediction \n",
        "# Note absence of ReLU activation here \n",
        "\n",
        "\n",
        "model.add(Flatten())  # Flatten the 2d array to 1d input for the softmax activation \n",
        "model.add(Activation('softmax'))   # Softmax activation to out class probabilities "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ba5ad8bc-180e-4c3c-8085-7e155cf3a396",
        "id": "zLeXZp_jtcFz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1156
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 26, 26, 10)        90        \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 26, 26, 10)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 26, 26, 10)        40        \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 26, 26, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 24, 24, 12)        1080      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 24, 24, 12)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 24, 24, 12)        48        \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 24, 24, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 22, 22, 16)        1728      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 22, 22, 16)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 22, 22, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 22, 22, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 11, 11, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 11, 11, 10)        160       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 11, 11, 10)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 11, 11, 10)        40        \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 9, 9, 12)          1080      \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 9, 9, 12)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 9, 9, 12)          48        \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 9, 9, 12)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 7, 7, 16)          1728      \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 7, 7, 16)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 7, 7, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 7, 7, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 7, 7, 10)          160       \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 7, 7, 10)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 7, 7, 10)          40        \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 1, 1, 10)          4900      \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 11,270\n",
            "Trainable params: 11,098\n",
            "Non-trainable params: 172\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KVxgzGEItcF1",
        "colab": {}
      },
      "source": [
        "# define a learning rate scheduler . We will use a simple scheduler that reduces the lr by 10% every 3 epochs subject to a minimum lr of 0.0005 \n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "def scheduler(epoch, lr):\n",
        "  if (epoch%3==0 and epoch):\n",
        "    new_lr = max(0.9*lr,0.0005) \n",
        "  else:\n",
        "    new_lr=lr\n",
        "  \n",
        "  return round(new_lr, 10)\n",
        "  \n",
        " \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RFpdzh6CtcF3",
        "colab": {}
      },
      "source": [
        "#start with a higher lr of 0.003 \n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.003), metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNM1NgZdtQm8",
        "colab_type": "text"
      },
      "source": [
        "###Define a ModelCheckPoint callback which will be called at the end of every training epoch . We will use this callback function to save the model whenever validation accuracy improves . We do this so that we can load and use the best model for further predictions after training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g05RhnRUtcFr",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "chkpoint_model=ModelCheckpoint(\"model_custom_v1_mnist_best.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='max') \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QNCgsndstcF5",
        "colab": {}
      },
      "source": [
        " callback_list=[chkpoint_model,LearningRateScheduler(scheduler, verbose=1)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GnmzsbMYvvj",
        "colab_type": "text"
      },
      "source": [
        "# Image Normalization \n",
        "\n",
        "\n",
        "Image Normalization is done so that the input data /pixel values are in a similar data distribution . We subtract the mean from each pixel value and divide by the standard deviation to get a zero centered and normally distributed data. Typically the data is scaled such that min pixel intensity is set to 0 and max pixel intensity of the input is at 1. \n",
        "![Data distribution - Image Normalization](https://raw.githubusercontent.com/ravindrabharathi/eip3/master/images/image-normalization1.png)\n",
        "\n",
        "By doing this , we ensure that weight updates are in a similar range thus resulting in a faster convergence. In the below image weight updates for w1 and w2 will progress at different speed towards a minima\n",
        "\n",
        "![un-normalized](https://raw.githubusercontent.com/ravindrabharathi/eip3/master/images/weights-unnormalized.png)\n",
        "\n",
        "Whereas in the case of a normalized data , the weight updates will be in similar range \n",
        "\n",
        "![normalized data](https://raw.githubusercontent.com/ravindrabharathi/eip3/master/images/normalized-weights.png)\n",
        "\n",
        "\n",
        "In keras we perform image normalization by using an imagedatagenerator and setting its featurewise_center and featurewise_std_normalization to True "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3Dk6B6ZYs_Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "batch_size=128\n",
        "\n",
        "train_datagen=ImageDataGenerator(\n",
        "        featurewise_center=True,  # set input mean to 0 over the dataset\n",
        "        #samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=True,  # divide inputs by std of the dataset\n",
        "        #samplewise_std_normalization=False,  # divide each input by its std\n",
        ")\n",
        "\n",
        "val_datagen= ImageDataGenerator(\n",
        "        featurewise_center=True,  # set input mean to 0 over the dataset\n",
        "        \n",
        "        featurewise_std_normalization=True,  # divide inputs by std of the dataset\n",
        "        \n",
        ")\n",
        "\n",
        "\n",
        "train_datagen.fit(X_train)\n",
        "\n",
        "val_datagen.fit(X_test)\n",
        "\n",
        "training_generator=train_datagen.flow(X_train,Y_train, batch_size=batch_size,shuffle=True,seed=42)\n",
        "\n",
        "validation_generator=val_datagen.flow(X_test,Y_test, batch_size=batch_size,shuffle=True,seed=42)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zr4kNmqGpD_5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4185
        },
        "outputId": "49aa050c-23b3-460f-8ef7-e4a027c02ae0"
      },
      "source": [
        "model.fit_generator(training_generator, epochs=40, \n",
        "                        steps_per_epoch=60000//batch_size, \n",
        "                    validation_steps=10000//batch_size, \n",
        "                    validation_data=validation_generator,shuffle=True,callbacks=callback_list,verbose=1)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/40\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "468/468 [==============================] - 10s 22ms/step - loss: 0.1883 - acc: 0.9405 - val_loss: 0.0579 - val_acc: 0.9815\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.98147, saving model to model_custom_v1_mnist_best.h5\n",
            "Epoch 2/40\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.003.\n",
            "468/468 [==============================] - 7s 14ms/step - loss: 0.0646 - acc: 0.9800 - val_loss: 0.0463 - val_acc: 0.9849\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.98147 to 0.98491, saving model to model_custom_v1_mnist_best.h5\n",
            "Epoch 3/40\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.003.\n",
            "468/468 [==============================] - 7s 14ms/step - loss: 0.0518 - acc: 0.9833 - val_loss: 0.0456 - val_acc: 0.9854\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.98491 to 0.98541, saving model to model_custom_v1_mnist_best.h5\n",
            "Epoch 4/40\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0027.\n",
            "468/468 [==============================] - 7s 14ms/step - loss: 0.0429 - acc: 0.9862 - val_loss: 0.0395 - val_acc: 0.9874\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.98541 to 0.98744, saving model to model_custom_v1_mnist_best.h5\n",
            "Epoch 5/40\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0027000001.\n",
            "468/468 [==============================] - 7s 14ms/step - loss: 0.0371 - acc: 0.9884 - val_loss: 0.0310 - val_acc: 0.9904\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.98744 to 0.99038, saving model to model_custom_v1_mnist_best.h5\n",
            "Epoch 6/40\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0027000001.\n",
            "468/468 [==============================] - 7s 14ms/step - loss: 0.0359 - acc: 0.9889 - val_loss: 0.0344 - val_acc: 0.9900\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.99038\n",
            "Epoch 7/40\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0024300001.\n",
            "468/468 [==============================] - 7s 14ms/step - loss: 0.0307 - acc: 0.9901 - val_loss: 0.0252 - val_acc: 0.9918\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.99038 to 0.99179, saving model to model_custom_v1_mnist_best.h5\n",
            "Epoch 8/40\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0024300001.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0294 - acc: 0.9906 - val_loss: 0.0277 - val_acc: 0.9909\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.99179\n",
            "Epoch 9/40\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0024300001.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0282 - acc: 0.9907 - val_loss: 0.0342 - val_acc: 0.9900\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.99179\n",
            "Epoch 10/40\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0021870001.\n",
            "468/468 [==============================] - 7s 14ms/step - loss: 0.0266 - acc: 0.9914 - val_loss: 0.0232 - val_acc: 0.9930\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.99179 to 0.99301, saving model to model_custom_v1_mnist_best.h5\n",
            "Epoch 11/40\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0021870001.\n",
            "468/468 [==============================] - 7s 14ms/step - loss: 0.0256 - acc: 0.9916 - val_loss: 0.0285 - val_acc: 0.9902\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.99301\n",
            "Epoch 12/40\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.0021870001.\n",
            "468/468 [==============================] - 7s 14ms/step - loss: 0.0243 - acc: 0.9921 - val_loss: 0.0269 - val_acc: 0.9918\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.99301\n",
            "Epoch 13/40\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0019683001.\n",
            "468/468 [==============================] - 7s 14ms/step - loss: 0.0209 - acc: 0.9933 - val_loss: 0.0246 - val_acc: 0.9926\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.99301\n",
            "Epoch 14/40\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0019683002.\n",
            "468/468 [==============================] - 7s 14ms/step - loss: 0.0207 - acc: 0.9933 - val_loss: 0.0234 - val_acc: 0.9926\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.99301\n",
            "Epoch 15/40\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0019683002.\n",
            "468/468 [==============================] - 7s 14ms/step - loss: 0.0213 - acc: 0.9932 - val_loss: 0.0257 - val_acc: 0.9918\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.99301\n",
            "Epoch 16/40\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0017714702.\n",
            "468/468 [==============================] - 7s 14ms/step - loss: 0.0190 - acc: 0.9942 - val_loss: 0.0310 - val_acc: 0.9906\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.99301\n",
            "Epoch 17/40\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.0017714702.\n",
            "468/468 [==============================] - 7s 14ms/step - loss: 0.0184 - acc: 0.9940 - val_loss: 0.0281 - val_acc: 0.9916\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.99301\n",
            "Epoch 18/40\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0017714702.\n",
            "468/468 [==============================] - 7s 14ms/step - loss: 0.0180 - acc: 0.9938 - val_loss: 0.0210 - val_acc: 0.9931\n",
            "\n",
            "Epoch 00018: val_acc improved from 0.99301 to 0.99311, saving model to model_custom_v1_mnist_best.h5\n",
            "Epoch 19/40\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0015943232.\n",
            "468/468 [==============================] - 7s 14ms/step - loss: 0.0161 - acc: 0.9946 - val_loss: 0.0260 - val_acc: 0.9919\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.99311\n",
            "Epoch 20/40\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.0015943232.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0155 - acc: 0.9945 - val_loss: 0.0301 - val_acc: 0.9919\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.99311\n",
            "Epoch 21/40\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.0015943232.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0169 - acc: 0.9943 - val_loss: 0.0301 - val_acc: 0.9917\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.99311\n",
            "Epoch 22/40\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 0.0014348909.\n",
            "468/468 [==============================] - 7s 14ms/step - loss: 0.0140 - acc: 0.9953 - val_loss: 0.0251 - val_acc: 0.9923\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.99311\n",
            "Epoch 23/40\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.0014348909.\n",
            "468/468 [==============================] - 7s 14ms/step - loss: 0.0134 - acc: 0.9954 - val_loss: 0.0266 - val_acc: 0.9913\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.99311\n",
            "Epoch 24/40\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 0.0014348909.\n",
            "468/468 [==============================] - 7s 14ms/step - loss: 0.0142 - acc: 0.9950 - val_loss: 0.0311 - val_acc: 0.9910\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.99311\n",
            "Epoch 25/40\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 0.0012914018.\n",
            "468/468 [==============================] - 7s 14ms/step - loss: 0.0127 - acc: 0.9955 - val_loss: 0.0262 - val_acc: 0.9929\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.99311\n",
            "Epoch 26/40\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 0.0012914018.\n",
            "468/468 [==============================] - 7s 14ms/step - loss: 0.0144 - acc: 0.9949 - val_loss: 0.0257 - val_acc: 0.9910\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.99311\n",
            "Epoch 27/40\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 0.0012914018.\n",
            "468/468 [==============================] - 7s 14ms/step - loss: 0.0132 - acc: 0.9957 - val_loss: 0.0249 - val_acc: 0.9929\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.99311\n",
            "Epoch 28/40\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 0.0011622616.\n",
            "468/468 [==============================] - 7s 14ms/step - loss: 0.0125 - acc: 0.9958 - val_loss: 0.0248 - val_acc: 0.9933\n",
            "\n",
            "Epoch 00028: val_acc improved from 0.99311 to 0.99331, saving model to model_custom_v1_mnist_best.h5\n",
            "Epoch 29/40\n",
            "\n",
            "Epoch 00029: LearningRateScheduler setting learning rate to 0.0011622616.\n",
            "468/468 [==============================] - 7s 14ms/step - loss: 0.0114 - acc: 0.9960 - val_loss: 0.0240 - val_acc: 0.9929\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.99331\n",
            "Epoch 30/40\n",
            "\n",
            "Epoch 00030: LearningRateScheduler setting learning rate to 0.0011622616.\n",
            "468/468 [==============================] - 7s 14ms/step - loss: 0.0115 - acc: 0.9963 - val_loss: 0.0265 - val_acc: 0.9925\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.99331\n",
            "Epoch 31/40\n",
            "\n",
            "Epoch 00031: LearningRateScheduler setting learning rate to 0.0010460354.\n",
            "468/468 [==============================] - 7s 14ms/step - loss: 0.0103 - acc: 0.9965 - val_loss: 0.0228 - val_acc: 0.9943\n",
            "\n",
            "Epoch 00031: val_acc improved from 0.99331 to 0.99433, saving model to model_custom_v1_mnist_best.h5\n",
            "Epoch 32/40\n",
            "\n",
            "Epoch 00032: LearningRateScheduler setting learning rate to 0.0010460354.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0104 - acc: 0.9960 - val_loss: 0.0245 - val_acc: 0.9929\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.99433\n",
            "Epoch 33/40\n",
            "\n",
            "Epoch 00033: LearningRateScheduler setting learning rate to 0.0010460354.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0098 - acc: 0.9968 - val_loss: 0.0285 - val_acc: 0.9927\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.99433\n",
            "Epoch 34/40\n",
            "\n",
            "Epoch 00034: LearningRateScheduler setting learning rate to 0.0009414319.\n",
            "468/468 [==============================] - 7s 14ms/step - loss: 0.0100 - acc: 0.9967 - val_loss: 0.0223 - val_acc: 0.9936\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.99433\n",
            "Epoch 35/40\n",
            "\n",
            "Epoch 00035: LearningRateScheduler setting learning rate to 0.0009414319.\n",
            "468/468 [==============================] - 7s 14ms/step - loss: 0.0097 - acc: 0.9968 - val_loss: 0.0241 - val_acc: 0.9945\n",
            "\n",
            "Epoch 00035: val_acc improved from 0.99433 to 0.99453, saving model to model_custom_v1_mnist_best.h5\n",
            "Epoch 36/40\n",
            "\n",
            "Epoch 00036: LearningRateScheduler setting learning rate to 0.0009414319.\n",
            "468/468 [==============================] - 7s 14ms/step - loss: 0.0099 - acc: 0.9968 - val_loss: 0.0268 - val_acc: 0.9928\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.99453\n",
            "Epoch 37/40\n",
            "\n",
            "Epoch 00037: LearningRateScheduler setting learning rate to 0.0008472887.\n",
            "468/468 [==============================] - 7s 14ms/step - loss: 0.0094 - acc: 0.9967 - val_loss: 0.0247 - val_acc: 0.9940\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.99453\n",
            "Epoch 38/40\n",
            "\n",
            "Epoch 00038: LearningRateScheduler setting learning rate to 0.0008472887.\n",
            "468/468 [==============================] - 7s 14ms/step - loss: 0.0089 - acc: 0.9971 - val_loss: 0.0234 - val_acc: 0.9937\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.99453\n",
            "Epoch 39/40\n",
            "\n",
            "Epoch 00039: LearningRateScheduler setting learning rate to 0.0008472887.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0089 - acc: 0.9971 - val_loss: 0.0297 - val_acc: 0.9924\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.99453\n",
            "Epoch 40/40\n",
            "\n",
            "Epoch 00040: LearningRateScheduler setting learning rate to 0.0007625598.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0092 - acc: 0.9970 - val_loss: 0.0236 - val_acc: 0.9939\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.99453\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f82d0ccec50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lbjVtIgc_sY",
        "colab_type": "text"
      },
      "source": [
        "### With Image Normalization added , we trained for 40 epochs and the max validation accuracy is 99.453 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlpQp7lskjwS",
        "colab_type": "text"
      },
      "source": [
        "#Add Weight Regularization \n",
        "\n",
        "L1 and L2 weight decay are a way to regularize the cost function and in the process help overcome overfitting \n",
        "\n",
        "L1 weight regularization ![L1](https://raw.githubusercontent.com/ravindrabharathi/eip3/master/images/L1-weight-decay.png)\n",
        "\n",
        "\n",
        "L2 weight regularization ![L2](https://raw.githubusercontent.com/ravindrabharathi/eip3/master/images/L2-weight-decay.png)\n",
        "\n",
        "\n",
        "Typically L2 regularization is used  and this is what we will use in for this model . \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ck3qn081lHF-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import regularizers\n",
        "\n",
        "reg=regularizers.l2(0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NO6CNA9ykh9J",
        "colab": {}
      },
      "source": [
        "# to get a certain degeree of predictability when generating random numbers,\n",
        "# set a random seed to initialize the pseudo-random number generator \n",
        "np.random.seed(seed=42)  \n",
        "\n",
        "# instantiate a sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# add the first convolution layer - 10 numbers of 3x3 filters , \n",
        "#This layer sees the input image of 28x28 x 1 channel . \n",
        "\n",
        "model.add(Conv2D(10, (3, 3), input_shape=(28,28,1), use_bias=False, kernel_regularizer=reg))  # remove bias param by setting it to false \n",
        " \n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "model.add(BatchNormalization())  #add Batch normalization \n",
        "model.add(Dropout(0.1))  #add dropout\n",
        "\n",
        "# Now the global receptive field is 3 x 3 \n",
        "\n",
        "# First Convolution Block\n",
        "# Block1 conv layer 1 - 12 filters of shape  3x3x10 \n",
        "# input from previous layer is 26 x 26 x 10 . \n",
        "\n",
        "## Block 1\n",
        "\n",
        "model.add(Conv2D(12, 3, use_bias=False, kernel_regularizer=reg))  # remove bias param by setting it to false \n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "model.add(BatchNormalization())  #add Batch normalization \n",
        "model.add(Dropout(0.1))   #add dropout\n",
        "\n",
        "#Global receptive field is 5x5\n",
        "\n",
        "# Add convolution layer - 16 filters of shape  3x3x12 \n",
        "#input from previous layer is 24 x 24 x 12 . \n",
        "\n",
        "model.add(Conv2D(16, 3, use_bias=False, kernel_regularizer=reg))  # remove bias param by setting it to false \n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "model.add(BatchNormalization())  #add Batch normalization \n",
        "model.add(Dropout(0.1))    #add dropout\n",
        "\n",
        "#Global receptive field is 7x7\n",
        "\n",
        "##  Transition block \n",
        "\n",
        "# Perform 2x2 max pooling  . \n",
        "#Input from previous layer is 22 x 22 X 16 \n",
        "\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "\n",
        "# After max pooling , 2D spatial dimension reduces by half , i.e it becomes 11 x 11 . \n",
        "# Maxpooling (withpool size 2 and stride 1) doubles receptive field . \n",
        "# So global receptive field after max pooling is 14 x 14\n",
        "\n",
        "# add 1x1 convolution to reduce the channel numbers to 10 \n",
        "\n",
        "model.add(Conv2D(10, 1, use_bias=False, kernel_regularizer=reg))  # remove bias param by setting it to false \n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "model.add(BatchNormalization())  #add Batch normalization \n",
        "\n",
        "\n",
        "#Convolution Block 2\n",
        "\n",
        "# Add convolution layer - 12 filters of shape 3x3x10\n",
        "#input from transition layer is 11 x 11 x 10 .  \n",
        "\n",
        "model.add(Conv2D(12, 3,  use_bias=False, kernel_regularizer=reg))  # remove bias param by setting it to false \n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "model.add(BatchNormalization())  #add Batch normalization \n",
        "model.add(Dropout(0.1))   #add dropout\n",
        "\n",
        "#Global receptive field is now 16 x 16 \n",
        "\n",
        "# Add convolution layer - 16 filters of shape 3x3x12 \n",
        "#input coming from previous layer is 9 x 9 x 12 . \n",
        "\n",
        "model.add(Conv2D(16, 3,  use_bias=False, kernel_regularizer=reg)) # remove bias param by setting it to false \n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "model.add(BatchNormalization())  #add Batch normalization \n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "#  Global receptive field is now 18 x 18 \n",
        "\n",
        "\n",
        "# Add 1x1 convolution to reduce number of channels to 10  \n",
        "#input coming from previous layer is 7 x 7 x 16 .\n",
        "\n",
        "model.add(Conv2D(10, 1,  use_bias=False, kernel_regularizer=reg))  # remove bias param by setting it to false \n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "model.add(BatchNormalization())  #add Batch normalization \n",
        "\n",
        "\n",
        "#Global receptive field is now 18 x 18 \n",
        "\n",
        "# Last layer :  Add convolution layer - 10 filters of shape 7x7x10 \n",
        "#input coming from previous layer is 7 x 7 x 10 .\n",
        "\n",
        "model.add(Conv2D(10, 7,  use_bias=False,kernel_regularizer=reg))  # remove bias param by setting it to false \n",
        "#No Batch normalization before prediction \n",
        "# Note absence of ReLU activation here \n",
        "\n",
        "\n",
        "model.add(Flatten())  # Flatten the 2d array to 1d input for the softmax activation \n",
        "model.add(Activation('softmax'))   # Softmax activation to out class probabilities "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a59ec09e-8d1c-4ebc-b43b-5fac74c1b7ea",
        "id": "uWwsauvRluD4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1156
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_9 (Conv2D)            (None, 26, 26, 10)        90        \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 26, 26, 10)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 26, 26, 10)        40        \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 26, 26, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 24, 24, 12)        1080      \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 24, 24, 12)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 24, 24, 12)        48        \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 24, 24, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 22, 22, 16)        1728      \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 22, 22, 16)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 22, 22, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 22, 22, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 11, 11, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 11, 11, 10)        160       \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 11, 11, 10)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 11, 11, 10)        40        \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 9, 9, 12)          1080      \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 9, 9, 12)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 9, 9, 12)          48        \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 9, 9, 12)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 7, 7, 16)          1728      \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 7, 7, 16)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 7, 7, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 7, 7, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 7, 7, 10)          160       \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 7, 7, 10)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 7, 7, 10)          40        \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 1, 1, 10)          4900      \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_16 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 11,270\n",
            "Trainable params: 11,098\n",
            "Non-trainable params: 172\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lWJS1PavluD7",
        "colab": {}
      },
      "source": [
        "# define a learning rate scheduler . We will use a simple scheduler that reduces the lr by 10% every 3 epochs subject to a minimum lr of 0.0005 \n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "def scheduler(epoch, lr):\n",
        "  if (epoch%3==0 and epoch):\n",
        "    new_lr = max(0.9*lr,0.0005) \n",
        "  else:\n",
        "    new_lr=lr\n",
        "  \n",
        "  return round(new_lr, 10)\n",
        "  \n",
        " \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mfnIi3GuluD8",
        "colab": {}
      },
      "source": [
        "#start with a higher lr of 0.003 \n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.003), metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TuqMGqBYoCj-",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "chkpoint_model=ModelCheckpoint(\"model_custom_v2_mnist_best.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='max') \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wOXwJrW2oCj_",
        "colab": {}
      },
      "source": [
        " callback_list=[chkpoint_model,LearningRateScheduler(scheduler, verbose=1)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvCaTdWBnEpU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4114
        },
        "outputId": "a2d663bf-c30a-48b1-a52d-1caebe04b000"
      },
      "source": [
        "batch_size=128\n",
        "\n",
        "train_datagen=ImageDataGenerator(\n",
        "        featurewise_center=True,  # set input mean to 0 over the dataset\n",
        "        #samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=True,  # divide inputs by std of the dataset\n",
        "        #samplewise_std_normalization=False,  # divide each input by its std\n",
        ")\n",
        "\n",
        "val_datagen= ImageDataGenerator(\n",
        "        featurewise_center=True,  # set input mean to 0 over the dataset\n",
        "        \n",
        "        featurewise_std_normalization=True,  # divide inputs by std of the dataset\n",
        "        \n",
        ")\n",
        "\n",
        "\n",
        "train_datagen.fit(X_train)\n",
        "\n",
        "val_datagen.fit(X_test)\n",
        "\n",
        "training_generator=train_datagen.flow(X_train,Y_train, batch_size=batch_size,shuffle=True,seed=42)\n",
        "\n",
        "validation_generator=val_datagen.flow(X_test,Y_test, batch_size=batch_size,shuffle=True,seed=42)\n",
        "\n",
        "\n",
        "\n",
        "model.fit_generator(training_generator, epochs=40, \n",
        "                        steps_per_epoch=60000//batch_size, \n",
        "                    validation_steps=10000//batch_size, \n",
        "                    validation_data=validation_generator,shuffle=True,callbacks=callback_list,verbose=1)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "468/468 [==============================] - 9s 20ms/step - loss: 0.2742 - acc: 0.9405 - val_loss: 0.1286 - val_acc: 0.9828\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.98277, saving model to model_custom_v2_mnist_best.h5\n",
            "Epoch 2/40\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.003.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.1331 - acc: 0.9799 - val_loss: 0.1128 - val_acc: 0.9827\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.98277\n",
            "Epoch 3/40\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.003.\n",
            "468/468 [==============================] - 8s 16ms/step - loss: 0.1107 - acc: 0.9826 - val_loss: 0.0970 - val_acc: 0.9852\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.98277 to 0.98521, saving model to model_custom_v2_mnist_best.h5\n",
            "Epoch 4/40\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0027.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0971 - acc: 0.9845 - val_loss: 0.0985 - val_acc: 0.9846\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.98521\n",
            "Epoch 5/40\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0027000001.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0947 - acc: 0.9844 - val_loss: 0.0911 - val_acc: 0.9853\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.98521 to 0.98531, saving model to model_custom_v2_mnist_best.h5\n",
            "Epoch 6/40\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0027000001.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0911 - acc: 0.9851 - val_loss: 0.0802 - val_acc: 0.9888\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.98531 to 0.98876, saving model to model_custom_v2_mnist_best.h5\n",
            "Epoch 7/40\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0024300001.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0838 - acc: 0.9868 - val_loss: 0.0706 - val_acc: 0.9897\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.98876 to 0.98967, saving model to model_custom_v2_mnist_best.h5\n",
            "Epoch 8/40\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0024300001.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0844 - acc: 0.9859 - val_loss: 0.0748 - val_acc: 0.9884\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.98967\n",
            "Epoch 9/40\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0024300001.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0822 - acc: 0.9861 - val_loss: 0.0740 - val_acc: 0.9879\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.98967\n",
            "Epoch 10/40\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0021870001.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0794 - acc: 0.9862 - val_loss: 0.0714 - val_acc: 0.9892\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.98967\n",
            "Epoch 11/40\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0021870001.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0769 - acc: 0.9869 - val_loss: 0.0700 - val_acc: 0.9897\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.98967\n",
            "Epoch 12/40\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.0021870001.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0770 - acc: 0.9867 - val_loss: 0.0709 - val_acc: 0.9888\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.98967\n",
            "Epoch 13/40\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0019683001.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0720 - acc: 0.9884 - val_loss: 0.0661 - val_acc: 0.9890\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.98967\n",
            "Epoch 14/40\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0019683002.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0707 - acc: 0.9882 - val_loss: 0.0688 - val_acc: 0.9871\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.98967\n",
            "Epoch 15/40\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0019683002.\n",
            "468/468 [==============================] - 8s 16ms/step - loss: 0.0716 - acc: 0.9874 - val_loss: 0.0602 - val_acc: 0.9896\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.98967\n",
            "Epoch 16/40\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0017714702.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0674 - acc: 0.9889 - val_loss: 0.0615 - val_acc: 0.9894\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.98967\n",
            "Epoch 17/40\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.0017714702.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0692 - acc: 0.9881 - val_loss: 0.0701 - val_acc: 0.9884\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.98967\n",
            "Epoch 18/40\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0017714702.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0662 - acc: 0.9889 - val_loss: 0.0599 - val_acc: 0.9913\n",
            "\n",
            "Epoch 00018: val_acc improved from 0.98967 to 0.99129, saving model to model_custom_v2_mnist_best.h5\n",
            "Epoch 19/40\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0015943232.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0624 - acc: 0.9899 - val_loss: 0.0563 - val_acc: 0.9914\n",
            "\n",
            "Epoch 00019: val_acc improved from 0.99129 to 0.99139, saving model to model_custom_v2_mnist_best.h5\n",
            "Epoch 20/40\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.0015943232.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0602 - acc: 0.9902 - val_loss: 0.0593 - val_acc: 0.9891\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.99139\n",
            "Epoch 21/40\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.0015943232.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0610 - acc: 0.9901 - val_loss: 0.0586 - val_acc: 0.9907\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.99139\n",
            "Epoch 22/40\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 0.0014348909.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0594 - acc: 0.9899 - val_loss: 0.0628 - val_acc: 0.9886\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.99139\n",
            "Epoch 23/40\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.0014348909.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0583 - acc: 0.9900 - val_loss: 0.0614 - val_acc: 0.9882\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.99139\n",
            "Epoch 24/40\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 0.0014348909.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0589 - acc: 0.9897 - val_loss: 0.0596 - val_acc: 0.9891\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.99139\n",
            "Epoch 25/40\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 0.0012914018.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0563 - acc: 0.9905 - val_loss: 0.0547 - val_acc: 0.9917\n",
            "\n",
            "Epoch 00025: val_acc improved from 0.99139 to 0.99169, saving model to model_custom_v2_mnist_best.h5\n",
            "Epoch 26/40\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 0.0012914018.\n",
            "468/468 [==============================] - 7s 16ms/step - loss: 0.0544 - acc: 0.9909 - val_loss: 0.0522 - val_acc: 0.9908\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.99169\n",
            "Epoch 27/40\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 0.0012914018.\n",
            "468/468 [==============================] - 7s 16ms/step - loss: 0.0544 - acc: 0.9910 - val_loss: 0.0574 - val_acc: 0.9903\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.99169\n",
            "Epoch 28/40\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 0.0011622616.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0503 - acc: 0.9921 - val_loss: 0.0502 - val_acc: 0.9919\n",
            "\n",
            "Epoch 00028: val_acc improved from 0.99169 to 0.99190, saving model to model_custom_v2_mnist_best.h5\n",
            "Epoch 29/40\n",
            "\n",
            "Epoch 00029: LearningRateScheduler setting learning rate to 0.0011622616.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0501 - acc: 0.9916 - val_loss: 0.0496 - val_acc: 0.9922\n",
            "\n",
            "Epoch 00029: val_acc improved from 0.99190 to 0.99220, saving model to model_custom_v2_mnist_best.h5\n",
            "Epoch 30/40\n",
            "\n",
            "Epoch 00030: LearningRateScheduler setting learning rate to 0.0011622616.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0521 - acc: 0.9911 - val_loss: 0.0584 - val_acc: 0.9897\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.99220\n",
            "Epoch 31/40\n",
            "\n",
            "Epoch 00031: LearningRateScheduler setting learning rate to 0.0010460354.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0480 - acc: 0.9921 - val_loss: 0.0471 - val_acc: 0.9919\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.99220\n",
            "Epoch 32/40\n",
            "\n",
            "Epoch 00032: LearningRateScheduler setting learning rate to 0.0010460354.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0492 - acc: 0.9916 - val_loss: 0.0501 - val_acc: 0.9912\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.99220\n",
            "Epoch 33/40\n",
            "\n",
            "Epoch 00033: LearningRateScheduler setting learning rate to 0.0010460354.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0474 - acc: 0.9915 - val_loss: 0.0476 - val_acc: 0.9914\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.99220\n",
            "Epoch 34/40\n",
            "\n",
            "Epoch 00034: LearningRateScheduler setting learning rate to 0.0009414319.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0463 - acc: 0.9924 - val_loss: 0.0463 - val_acc: 0.9914\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.99220\n",
            "Epoch 35/40\n",
            "\n",
            "Epoch 00035: LearningRateScheduler setting learning rate to 0.0009414319.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0464 - acc: 0.9923 - val_loss: 0.0530 - val_acc: 0.9898\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.99220\n",
            "Epoch 36/40\n",
            "\n",
            "Epoch 00036: LearningRateScheduler setting learning rate to 0.0009414319.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0448 - acc: 0.9926 - val_loss: 0.0441 - val_acc: 0.9930\n",
            "\n",
            "Epoch 00036: val_acc improved from 0.99220 to 0.99301, saving model to model_custom_v2_mnist_best.h5\n",
            "Epoch 37/40\n",
            "\n",
            "Epoch 00037: LearningRateScheduler setting learning rate to 0.0008472887.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0434 - acc: 0.9926 - val_loss: 0.0431 - val_acc: 0.9938\n",
            "\n",
            "Epoch 00037: val_acc improved from 0.99301 to 0.99382, saving model to model_custom_v2_mnist_best.h5\n",
            "Epoch 38/40\n",
            "\n",
            "Epoch 00038: LearningRateScheduler setting learning rate to 0.0008472887.\n",
            "468/468 [==============================] - 8s 16ms/step - loss: 0.0421 - acc: 0.9928 - val_loss: 0.0428 - val_acc: 0.9929\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.99382\n",
            "Epoch 39/40\n",
            "\n",
            "Epoch 00039: LearningRateScheduler setting learning rate to 0.0008472887.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0417 - acc: 0.9928 - val_loss: 0.0443 - val_acc: 0.9921\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.99382\n",
            "Epoch 40/40\n",
            "\n",
            "Epoch 00040: LearningRateScheduler setting learning rate to 0.0007625598.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0404 - acc: 0.9932 - val_loss: 0.0476 - val_acc: 0.9912\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.99382\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f82b1693908>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jB2DVcEHciNS",
        "colab_type": "text"
      },
      "source": [
        "####We added L2 regularization , trained for 40 epochs and reached a max validation accuracy of 99.382 . The effect of regularization is to prevent overfitting and reduxe the gap between training and validation accuracies "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmyGmS8oy2qE",
        "colab_type": "text"
      },
      "source": [
        "#Model with BatchNormalization placed before ReLU activation \n",
        "\n",
        "Let us try experiment with the placement of BatchNormalization before ReLU activation to see if it makes any diference \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rdn9MvWmVOtA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# to get a certain degeree of predictability when generating random numbers,\n",
        "# set a random seed to initialize the pseudo-random number generator \n",
        "np.random.seed(seed=42)  \n",
        "\n",
        "# instantiate a sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# add the first convolution layer - 10 numbers of 3x3 filters , \n",
        "#This layer sees the input image of 28x28 x 1 channel . \n",
        "\n",
        "model.add(Conv2D(10, (3, 3), input_shape=(28,28,1), use_bias=False,kernel_regularizer=reg))  # remove bias param by setting it to false \n",
        "model.add(BatchNormalization())  #add Batch normalization  \n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "model.add(Dropout(0.1))  #add dropout\n",
        "\n",
        "# Now the global receptive field is 3 x 3 \n",
        "\n",
        "# First Convolution Block\n",
        "# Block1 conv layer 1 - 12 filters of shape  3x3x10 \n",
        "# input from previous layer is 26 x 26 x 10 . \n",
        "\n",
        "## Block 1\n",
        "\n",
        "model.add(Conv2D(12, 3, use_bias=False,kernel_regularizer=reg))  # remove bias param by setting it to false \n",
        "model.add(BatchNormalization())  #add Batch normalization \n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "model.add(Dropout(0.1))   #add dropout\n",
        "\n",
        "#Global receptive field is 5x5\n",
        "\n",
        "# Add convolution layer - 16 filters of shape  3x3x12 \n",
        "#input from previous layer is 24 x 24 x 12 . \n",
        "\n",
        "model.add(Conv2D(16, 3, use_bias=False,kernel_regularizer=reg))  # remove bias param by setting it to false \n",
        "model.add(BatchNormalization())  #add Batch normalization\n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "model.add(Dropout(0.1))    #add dropout\n",
        "\n",
        "#Global receptive field is 7x7\n",
        "\n",
        "##  Transition block \n",
        "\n",
        "# Perform 2x2 max pooling  . \n",
        "#Input from previous layer is 22 x 22 X 16 \n",
        "\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "\n",
        "# After max pooling , 2D spatial dimension reduces by half , i.e it becomes 11 x 11 . \n",
        "# Maxpooling (withpool size 2 and stride 1) doubles receptive field . \n",
        "# So global receptive field after max pooling is 14 x 14\n",
        "\n",
        "# add 1x1 convolution to reduce the channel numbers to 10 \n",
        "\n",
        "model.add(Conv2D(10, 1, use_bias=False,kernel_regularizer=reg))  # remove bias param by setting it to false \n",
        "model.add(BatchNormalization())  #add Batch normalization \n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "\n",
        "\n",
        "#Convolution Block 2\n",
        "\n",
        "# Add convolution layer - 12 filters of shape 3x3x10\n",
        "#input from transition layer is 11 x 11 x 10 .  \n",
        "\n",
        "model.add(Conv2D(12, 3,  use_bias=False,kernel_regularizer=reg))  # remove bias param by setting it to false \n",
        "model.add(BatchNormalization())  #add Batch normalization\n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "model.add(Dropout(0.1))   #add dropout\n",
        "\n",
        "#Global receptive field is now 16 x 16 \n",
        "\n",
        "# Add convolution layer - 16 filters of shape 3x3x12 \n",
        "#input coming from previous layer is 9 x 9 x 12 . \n",
        "\n",
        "model.add(Conv2D(16, 3,  use_bias=False,kernel_regularizer=reg)) # remove bias param by setting it to false \n",
        "model.add(BatchNormalization())  #add Batch normalization \n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "#  Global receptive field is now 18 x 18 \n",
        "\n",
        "\n",
        "# Add 1x1 convolution to reduce number of channels to 10  \n",
        "#input coming from previous layer is 7 x 7 x 16 .\n",
        "\n",
        "model.add(Conv2D(10, 1,  use_bias=False,kernel_regularizer=reg))  # remove bias param by setting it to false \n",
        "model.add(BatchNormalization())  #add Batch normalization \n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "\n",
        "\n",
        "#Global receptive field is now 18 x 18 \n",
        "\n",
        "# Last layer :  Add convolution layer - 10 filters of shape 7x7x10 \n",
        "#input coming from previous layer is 7 x 7 x 10 .\n",
        "\n",
        "model.add(Conv2D(10, 7,  use_bias=False,kernel_regularizer=reg))  # remove bias param by setting it to false \n",
        "#No Batch normalization before prediction \n",
        "# Note absence of ReLU activation here \n",
        "\n",
        "\n",
        "model.add(Flatten())  # Flatten the 2d array to 1d input for the softmax activation \n",
        "model.add(Activation('softmax'))   # Softmax activation to out class probabilities "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTdVH726xhN3",
        "colab_type": "code",
        "outputId": "a9715a65-839f-498c-bfb8-7476c916b456",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1156
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_17 (Conv2D)           (None, 26, 26, 10)        90        \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 26, 26, 10)        40        \n",
            "_________________________________________________________________\n",
            "activation_17 (Activation)   (None, 26, 26, 10)        0         \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 26, 26, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 24, 24, 12)        1080      \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 24, 24, 12)        48        \n",
            "_________________________________________________________________\n",
            "activation_18 (Activation)   (None, 24, 24, 12)        0         \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 24, 24, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 22, 22, 16)        1728      \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 22, 22, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_19 (Activation)   (None, 22, 22, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 22, 22, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 11, 11, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 11, 11, 10)        160       \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 11, 11, 10)        40        \n",
            "_________________________________________________________________\n",
            "activation_20 (Activation)   (None, 11, 11, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_21 (Conv2D)           (None, 9, 9, 12)          1080      \n",
            "_________________________________________________________________\n",
            "batch_normalization_19 (Batc (None, 9, 9, 12)          48        \n",
            "_________________________________________________________________\n",
            "activation_21 (Activation)   (None, 9, 9, 12)          0         \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 9, 9, 12)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 7, 7, 16)          1728      \n",
            "_________________________________________________________________\n",
            "batch_normalization_20 (Batc (None, 7, 7, 16)          64        \n",
            "_________________________________________________________________\n",
            "activation_22 (Activation)   (None, 7, 7, 16)          0         \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 7, 7, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 7, 7, 10)          160       \n",
            "_________________________________________________________________\n",
            "batch_normalization_21 (Batc (None, 7, 7, 10)          40        \n",
            "_________________________________________________________________\n",
            "activation_23 (Activation)   (None, 7, 7, 10)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_24 (Conv2D)           (None, 1, 1, 10)          4900      \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_24 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 11,270\n",
            "Trainable params: 11,098\n",
            "Non-trainable params: 172\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "STjmgKadoO7F",
        "colab": {}
      },
      "source": [
        "# define a learning rate scheduler . We will use a simple scheduler that reduces the lr by 10% every 3 epochs subject to a minimum lr of 0.0005 \n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "def scheduler(epoch, lr):\n",
        "  if (epoch%3==0 and epoch):\n",
        "    new_lr = max(0.9*lr,0.0005) \n",
        "  else:\n",
        "    new_lr=lr\n",
        "  \n",
        "  return round(new_lr, 10)\n",
        "  \n",
        " \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tYliZCdAoO7G",
        "colab": {}
      },
      "source": [
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.003), metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GqlVzWomoO7I",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "chkpoint_model=ModelCheckpoint(\"model_custom_v3_mnist_best.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='max') \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gEUYjDgSoO7K",
        "colab": {}
      },
      "source": [
        " callback_list=[chkpoint_model,LearningRateScheduler(scheduler, verbose=1)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1t_u6PfcoO7M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4114
        },
        "outputId": "9559e6df-ae64-416d-a2e9-dedb0f513fa5"
      },
      "source": [
        "batch_size=128\n",
        "\n",
        "train_datagen=ImageDataGenerator(\n",
        "        featurewise_center=True,  # set input mean to 0 over the dataset\n",
        "        #samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=True,  # divide inputs by std of the dataset\n",
        "        #samplewise_std_normalization=False,  # divide each input by its std\n",
        ")\n",
        "\n",
        "val_datagen= ImageDataGenerator(\n",
        "        featurewise_center=True,  # set input mean to 0 over the dataset\n",
        "        \n",
        "        featurewise_std_normalization=True,  # divide inputs by std of the dataset\n",
        "        \n",
        ")\n",
        "\n",
        "\n",
        "train_datagen.fit(X_train)\n",
        "\n",
        "val_datagen.fit(X_test)\n",
        "\n",
        "training_generator=train_datagen.flow(X_train,Y_train, batch_size=batch_size,shuffle=True,seed=42)\n",
        "\n",
        "validation_generator=val_datagen.flow(X_test,Y_test, batch_size=batch_size,shuffle=True,seed=42)\n",
        "\n",
        "\n",
        "\n",
        "model.fit_generator(training_generator, epochs=40, \n",
        "                        steps_per_epoch=60000//batch_size, \n",
        "                    validation_steps=10000//batch_size, \n",
        "                    validation_data=validation_generator,shuffle=True,callbacks=callback_list,verbose=1)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "468/468 [==============================] - 9s 20ms/step - loss: 0.2885 - acc: 0.9372 - val_loss: 0.1544 - val_acc: 0.9772\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.97716, saving model to model_custom_v3_mnist_best.h5\n",
            "Epoch 2/40\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.003.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.1354 - acc: 0.9781 - val_loss: 0.1375 - val_acc: 0.9746\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.97716\n",
            "Epoch 3/40\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.003.\n",
            "468/468 [==============================] - 8s 16ms/step - loss: 0.1127 - acc: 0.9809 - val_loss: 0.0932 - val_acc: 0.9851\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.97716 to 0.98511, saving model to model_custom_v3_mnist_best.h5\n",
            "Epoch 4/40\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0027.\n",
            "468/468 [==============================] - 7s 16ms/step - loss: 0.1016 - acc: 0.9830 - val_loss: 0.0977 - val_acc: 0.9849\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.98511\n",
            "Epoch 5/40\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0027000001.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0936 - acc: 0.9837 - val_loss: 0.0942 - val_acc: 0.9829\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.98511\n",
            "Epoch 6/40\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0027000001.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0924 - acc: 0.9845 - val_loss: 0.0773 - val_acc: 0.9897\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.98511 to 0.98967, saving model to model_custom_v3_mnist_best.h5\n",
            "Epoch 7/40\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0024300001.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0846 - acc: 0.9864 - val_loss: 0.0728 - val_acc: 0.9893\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.98967\n",
            "Epoch 8/40\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0024300001.\n",
            "468/468 [==============================] - 8s 16ms/step - loss: 0.0853 - acc: 0.9852 - val_loss: 0.0929 - val_acc: 0.9820\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.98967\n",
            "Epoch 9/40\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0024300001.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0840 - acc: 0.9854 - val_loss: 0.0749 - val_acc: 0.9890\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.98967\n",
            "Epoch 10/40\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0021870001.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0778 - acc: 0.9873 - val_loss: 0.0759 - val_acc: 0.9876\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.98967\n",
            "Epoch 11/40\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0021870001.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0793 - acc: 0.9867 - val_loss: 0.0746 - val_acc: 0.9881\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.98967\n",
            "Epoch 12/40\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.0021870001.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0754 - acc: 0.9873 - val_loss: 0.0651 - val_acc: 0.9899\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.98967 to 0.98987, saving model to model_custom_v3_mnist_best.h5\n",
            "Epoch 13/40\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0019683001.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0731 - acc: 0.9877 - val_loss: 0.0622 - val_acc: 0.9907\n",
            "\n",
            "Epoch 00013: val_acc improved from 0.98987 to 0.99068, saving model to model_custom_v3_mnist_best.h5\n",
            "Epoch 14/40\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0019683002.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0715 - acc: 0.9876 - val_loss: 0.0764 - val_acc: 0.9867\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.99068\n",
            "Epoch 15/40\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0019683002.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0741 - acc: 0.9873 - val_loss: 0.0622 - val_acc: 0.9909\n",
            "\n",
            "Epoch 00015: val_acc improved from 0.99068 to 0.99088, saving model to model_custom_v3_mnist_best.h5\n",
            "Epoch 16/40\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0017714702.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0668 - acc: 0.9892 - val_loss: 0.0607 - val_acc: 0.9897\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.99088\n",
            "Epoch 17/40\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.0017714702.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0680 - acc: 0.9886 - val_loss: 0.0754 - val_acc: 0.9865\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.99088\n",
            "Epoch 18/40\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0017714702.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0674 - acc: 0.9885 - val_loss: 0.0682 - val_acc: 0.9882\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.99088\n",
            "Epoch 19/40\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0015943232.\n",
            "468/468 [==============================] - 7s 16ms/step - loss: 0.0620 - acc: 0.9895 - val_loss: 0.0570 - val_acc: 0.9920\n",
            "\n",
            "Epoch 00019: val_acc improved from 0.99088 to 0.99200, saving model to model_custom_v3_mnist_best.h5\n",
            "Epoch 20/40\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.0015943232.\n",
            "468/468 [==============================] - 7s 16ms/step - loss: 0.0628 - acc: 0.9896 - val_loss: 0.0551 - val_acc: 0.9920\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.99200\n",
            "Epoch 21/40\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.0015943232.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0609 - acc: 0.9895 - val_loss: 0.0616 - val_acc: 0.9897\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.99200\n",
            "Epoch 22/40\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 0.0014348909.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0608 - acc: 0.9892 - val_loss: 0.0569 - val_acc: 0.9909\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.99200\n",
            "Epoch 23/40\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.0014348909.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0589 - acc: 0.9901 - val_loss: 0.0531 - val_acc: 0.9915\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.99200\n",
            "Epoch 24/40\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 0.0014348909.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0586 - acc: 0.9898 - val_loss: 0.0562 - val_acc: 0.9908\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.99200\n",
            "Epoch 25/40\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 0.0012914018.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0553 - acc: 0.9908 - val_loss: 0.0545 - val_acc: 0.9908\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.99200\n",
            "Epoch 26/40\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 0.0012914018.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0538 - acc: 0.9908 - val_loss: 0.0585 - val_acc: 0.9888\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.99200\n",
            "Epoch 27/40\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 0.0012914018.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0541 - acc: 0.9904 - val_loss: 0.0467 - val_acc: 0.9929\n",
            "\n",
            "Epoch 00027: val_acc improved from 0.99200 to 0.99291, saving model to model_custom_v3_mnist_best.h5\n",
            "Epoch 28/40\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 0.0011622616.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0523 - acc: 0.9910 - val_loss: 0.0530 - val_acc: 0.9899\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.99291\n",
            "Epoch 29/40\n",
            "\n",
            "Epoch 00029: LearningRateScheduler setting learning rate to 0.0011622616.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0518 - acc: 0.9915 - val_loss: 0.0500 - val_acc: 0.9916\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.99291\n",
            "Epoch 30/40\n",
            "\n",
            "Epoch 00030: LearningRateScheduler setting learning rate to 0.0011622616.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0513 - acc: 0.9910 - val_loss: 0.0461 - val_acc: 0.9939\n",
            "\n",
            "Epoch 00030: val_acc improved from 0.99291 to 0.99392, saving model to model_custom_v3_mnist_best.h5\n",
            "Epoch 31/40\n",
            "\n",
            "Epoch 00031: LearningRateScheduler setting learning rate to 0.0010460354.\n",
            "468/468 [==============================] - 8s 16ms/step - loss: 0.0480 - acc: 0.9920 - val_loss: 0.0465 - val_acc: 0.9920\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.99392\n",
            "Epoch 32/40\n",
            "\n",
            "Epoch 00032: LearningRateScheduler setting learning rate to 0.0010460354.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0498 - acc: 0.9913 - val_loss: 0.0485 - val_acc: 0.9908\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.99392\n",
            "Epoch 33/40\n",
            "\n",
            "Epoch 00033: LearningRateScheduler setting learning rate to 0.0010460354.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0471 - acc: 0.9923 - val_loss: 0.0513 - val_acc: 0.9903\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.99392\n",
            "Epoch 34/40\n",
            "\n",
            "Epoch 00034: LearningRateScheduler setting learning rate to 0.0009414319.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0470 - acc: 0.9918 - val_loss: 0.0466 - val_acc: 0.9921\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.99392\n",
            "Epoch 35/40\n",
            "\n",
            "Epoch 00035: LearningRateScheduler setting learning rate to 0.0009414319.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0448 - acc: 0.9926 - val_loss: 0.0474 - val_acc: 0.9923\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.99392\n",
            "Epoch 36/40\n",
            "\n",
            "Epoch 00036: LearningRateScheduler setting learning rate to 0.0009414319.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0454 - acc: 0.9920 - val_loss: 0.0435 - val_acc: 0.9932\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.99392\n",
            "Epoch 37/40\n",
            "\n",
            "Epoch 00037: LearningRateScheduler setting learning rate to 0.0008472887.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0448 - acc: 0.9921 - val_loss: 0.0422 - val_acc: 0.9921\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.99392\n",
            "Epoch 38/40\n",
            "\n",
            "Epoch 00038: LearningRateScheduler setting learning rate to 0.0008472887.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0427 - acc: 0.9929 - val_loss: 0.0459 - val_acc: 0.9926\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.99392\n",
            "Epoch 39/40\n",
            "\n",
            "Epoch 00039: LearningRateScheduler setting learning rate to 0.0008472887.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0422 - acc: 0.9931 - val_loss: 0.0424 - val_acc: 0.9934\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.99392\n",
            "Epoch 40/40\n",
            "\n",
            "Epoch 00040: LearningRateScheduler setting learning rate to 0.0007625598.\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 0.0409 - acc: 0.9932 - val_loss: 0.0419 - val_acc: 0.9921\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.99392\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f82b385e240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7MDx4-KdSH9",
        "colab_type": "text"
      },
      "source": [
        "### There isn't a very significant difference in the validation accuracy after 40 epochs when we changed BatchNormalization to be before ReLU activation . Max validation accuracy after 40 epochs is 99.392 whereas before this change it was 99.382 after 40 epochs "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7fa3y3MszKQ",
        "colab_type": "text"
      },
      "source": [
        "#Print the first 25 mis-classified images   \n",
        "\n",
        "### Let us load the Model with best validation accuracy and print the evaluation score "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvdXCXK2l9KQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model=load_model(\"model_custom_v3_mnist_best.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-4UdjONz1F3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size=128\n",
        "\n",
        "\n",
        "val_datagen= ImageDataGenerator(\n",
        "        featurewise_center=True,  # set input mean to 0 over the dataset\n",
        "        \n",
        "        featurewise_std_normalization=True,  # divide inputs by std of the dataset\n",
        "        \n",
        ")\n",
        "\n",
        "\n",
        "val_datagen.fit(X_test)\n",
        "\n",
        "validation_generator=val_datagen.flow(X_test,Y_test, batch_size=batch_size,shuffle=False,seed=42)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtsH-lLk-eLb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "score = model.evaluate_generator(validation_generator,steps=math.ceil(len(X_test)/batch_size) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkX8JMv79q9r",
        "colab_type": "code",
        "outputId": "f2c9fcac-c6c9-4eb8-eaf3-7897b2950da6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(score)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.0454264534085989, 0.9937]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Acfgy31CtWYc",
        "colab_type": "text"
      },
      "source": [
        "### Predict the classes using model.predict and print predicted probabilities and categorical array for True test classes "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCWoJkwE9suh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size=128\n",
        "\n",
        "\n",
        "val_datagen= ImageDataGenerator(\n",
        "        featurewise_center=True,  # set input mean to 0 over the dataset\n",
        "        \n",
        "        featurewise_std_normalization=True,  # divide inputs by std of the dataset\n",
        "        \n",
        ")\n",
        "\n",
        "\n",
        "val_datagen.fit(X_test)\n",
        "\n",
        "validation_generator=val_datagen.flow(X_test,Y_test, batch_size=batch_size,shuffle=False,seed=42)\n",
        "\n",
        "y_pred = model.predict_generator(validation_generator,steps=math.ceil(len(X_test)/batch_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "602F1oFPhFWX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7f4590e9-0a7f-4259-f581-f07389460c06"
      },
      "source": [
        "y_pred.shape"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zR7DAIy9hMtT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred1=np.rint(y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmFm4DFnij8j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred2=np.argmax(y_pred,axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yrc2dbJrhPjJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1ba9358a-7e45-4938-e841-55a11bc8b156"
      },
      "source": [
        "y_pred2.shape"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym7iCFBm9uBs",
        "colab_type": "code",
        "outputId": "5f289abf-7c4e-47e4-afec-8dfbc9ea9759",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "print(y_pred1[:9])\n",
        "print(y_pred2[:9])\n",
        "print(y_test[:9])"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
            "[7 2 1 0 4 1 4 9 5]\n",
            "[7 2 1 0 4 1 4 9 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYXyoQ8-byrL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6e621b08-dfa2-4e50-f70e-532af5402c00"
      },
      "source": [
        "print(len(y_pred1),len(Y_test))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBigYMPKeGHt",
        "colab_type": "text"
      },
      "source": [
        "### capture the images and labels that were misclassified "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HE2IR0tnjJm3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8f7a41b8-ceac-46ca-c402-8aeef85aa3d6"
      },
      "source": [
        "wrong_set=[]\n",
        "correct_set=[]\n",
        "wrong_labels=[]\n",
        "true_labels=[]\n",
        "for i in range(10000):\n",
        "  \n",
        "  if (y_pred2[i]==y_test[i]):\n",
        "    \n",
        "    correct_set.append(X_test[i])\n",
        "  else:\n",
        "    wrong_labels.append(y_pred2[i])\n",
        "    true_labels.append(y_test[i])\n",
        "    wrong_set.append(X_test[i])\n",
        "  \n",
        "print(len(wrong_set),len(correct_set)) "
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "63 9937\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tqCaUi_epEX",
        "colab_type": "text"
      },
      "source": [
        "###Render an image grid containing the first 25 mis-classified images "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHwFAA6kkOLq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658
        },
        "outputId": "3a3f6ea2-c7f5-4cc1-91d7-9dd8cad00066"
      },
      "source": [
        "print('            First 25 misclassified images \\n           _________________________________\\n')\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "\n",
        "fig = plt.figure(1, (10., 10.))\n",
        "\n",
        "grid = ImageGrid(fig, 111,  \n",
        "                 nrows_ncols=(5, 5),  \n",
        "                 axes_pad=0.3,  \n",
        "                 )\n",
        "for i in range(25):\n",
        "    grid[i].imshow(wrong_set[i].reshape(28,28),cmap='binary')\n",
        "    grid[i].set_title('{0} classified as {1}'.format(true_labels[i],wrong_labels[i]),color='red')\n",
        "    \n",
        "plt.show()    "
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "            First 25 misclassified images \n",
            "           _________________________________\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkUAAAJOCAYAAAC5uXMCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/2xCAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x720 with 50 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2q6fJ67qvlsY",
        "colab_type": "text"
      },
      "source": [
        "### We have now applied Image Normalization and L2 kernel regularization for the network . We also tested and found that applying BatchNormalization before or after ReLU activation didn't make a huge difference for this MNIST dataset . We also separated the images that were being wrongly classified and printed the first 25 images from the list \n",
        "\n"
      ]
    }
  ]
}