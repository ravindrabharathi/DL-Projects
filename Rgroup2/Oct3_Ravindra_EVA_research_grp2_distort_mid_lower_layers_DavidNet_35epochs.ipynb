{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Oct3_Ravindra-EVA_research_grp2_distort_mid_lower_layers_DavidNet-35epochs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ravindrabharathi/Project1/blob/master/Rgroup2/Oct3_Ravindra_EVA_research_grp2_distort_mid_lower_layers_DavidNet_35epochs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynqEXKR8Quk9",
        "colab_type": "text"
      },
      "source": [
        "# Effect of distortions/transformations in mid/lower layers of a Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndxyk_6t6FMY",
        "colab_type": "text"
      },
      "source": [
        "This exercise is to find out of there is any beneficial effect due to  performing distortions/transformation on the activation outputs of mid/lower layers (similar to augmenations on input images ) of a convolutional Neural Network . We will look at gain in test accuracy , regularization effects as seen in smaller gaps of train vs test accuracy , etc. Here we are using a reference model loosely based on the one defined by David Page in his DawnBench submission and explained very well in his series of blog posts (insert link here) . The Model is essentially a 3 block , nine layer modified Resnet Model .\n",
        "\n",
        "We will compare the accuracies achieved using the following methods \n",
        "\n",
        "1. Base Model with image augmnetation on input images alone \n",
        "2. Base Model without input image augmenation but with distortions after first resnet block \n",
        "3. Base Model without input image augmenation but with distortions after second resnet block \n",
        "4. Base Model without input image augmenation but with distortions after first and second resnet blocks \n",
        "5. Base Model with input image augmenation and with distortions after first resnet block \n",
        "6. Base Model without input image augmenation and with distortions after second resnet block \n",
        "\n",
        "Augmentations for input images as well as activation channels will be a combination of pad/random_crop , flip_left_right and cutout "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgLSEQaclSMi",
        "colab_type": "text"
      },
      "source": [
        "### install/import tensorflow_addons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3R6SSn4lb2D",
        "colab_type": "code",
        "outputId": "d3e26229-b0f6-45a3-d6f4-f650a316b7e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "!pip install tensorflow==2.0\n",
        "!pip install tensorflow-gpu==2.0\n",
        "!pip install tensorflow_addons\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import math\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/0f/7bd55361168bb32796b360ad15a25de6966c9c1beb58a8e30c01c8279862/tensorflow-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (86.3MB)\n",
            "\u001b[K     |████████████████████████████████| 86.3MB 2.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.11.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (3.7.1)\n",
            "Collecting gast==0.2.2 (from tensorflow==2.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.0.8)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (0.1.7)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (0.8.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.12.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (3.0.1)\n",
            "Collecting tensorflow-estimator<2.1.0,>=2.0.0 (from tensorflow==2.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/00/5e6cdf86190a70d7382d320b2b04e4ff0f8191a37d90a422a2f8ff0705bb/tensorflow_estimator-2.0.0-py2.py3-none-any.whl (449kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 44.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.1.0)\n",
            "Collecting tensorboard<2.1.0,>=2.0.0 (from tensorflow==2.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/a6/e8ffa4e2ddb216449d34cfcb825ebb38206bee5c4553d69e7bc8bc2c5d64/tensorboard-2.0.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 30.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (0.8.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (0.33.6)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.16.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==2.0) (41.2.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0) (2.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (0.16.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (3.1.1)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=ac0e83f03d768e39e520be187d1a3f93a220383aecbcdc51bf41f127620a96aa\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "Installing collected packages: gast, tensorflow-estimator, tensorboard, tensorflow\n",
            "  Found existing installation: gast 0.3.2\n",
            "    Uninstalling gast-0.3.2:\n",
            "      Successfully uninstalled gast-0.3.2\n",
            "  Found existing installation: tensorflow-estimator 1.14.0\n",
            "    Uninstalling tensorflow-estimator-1.14.0:\n",
            "      Successfully uninstalled tensorflow-estimator-1.14.0\n",
            "  Found existing installation: tensorboard 1.14.0\n",
            "    Uninstalling tensorboard-1.14.0:\n",
            "      Successfully uninstalled tensorboard-1.14.0\n",
            "  Found existing installation: tensorflow 1.14.0\n",
            "    Uninstalling tensorflow-1.14.0:\n",
            "      Successfully uninstalled tensorflow-1.14.0\n",
            "Successfully installed gast-0.2.2 tensorboard-2.0.0 tensorflow-2.0.0 tensorflow-estimator-2.0.0\n",
            "Collecting tensorflow-gpu==2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/44/47f0722aea081697143fbcf5d2aa60d1aee4aaacb5869aee2b568974777b/tensorflow_gpu-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (380.8MB)\n",
            "\u001b[K     |████████████████████████████████| 380.8MB 75kB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (1.12.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (1.0.8)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (0.8.0)\n",
            "Requirement already satisfied: tensorboard<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (2.0.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (1.11.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (3.0.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (0.1.7)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (2.0.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (0.33.6)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (1.16.5)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (3.7.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (0.8.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (0.2.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==2.0) (2.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (0.16.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (41.2.0)\n",
            "Installing collected packages: tensorflow-gpu\n",
            "Successfully installed tensorflow-gpu-2.0.0\n",
            "Collecting tensorflow_addons\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/b6/574034405ad527eec40ac426081694f50da9766db6b81c2b899041c44ef2/tensorflow_addons-0.5.2-cp36-cp36m-manylinux2010_x86_64.whl (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-gpu==2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_addons) (2.0.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_addons) (1.12.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (0.33.6)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (1.16.5)\n",
            "Requirement already satisfied: tensorflow-estimator<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (2.0.0)\n",
            "Requirement already satisfied: tensorboard<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (2.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (3.0.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (3.7.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (1.11.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (0.8.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (0.8.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (1.15.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (0.1.7)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (0.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0->tensorflow_addons) (0.16.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0->tensorflow_addons) (3.1.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0->tensorflow_addons) (41.2.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==2.0.0->tensorflow_addons) (2.8.0)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.5.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFPFRpPDQ2e6",
        "colab_type": "text"
      },
      "source": [
        "### Install tf_utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0yHfmeMv-rI",
        "colab_type": "code",
        "outputId": "3bc6cca9-b13f-449c-813e-4a2bc18ecd29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "!pip install --upgrade git+https://github.com/ravindrabharathi/tf_utils "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/ravindrabharathi/tf_utils\n",
            "  Cloning https://github.com/ravindrabharathi/tf_utils to /tmp/pip-req-build-ou4mq2q5\n",
            "  Running command git clone -q https://github.com/ravindrabharathi/tf_utils /tmp/pip-req-build-ou4mq2q5\n",
            "Building wheels for collected packages: tf-utils\n",
            "  Building wheel for tf-utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tf-utils: filename=tf_utils-0.2-cp36-none-any.whl size=8945 sha256=9532eceb8ecfb959ea5d6394d404bc8f6936f2e94889fb0ea90e00be82953c7d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-y9b6nnef/wheels/95/af/bb/690b94c65a5aad47a5c39e75f158a2b043448e908c5c121791\n",
            "Successfully built tf-utils\n",
            "Installing collected packages: tf-utils\n",
            "Successfully installed tf-utils-0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VH-5ffTMQ8ms",
        "colab_type": "text"
      },
      "source": [
        "### import the data module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBcelJMpwYoX",
        "colab_type": "code",
        "outputId": "1b9679dd-36ed-49c1-cc1e-a5d71a2a0701",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tf_utils.data as ds"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished 'get_cpu_num' in 0.0000 secs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vb0kbJalRCPJ",
        "colab_type": "text"
      },
      "source": [
        "### set batch size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJTA3CO8xOtg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size=512\n",
        "ds.batch_size=batch_size\n",
        "EPOCHS=35"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTroxaquRE2Z",
        "colab_type": "text"
      },
      "source": [
        "### downlaod data and create tf records"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zF3qoDnbykb6",
        "colab_type": "code",
        "outputId": "e9481bae-40b8-43c5-d0b7-b9dd1ead4692",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        }
      },
      "source": [
        "ds.get_cifar10_and_create_tfrecords()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a153db5c6cbb499cad63eb8d91fbab3d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='cifar-10-python.tar.gz', max=170498071, style=ProgressStyle(d…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Finished 'download_file' in 2.1123 secs\n",
            "Finished 'download_cifar10_files' in 2.1129 secs\n",
            "Done\n",
            "Finished 'extract_cifar10_files' in 1.8917 secs\n",
            "Finished '_get_file_names' in 0.0000 secs\n",
            "Generating ./train.tfrecords\n",
            "Finished 'read_pickle_from_file' in 0.1525 secs\n",
            "Finished 'read_pickle_from_file' in 0.1471 secs\n",
            "Finished 'read_pickle_from_file' in 0.1448 secs\n",
            "Finished 'read_pickle_from_file' in 0.1364 secs\n",
            "Finished 'read_pickle_from_file' in 0.1357 secs\n",
            "Finished 'convert_to_tfrecord' in 3.1199 secs\n",
            "Done!\n",
            "Generating ./eval.tfrecords\n",
            "Finished 'read_pickle_from_file' in 0.1367 secs\n",
            "Finished 'convert_to_tfrecord' in 0.6289 secs\n",
            "Done!\n",
            "Finished 'create_tf_records' in 3.7512 secs\n",
            "Finished 'get_cifar10_and_create_tfrecords' in 7.7563 secs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7U-_CmC6RKDF",
        "colab_type": "text"
      },
      "source": [
        "### create train and test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lYOp7UKyoG5",
        "colab_type": "code",
        "outputId": "6cfc5dc5-0bf4-4cdb-87d1-bf36cb893063",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "def aug_fn(image):\n",
        "  return ds.cutout(ds.flip_left_right(ds.random_pad_crop(image)))\n",
        "\n",
        "train_ds1=ds.get_train_ds(batch_size=batch_size,shuffle=True,distort=True,distort_fn=aug_fn)\n",
        "train_ds2=ds.get_train_ds(batch_size=batch_size,shuffle=True,distort=False)\n",
        "\n",
        "test_ds=ds.get_eval_ds(batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "distorting...\n",
            "Finished 'get_tf_dataset_2' in 2.9924 secs\n",
            "Finished 'get_tf_dataset_in_batches' in 2.9928 secs\n",
            "Finished 'get_train_ds' in 2.9929 secs\n",
            "Finished 'get_tf_dataset' in 0.1579 secs\n",
            "Finished 'get_tf_dataset_in_batches' in 0.1582 secs\n",
            "Finished 'get_train_ds' in 0.1583 secs\n",
            "Finished 'get_tf_dataset' in 0.0263 secs\n",
            "Finished 'get_tf_dataset_in_batches' in 0.0266 secs\n",
            "Finished 'get_eval_ds' in 0.0267 secs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7z9t4e5RRbH",
        "colab_type": "text"
      },
      "source": [
        "### import visualization module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHq2g5VjzmDz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tf_utils.visualize as vz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utItaz0RRXft",
        "colab_type": "text"
      },
      "source": [
        "### plot images from train dataset1 , train dataset by default uses image augmenttation of cutout,flip-left-right,random-pad-crop "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvonf0IP0uOX",
        "colab_type": "code",
        "outputId": "16c8385b-7bac-4e2c-b1fa-0ff6924b50fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "vz.plot_cifar10_files(train_ds1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ng5+QSGHwExIp/wdWPdU94Kr3MwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nI42wAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFPxCZMr/Bey8fnT+3LiWAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJ36BEXl2AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKnsdKUUovds",
        "colab_type": "code",
        "outputId": "4c4856d1-6947-45f6-8719-f9ddbe30d3ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "vz.plot_cifar10_files(train_ds2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYkh+K0QkQ8sAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSBQ5vxCJIucXIlHk/EIkipxfiET5vwq3TnG+VpVvAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qN5z9vjRsvL",
        "colab_type": "text"
      },
      "source": [
        "### Training - Build model , compile and train "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m26XDkJi1YyS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, GlobalAveragePooling2D,  Activation, GlobalMaxPooling2D\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization , Dense, Lambda\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam,SGD\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "WEIGHT_DECAY=1.25e-4\n",
        "reg=tf.keras.regularizers.l2(WEIGHT_DECAY)\n",
        "def init_pytorch(shape, dtype=tf.float32, partition_info=None):\n",
        "  fan = np.prod(shape[:-1])\n",
        "  bound = 1 / math.sqrt(fan)\n",
        "  return tf.random.uniform(shape, minval=-bound, maxval=bound, dtype=dtype)\n",
        "\n",
        "def conv(inp,f=32,k=3):\n",
        "  conv_layer=Conv2D(f,k,use_bias=False,padding='same',kernel_initializer=init_pytorch, kernel_regularizer=reg)(inp)\n",
        "  conv_layer=BatchNormalization(momentum=0.9, epsilon=1e-5)(conv_layer)\n",
        "  conv_layer=Activation('relu')(conv_layer)\n",
        "  return conv_layer\n",
        "def resBlk(inp,f=32,k=3,residual=True) :\n",
        "  res1=conv(inp,f,k)\n",
        "  res1=MaxPooling2D(pool_size=(2,2))(res1)\n",
        "  if residual:\n",
        "    res2=conv(res1,f,k)\n",
        "    res3=conv(res2,f,k)\n",
        "    return res1+res3\n",
        "  else:\n",
        "    return res1  \n",
        "\n",
        "def apply_weight(x):\n",
        "  return x*0.125  \n",
        "\n",
        "def random_pad_crop(image,padding=2):\n",
        "  shp=tf.shape(image)\n",
        "  \n",
        "  image=tf.pad(image,[(0, 0), (padding, padding), (padding, padding), (0, 0)], mode='reflect')\n",
        "  \n",
        "  image=tf.image.random_crop(image,size=shp)\n",
        "  return image  \n",
        "\n",
        "def flip_left_right(image):\n",
        "  return tf.image.random_flip_left_right(image)   \n",
        "\n",
        "def aug_fn1(img):\n",
        "  shp=img.get_shape().as_list()\n",
        "  out_shp=[shp[0],shp[1],shp[2]]\n",
        "  return ds.flip_left_right(ds.random_crop(ds.cutout(ds.pad_img(img,padding=2),100,size=4),out_shape=out_shp))\n",
        "\n",
        "def aug_fn2(img):\n",
        "  shp=img.get_shape().as_list()\n",
        "  out_shp=[shp[0],shp[1],shp[2]]\n",
        "  return ds.flip_left_right(ds.random_crop(ds.cutout(ds.pad_img(img,padding=1),100,size=2),out_shape=out_shp))\n",
        "\n",
        "def aug1(image):\n",
        "  print('is_training',is_training)\n",
        "  if is_training:\n",
        "    \n",
        "    #return tf.map_fn(lambda img: aug_fn1(img),image,parallel_iterations=ds.CPU_CORES,back_prop=is_training) \n",
        "    return tf.map_fn(lambda img: aug_fn1(img) ,image,parallel_iterations=10*ds.CPU_CORES,back_prop=is_training)    \n",
        "  else:\n",
        "    print('inside validation cycle\\n===============\\n')\n",
        "    return image  \n",
        "\n",
        "def aug2(image):\n",
        "  \n",
        "  print('is_training',is_training)\n",
        "  if is_training:\n",
        "    \n",
        "    return tf.map_fn(lambda img: aug_fn2(img) ,image,parallel_iterations=10*ds.CPU_CORES,back_prop=is_training)\n",
        "  else:\n",
        "    print('inside validation cycle\\n===============\\n')\n",
        "    return image        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVqdMyzkx4tY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(distort_param=0):\n",
        "  f=64\n",
        "  inp=Input(shape=(32,32,3))\n",
        "  layer1=conv(inp,f,3)\n",
        "  res1=resBlk(layer1,f*2,3)\n",
        "  if (distort_param in [1,3,4]):\n",
        "    res1=Lambda(aug1)(res1)\n",
        "  res2=resBlk(res1,f*4,3,False)\n",
        "  if (distort_param in [2,3,5]):\n",
        "    res2=Lambda(aug2)(res2)\n",
        "  res3=resBlk(res2,f*8,3)\n",
        "  \n",
        "  layer2=GlobalMaxPooling2D()(res3)\n",
        "  layer3=Dense(10, kernel_initializer=init_pytorch, use_bias=False,kernel_regularizer=reg)(layer2)\n",
        "  layer4=Lambda(lambda x: x*0.125)(layer3)\n",
        "  out=Activation('softmax')(layer4)\n",
        "  model=Model(inputs=[inp],outputs=[out])\n",
        "  model.summary()\n",
        "  return model "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nng9bFm1rQq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "\n",
        "def lr_schedule():\n",
        "    \n",
        "    def schedule(epoch):\n",
        "\n",
        "      lr=lr1=np.interp([epoch],[0, EPOCHS//5,EPOCHS], [0.025, 0.4, 0])[0]\n",
        "      print('epoch ', epoch+1, ': setting learning rate to ',lr1)\n",
        "      return lr\n",
        "    \n",
        "    return LearningRateScheduler(schedule)\n",
        "\n",
        "lr_sched = lr_schedule()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaDK2E2h81vV",
        "colab_type": "code",
        "outputId": "6874510d-e29e-473b-9f93-69ba5e3d63cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "for model_params in [5,4,3,2,1,0]:\n",
        "  is_training=True\n",
        "  #global_step = tf.train.get_or_create_global_step()\n",
        "  model=model=build_model(model_params)\n",
        "  opt=SGD(lr=0.025,momentum=0.9,nesterov=True)\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,metrics=['accuracy']\n",
        "              )\n",
        "  batch_size=512\n",
        "  \n",
        "  if model_params in [0,4,5]:\n",
        "    train_ds=train_ds1\n",
        "    print('Model will be trained with image augmentation in put layer\\n====================')\n",
        "    if model_params==4:\n",
        "      print('Model will also include distortion after Res Blk 1\\n=======================')\n",
        "    elif model_params==5:\n",
        "      print('Model will also include distortion after Res Blk 2\\n=======================')\n",
        "      \n",
        "  else:\n",
        "    if model_params==1:\n",
        "      print('Model will be trained with distortion after Res Blk 1\\n=======================')\n",
        "    elif model_params==2:\n",
        "      print('Model will be trained with distortion after Res Blk 2\\n=======================')\n",
        "    elif model_params==3:\n",
        "      print('Model will be trained with distortion after Res Blk 1 and Res Blk 2\\n=======================')    \n",
        "    train_ds=train_ds2  \n",
        "  model.fit(train_ds,epochs=EPOCHS, steps_per_epoch=np.ceil(50000/batch_size), \n",
        "          callbacks=[lr_sched],\n",
        "          verbose=1)\n",
        "  is_training=False\n",
        "  score=model.evaluate(test_ds, steps =np.ceil(10000/batch_size), verbose=1)\n",
        "\n",
        "  del(model)\n",
        "  del(train_ds)\n",
        "  \n",
        "  print('val accuracy score at the end of training model type ',model_params, score)\n",
        "  print(\"=========================================\\n\")\n",
        "\n",
        "#validation_data=test_ds, validation_steps=np.ceil(10000/batch_size),"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "is_training True\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 32, 32, 64)   1728        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 32, 32, 64)   256         conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 32, 32, 64)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 128)  73728       activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 128)  512         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 128)  0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 16, 16, 128)  0           activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 16, 16, 128)  147456      max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 16, 16, 128)  512         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 16, 16, 128)  0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 16, 16, 128)  147456      activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 16, 16, 128)  512         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 16, 16, 128)  0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add (TensorFlowOpLa [(None, 16, 16, 128) 0           max_pooling2d[0][0]              \n",
            "                                                                 activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 16, 16, 256)  294912      tf_op_layer_add[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 16, 16, 256)  1024        conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 16, 16, 256)  0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 8, 8, 256)    0           activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, 8, 8, 256)    0           max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 8, 8, 512)    1179648     lambda[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 8, 8, 512)    2048        conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 8, 8, 512)    0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 4, 4, 512)    0           activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 4, 4, 512)    2359296     max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 4, 4, 512)    2048        conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 4, 4, 512)    0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 4, 4, 512)    2359296     activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 4, 4, 512)    2048        conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 4, 4, 512)    0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_1 (TensorFlowOp [(None, 4, 4, 512)]  0           max_pooling2d_2[0][0]            \n",
            "                                                                 activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling2d (GlobalMax (None, 512)          0           tf_op_layer_add_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 10)           5120        global_max_pooling2d[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 10)           0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 10)           0           lambda_1[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 6,577,600\n",
            "Trainable params: 6,573,120\n",
            "Non-trainable params: 4,480\n",
            "__________________________________________________________________________________________________\n",
            "Model will be trained with image augmentation in put layer\n",
            "====================\n",
            "Model will also include distortion after Res Blk 2\n",
            "=======================\n",
            "Train for 98.0 steps\n",
            "epoch  1 : setting learning rate to  0.025\n",
            "Epoch 1/35\n",
            "is_training True\n",
            "is_training True\n",
            "98/98 [==============================] - 166s 2s/step - loss: 1.7338 - accuracy: 0.4046\n",
            "epoch  2 : setting learning rate to  0.07857142857142857\n",
            "Epoch 2/35\n",
            "98/98 [==============================] - 155s 2s/step - loss: 1.3590 - accuracy: 0.5452\n",
            "epoch  3 : setting learning rate to  0.13214285714285715\n",
            "Epoch 3/35\n",
            "98/98 [==============================] - 156s 2s/step - loss: 1.0542 - accuracy: 0.6620\n",
            "epoch  4 : setting learning rate to  0.1857142857142857\n",
            "Epoch 4/35\n",
            "98/98 [==============================] - 155s 2s/step - loss: 0.9178 - accuracy: 0.7173\n",
            "epoch  5 : setting learning rate to  0.23928571428571427\n",
            "Epoch 5/35\n",
            "98/98 [==============================] - 156s 2s/step - loss: 0.8572 - accuracy: 0.7460\n",
            "epoch  6 : setting learning rate to  0.29285714285714287\n",
            "Epoch 6/35\n",
            "98/98 [==============================] - 155s 2s/step - loss: 0.7988 - accuracy: 0.7750\n",
            "epoch  7 : setting learning rate to  0.3464285714285714\n",
            "Epoch 7/35\n",
            "98/98 [==============================] - 155s 2s/step - loss: 0.7809 - accuracy: 0.7888\n",
            "epoch  8 : setting learning rate to  0.4\n",
            "Epoch 8/35\n",
            "98/98 [==============================] - 155s 2s/step - loss: 0.7741 - accuracy: 0.7995\n",
            "epoch  9 : setting learning rate to  0.38571428571428573\n",
            "Epoch 9/35\n",
            "98/98 [==============================] - 155s 2s/step - loss: 0.7079 - accuracy: 0.8268\n",
            "epoch  10 : setting learning rate to  0.37142857142857144\n",
            "Epoch 10/35\n",
            "98/98 [==============================] - 154s 2s/step - loss: 0.6786 - accuracy: 0.8398\n",
            "epoch  11 : setting learning rate to  0.35714285714285715\n",
            "Epoch 11/35\n",
            "98/98 [==============================] - 154s 2s/step - loss: 0.6480 - accuracy: 0.8529\n",
            "epoch  12 : setting learning rate to  0.34285714285714286\n",
            "Epoch 12/35\n",
            "98/98 [==============================] - 153s 2s/step - loss: 0.6272 - accuracy: 0.8601\n",
            "epoch  13 : setting learning rate to  0.32857142857142857\n",
            "Epoch 13/35\n",
            "98/98 [==============================] - 155s 2s/step - loss: 0.6018 - accuracy: 0.8698\n",
            "epoch  14 : setting learning rate to  0.3142857142857143\n",
            "Epoch 14/35\n",
            "98/98 [==============================] - 155s 2s/step - loss: 0.5881 - accuracy: 0.8757\n",
            "epoch  15 : setting learning rate to  0.30000000000000004\n",
            "Epoch 15/35\n",
            "98/98 [==============================] - 155s 2s/step - loss: 0.5695 - accuracy: 0.8841\n",
            "epoch  16 : setting learning rate to  0.2857142857142857\n",
            "Epoch 16/35\n",
            "98/98 [==============================] - 154s 2s/step - loss: 0.5560 - accuracy: 0.8880\n",
            "epoch  17 : setting learning rate to  0.27142857142857146\n",
            "Epoch 17/35\n",
            "98/98 [==============================] - 155s 2s/step - loss: 0.5420 - accuracy: 0.8911\n",
            "epoch  18 : setting learning rate to  0.2571428571428571\n",
            "Epoch 18/35\n",
            "98/98 [==============================] - 156s 2s/step - loss: 0.5241 - accuracy: 0.8973\n",
            "epoch  19 : setting learning rate to  0.24285714285714285\n",
            "Epoch 19/35\n",
            "98/98 [==============================] - 155s 2s/step - loss: 0.5133 - accuracy: 0.9019\n",
            "epoch  20 : setting learning rate to  0.2285714285714286\n",
            "Epoch 20/35\n",
            "98/98 [==============================] - 153s 2s/step - loss: 0.4917 - accuracy: 0.9082\n",
            "epoch  21 : setting learning rate to  0.2142857142857143\n",
            "Epoch 21/35\n",
            "98/98 [==============================] - 154s 2s/step - loss: 0.4833 - accuracy: 0.9111\n",
            "epoch  22 : setting learning rate to  0.2\n",
            "Epoch 22/35\n",
            "98/98 [==============================] - 153s 2s/step - loss: 0.4638 - accuracy: 0.9179\n",
            "epoch  23 : setting learning rate to  0.18571428571428572\n",
            "Epoch 23/35\n",
            "98/98 [==============================] - 154s 2s/step - loss: 0.4522 - accuracy: 0.9212\n",
            "epoch  24 : setting learning rate to  0.17142857142857143\n",
            "Epoch 24/35\n",
            "98/98 [==============================] - 152s 2s/step - loss: 0.4329 - accuracy: 0.9262\n",
            "epoch  25 : setting learning rate to  0.15714285714285714\n",
            "Epoch 25/35\n",
            "98/98 [==============================] - 154s 2s/step - loss: 0.4179 - accuracy: 0.9319\n",
            "epoch  26 : setting learning rate to  0.14285714285714285\n",
            "Epoch 26/35\n",
            "98/98 [==============================] - 154s 2s/step - loss: 0.3999 - accuracy: 0.9378\n",
            "epoch  27 : setting learning rate to  0.12857142857142856\n",
            "Epoch 27/35\n",
            "98/98 [==============================] - 155s 2s/step - loss: 0.3864 - accuracy: 0.9408\n",
            "epoch  28 : setting learning rate to  0.11428571428571427\n",
            "Epoch 28/35\n",
            "98/98 [==============================] - 154s 2s/step - loss: 0.3654 - accuracy: 0.9458\n",
            "epoch  29 : setting learning rate to  0.09999999999999998\n",
            "Epoch 29/35\n",
            "98/98 [==============================] - 155s 2s/step - loss: 0.3520 - accuracy: 0.9494\n",
            "epoch  30 : setting learning rate to  0.08571428571428569\n",
            "Epoch 30/35\n",
            "98/98 [==============================] - 154s 2s/step - loss: 0.3334 - accuracy: 0.9548\n",
            "epoch  31 : setting learning rate to  0.0714285714285714\n",
            "Epoch 31/35\n",
            "98/98 [==============================] - 154s 2s/step - loss: 0.3193 - accuracy: 0.9601\n",
            "epoch  32 : setting learning rate to  0.05714285714285716\n",
            "Epoch 32/35\n",
            "98/98 [==============================] - 153s 2s/step - loss: 0.3026 - accuracy: 0.9640\n",
            "epoch  33 : setting learning rate to  0.04285714285714287\n",
            "Epoch 33/35\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.2886 - accuracy: 0.9683\n",
            "epoch  34 : setting learning rate to  0.02857142857142858\n",
            "Epoch 34/35\n",
            "98/98 [==============================] - 155s 2s/step - loss: 0.2728 - accuracy: 0.9729\n",
            "epoch  35 : setting learning rate to  0.01428571428571429\n",
            "Epoch 35/35\n",
            "98/98 [==============================] - 155s 2s/step - loss: 0.2617 - accuracy: 0.9770\n",
            "is_training False\n",
            "inside validation cycle\n",
            "===============\n",
            "\n",
            "20/20 [==============================] - 3s 138ms/step - loss: 0.3933 - accuracy: 0.9380\n",
            "val accuracy score at the end of training model type  5 [0.39331924766302107, 0.9379883]\n",
            "=========================================\n",
            "\n",
            "is_training True\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 32, 32, 64)   1728        input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 32, 32, 64)   256         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 32, 32, 64)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 32, 32, 128)  73728       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 32, 32, 128)  512         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 32, 32, 128)  0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 16, 16, 128)  0           activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 16, 16, 128)  147456      max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 16, 16, 128)  512         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 16, 16, 128)  0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 16, 16, 128)  147456      activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 16, 16, 128)  512         conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 16, 16, 128)  0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_2 (TensorFlowOp [(None, 16, 16, 128) 0           max_pooling2d_3[0][0]            \n",
            "                                                                 activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_2 (Lambda)               (None, 16, 16, 128)  0           tf_op_layer_add_2[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 16, 16, 256)  294912      lambda_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 16, 16, 256)  1024        conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 16, 16, 256)  0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 8, 8, 256)    0           activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 8, 8, 512)    1179648     max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 8, 8, 512)    2048        conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 8, 8, 512)    0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2D)  (None, 4, 4, 512)    0           activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 4, 4, 512)    2359296     max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 4, 4, 512)    2048        conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 4, 4, 512)    0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 4, 4, 512)    2359296     activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 4, 4, 512)    2048        conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 4, 4, 512)    0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_3 (TensorFlowOp [(None, 4, 4, 512)]  0           max_pooling2d_5[0][0]            \n",
            "                                                                 activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling2d_1 (GlobalM (None, 512)          0           tf_op_layer_add_3[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 10)           5120        global_max_pooling2d_1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "lambda_3 (Lambda)               (None, 10)           0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 10)           0           lambda_3[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 6,577,600\n",
            "Trainable params: 6,573,120\n",
            "Non-trainable params: 4,480\n",
            "__________________________________________________________________________________________________\n",
            "Model will be trained with image augmentation in put layer\n",
            "====================\n",
            "Model will also include distortion after Res Blk 1\n",
            "=======================\n",
            "Train for 98.0 steps\n",
            "epoch  1 : setting learning rate to  0.025\n",
            "Epoch 1/35\n",
            "is_training True\n",
            "is_training True\n",
            "98/98 [==============================] - 161s 2s/step - loss: 1.7429 - accuracy: 0.3974\n",
            "epoch  2 : setting learning rate to  0.07857142857142857\n",
            "Epoch 2/35\n",
            "98/98 [==============================] - 159s 2s/step - loss: 1.3786 - accuracy: 0.5390\n",
            "epoch  3 : setting learning rate to  0.13214285714285715\n",
            "Epoch 3/35\n",
            "98/98 [==============================] - 158s 2s/step - loss: 1.0579 - accuracy: 0.6592\n",
            "epoch  4 : setting learning rate to  0.1857142857142857\n",
            "Epoch 4/35\n",
            "98/98 [==============================] - 158s 2s/step - loss: 0.9295 - accuracy: 0.7135\n",
            "epoch  5 : setting learning rate to  0.23928571428571427\n",
            "Epoch 5/35\n",
            "98/98 [==============================] - 158s 2s/step - loss: 0.8532 - accuracy: 0.7467\n",
            "epoch  6 : setting learning rate to  0.29285714285714287\n",
            "Epoch 6/35\n",
            "98/98 [==============================] - 158s 2s/step - loss: 0.8129 - accuracy: 0.7703\n",
            "epoch  7 : setting learning rate to  0.3464285714285714\n",
            "Epoch 7/35\n",
            "98/98 [==============================] - 158s 2s/step - loss: 0.7902 - accuracy: 0.7847\n",
            "epoch  8 : setting learning rate to  0.4\n",
            "Epoch 8/35\n",
            "98/98 [==============================] - 158s 2s/step - loss: 0.7701 - accuracy: 0.8010\n",
            "epoch  9 : setting learning rate to  0.38571428571428573\n",
            "Epoch 9/35\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.7134 - accuracy: 0.8240\n",
            "epoch  10 : setting learning rate to  0.37142857142857144\n",
            "Epoch 10/35\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.6869 - accuracy: 0.8342\n",
            "epoch  11 : setting learning rate to  0.35714285714285715\n",
            "Epoch 11/35\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.6552 - accuracy: 0.8496\n",
            "epoch  12 : setting learning rate to  0.34285714285714286\n",
            "Epoch 12/35\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.6317 - accuracy: 0.8589\n",
            "epoch  13 : setting learning rate to  0.32857142857142857\n",
            "Epoch 13/35\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.6075 - accuracy: 0.8687\n",
            "epoch  14 : setting learning rate to  0.3142857142857143\n",
            "Epoch 14/35\n",
            "98/98 [==============================] - 158s 2s/step - loss: 0.5974 - accuracy: 0.8719\n",
            "epoch  15 : setting learning rate to  0.30000000000000004\n",
            "Epoch 15/35\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.5792 - accuracy: 0.8793\n",
            "epoch  16 : setting learning rate to  0.2857142857142857\n",
            "Epoch 16/35\n",
            "98/98 [==============================] - 158s 2s/step - loss: 0.5575 - accuracy: 0.8874\n",
            "epoch  17 : setting learning rate to  0.27142857142857146\n",
            "Epoch 17/35\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.5407 - accuracy: 0.8943\n",
            "epoch  18 : setting learning rate to  0.2571428571428571\n",
            "Epoch 18/35\n",
            "98/98 [==============================] - 158s 2s/step - loss: 0.5258 - accuracy: 0.8979\n",
            "epoch  19 : setting learning rate to  0.24285714285714285\n",
            "Epoch 19/35\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.5098 - accuracy: 0.9024\n",
            "epoch  20 : setting learning rate to  0.2285714285714286\n",
            "Epoch 20/35\n",
            "98/98 [==============================] - 158s 2s/step - loss: 0.4953 - accuracy: 0.9079\n",
            "epoch  21 : setting learning rate to  0.2142857142857143\n",
            "Epoch 21/35\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.4858 - accuracy: 0.9119\n",
            "epoch  22 : setting learning rate to  0.2\n",
            "Epoch 22/35\n",
            "98/98 [==============================] - 158s 2s/step - loss: 0.4695 - accuracy: 0.9152\n",
            "epoch  23 : setting learning rate to  0.18571428571428572\n",
            "Epoch 23/35\n",
            "98/98 [==============================] - 158s 2s/step - loss: 0.4541 - accuracy: 0.9196\n",
            "epoch  24 : setting learning rate to  0.17142857142857143\n",
            "Epoch 24/35\n",
            "98/98 [==============================] - 158s 2s/step - loss: 0.4357 - accuracy: 0.9264\n",
            "epoch  25 : setting learning rate to  0.15714285714285714\n",
            "Epoch 25/35\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.4221 - accuracy: 0.9289\n",
            "epoch  26 : setting learning rate to  0.14285714285714285\n",
            "Epoch 26/35\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.4047 - accuracy: 0.9345\n",
            "epoch  27 : setting learning rate to  0.12857142857142856\n",
            "Epoch 27/35\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.3863 - accuracy: 0.9397\n",
            "epoch  28 : setting learning rate to  0.11428571428571427\n",
            "Epoch 28/35\n",
            "98/98 [==============================] - 158s 2s/step - loss: 0.3673 - accuracy: 0.9458\n",
            "epoch  29 : setting learning rate to  0.09999999999999998\n",
            "Epoch 29/35\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.3574 - accuracy: 0.9483\n",
            "epoch  30 : setting learning rate to  0.08571428571428569\n",
            "Epoch 30/35\n",
            "98/98 [==============================] - 158s 2s/step - loss: 0.3365 - accuracy: 0.9544\n",
            "epoch  31 : setting learning rate to  0.0714285714285714\n",
            "Epoch 31/35\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.3165 - accuracy: 0.9606\n",
            "epoch  32 : setting learning rate to  0.05714285714285716\n",
            "Epoch 32/35\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.3040 - accuracy: 0.9643\n",
            "epoch  33 : setting learning rate to  0.04285714285714287\n",
            "Epoch 33/35\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.2868 - accuracy: 0.9683\n",
            "epoch  34 : setting learning rate to  0.02857142857142858\n",
            "Epoch 34/35\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.2740 - accuracy: 0.9730\n",
            "epoch  35 : setting learning rate to  0.01428571428571429\n",
            "Epoch 35/35\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.2631 - accuracy: 0.9762\n",
            "is_training False\n",
            "inside validation cycle\n",
            "===============\n",
            "\n",
            "20/20 [==============================] - 3s 138ms/step - loss: 0.3903 - accuracy: 0.9366\n",
            "val accuracy score at the end of training model type  4 [0.39025679528713225, 0.93662107]\n",
            "=========================================\n",
            "\n",
            "is_training True\n",
            "is_training True\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 32, 32, 64)   1728        input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 32, 32, 64)   256         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 32, 32, 64)   0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 32, 32, 128)  73728       activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 32, 32, 128)  512         conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 32, 32, 128)  0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2D)  (None, 16, 16, 128)  0           activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 16, 16, 128)  147456      max_pooling2d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 16, 16, 128)  512         conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 16, 16, 128)  0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 16, 16, 128)  147456      activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 16, 16, 128)  512         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 16, 16, 128)  0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_4 (TensorFlowOp [(None, 16, 16, 128) 0           max_pooling2d_6[0][0]            \n",
            "                                                                 activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_4 (Lambda)               (None, 16, 16, 128)  0           tf_op_layer_add_4[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 16, 16, 256)  294912      lambda_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 16, 16, 256)  1024        conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 16, 16, 256)  0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2D)  (None, 8, 8, 256)    0           activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_5 (Lambda)               (None, 8, 8, 256)    0           max_pooling2d_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 8, 8, 512)    1179648     lambda_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 8, 8, 512)    2048        conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 8, 8, 512)    0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2D)  (None, 4, 4, 512)    0           activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 4, 4, 512)    2359296     max_pooling2d_8[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 4, 4, 512)    2048        conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 4, 4, 512)    0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 4, 4, 512)    2359296     activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 4, 4, 512)    2048        conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 4, 4, 512)    0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_5 (TensorFlowOp [(None, 4, 4, 512)]  0           max_pooling2d_8[0][0]            \n",
            "                                                                 activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling2d_2 (GlobalM (None, 512)          0           tf_op_layer_add_5[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 10)           5120        global_max_pooling2d_2[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "lambda_6 (Lambda)               (None, 10)           0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 10)           0           lambda_6[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 6,577,600\n",
            "Trainable params: 6,573,120\n",
            "Non-trainable params: 4,480\n",
            "__________________________________________________________________________________________________\n",
            "Model will be trained with distortion after Res Blk 1 and Res Blk 2\n",
            "=======================\n",
            "Train for 98.0 steps\n",
            "epoch  1 : setting learning rate to  0.025\n",
            "Epoch 1/35\n",
            "is_training True\n",
            "is_training True\n",
            "is_training True\n",
            "is_training True\n",
            "98/98 [==============================] - 256s 3s/step - loss: 1.6411 - accuracy: 0.4367\n",
            "epoch  2 : setting learning rate to  0.07857142857142857\n",
            "Epoch 2/35\n",
            "98/98 [==============================] - 253s 3s/step - loss: 1.1767 - accuracy: 0.6145\n",
            "epoch  3 : setting learning rate to  0.13214285714285715\n",
            "Epoch 3/35\n",
            "98/98 [==============================] - 251s 3s/step - loss: 0.8907 - accuracy: 0.7215\n",
            "epoch  4 : setting learning rate to  0.1857142857142857\n",
            "Epoch 4/35\n",
            "98/98 [==============================] - 249s 3s/step - loss: 0.7708 - accuracy: 0.7728\n",
            "epoch  5 : setting learning rate to  0.23928571428571427\n",
            "Epoch 5/35\n",
            "98/98 [==============================] - 249s 3s/step - loss: 0.7028 - accuracy: 0.8028\n",
            "epoch  6 : setting learning rate to  0.29285714285714287\n",
            "Epoch 6/35\n",
            "98/98 [==============================] - 248s 3s/step - loss: 0.6780 - accuracy: 0.8187\n",
            "epoch  7 : setting learning rate to  0.3464285714285714\n",
            "Epoch 7/35\n",
            "98/98 [==============================] - 250s 3s/step - loss: 0.6425 - accuracy: 0.8359\n",
            "epoch  8 : setting learning rate to  0.4\n",
            "Epoch 8/35\n",
            "98/98 [==============================] - 250s 3s/step - loss: 0.6376 - accuracy: 0.8446\n",
            "epoch  9 : setting learning rate to  0.38571428571428573\n",
            "Epoch 9/35\n",
            "98/98 [==============================] - 250s 3s/step - loss: 0.5848 - accuracy: 0.8683\n",
            "epoch  10 : setting learning rate to  0.37142857142857144\n",
            "Epoch 10/35\n",
            "98/98 [==============================] - 253s 3s/step - loss: 0.5472 - accuracy: 0.8824\n",
            "epoch  11 : setting learning rate to  0.35714285714285715\n",
            "Epoch 11/35\n",
            "98/98 [==============================] - 255s 3s/step - loss: 0.5227 - accuracy: 0.8947\n",
            "epoch  12 : setting learning rate to  0.34285714285714286\n",
            "Epoch 12/35\n",
            "98/98 [==============================] - 252s 3s/step - loss: 0.4958 - accuracy: 0.9052\n",
            "epoch  13 : setting learning rate to  0.32857142857142857\n",
            "Epoch 13/35\n",
            "98/98 [==============================] - 250s 3s/step - loss: 0.4751 - accuracy: 0.9136\n",
            "epoch  14 : setting learning rate to  0.3142857142857143\n",
            "Epoch 14/35\n",
            "98/98 [==============================] - 251s 3s/step - loss: 0.4604 - accuracy: 0.9182\n",
            "epoch  15 : setting learning rate to  0.30000000000000004\n",
            "Epoch 15/35\n",
            "98/98 [==============================] - 249s 3s/step - loss: 0.4332 - accuracy: 0.9298\n",
            "epoch  16 : setting learning rate to  0.2857142857142857\n",
            "Epoch 16/35\n",
            "98/98 [==============================] - 248s 3s/step - loss: 0.4171 - accuracy: 0.9333\n",
            "epoch  17 : setting learning rate to  0.27142857142857146\n",
            "Epoch 17/35\n",
            "98/98 [==============================] - 249s 3s/step - loss: 0.4046 - accuracy: 0.9392\n",
            "epoch  18 : setting learning rate to  0.2571428571428571\n",
            "Epoch 18/35\n",
            "98/98 [==============================] - 248s 3s/step - loss: 0.3910 - accuracy: 0.9426\n",
            "epoch  19 : setting learning rate to  0.24285714285714285\n",
            "Epoch 19/35\n",
            "98/98 [==============================] - 249s 3s/step - loss: 0.3704 - accuracy: 0.9496\n",
            "epoch  20 : setting learning rate to  0.2285714285714286\n",
            "Epoch 20/35\n",
            "98/98 [==============================] - 255s 3s/step - loss: 0.3558 - accuracy: 0.9552\n",
            "epoch  21 : setting learning rate to  0.2142857142857143\n",
            "Epoch 21/35\n",
            "98/98 [==============================] - 250s 3s/step - loss: 0.3433 - accuracy: 0.9573\n",
            "epoch  22 : setting learning rate to  0.2\n",
            "Epoch 22/35\n",
            "98/98 [==============================] - 250s 3s/step - loss: 0.3245 - accuracy: 0.9632\n",
            "epoch  23 : setting learning rate to  0.18571428571428572\n",
            "Epoch 23/35\n",
            "98/98 [==============================] - 250s 3s/step - loss: 0.3102 - accuracy: 0.9668\n",
            "epoch  24 : setting learning rate to  0.17142857142857143\n",
            "Epoch 24/35\n",
            "98/98 [==============================] - 249s 3s/step - loss: 0.2951 - accuracy: 0.9717\n",
            "epoch  25 : setting learning rate to  0.15714285714285714\n",
            "Epoch 25/35\n",
            "98/98 [==============================] - 251s 3s/step - loss: 0.2778 - accuracy: 0.9755\n",
            "epoch  26 : setting learning rate to  0.14285714285714285\n",
            "Epoch 26/35\n",
            "98/98 [==============================] - 249s 3s/step - loss: 0.2617 - accuracy: 0.9799\n",
            "epoch  27 : setting learning rate to  0.12857142857142856\n",
            "Epoch 27/35\n",
            "98/98 [==============================] - 249s 3s/step - loss: 0.2453 - accuracy: 0.9840\n",
            "epoch  28 : setting learning rate to  0.11428571428571427\n",
            "Epoch 28/35\n",
            "98/98 [==============================] - 248s 3s/step - loss: 0.2315 - accuracy: 0.9859\n",
            "epoch  29 : setting learning rate to  0.09999999999999998\n",
            "Epoch 29/35\n",
            "98/98 [==============================] - 251s 3s/step - loss: 0.2201 - accuracy: 0.9886\n",
            "epoch  30 : setting learning rate to  0.08571428571428569\n",
            "Epoch 30/35\n",
            "98/98 [==============================] - 255s 3s/step - loss: 0.2056 - accuracy: 0.9918\n",
            "epoch  31 : setting learning rate to  0.0714285714285714\n",
            "Epoch 31/35\n",
            "98/98 [==============================] - 257s 3s/step - loss: 0.1938 - accuracy: 0.9940\n",
            "epoch  32 : setting learning rate to  0.05714285714285716\n",
            "Epoch 32/35\n",
            "98/98 [==============================] - 257s 3s/step - loss: 0.1851 - accuracy: 0.9952\n",
            "epoch  33 : setting learning rate to  0.04285714285714287\n",
            "Epoch 33/35\n",
            "98/98 [==============================] - 258s 3s/step - loss: 0.1784 - accuracy: 0.9964\n",
            "epoch  34 : setting learning rate to  0.02857142857142858\n",
            "Epoch 34/35\n",
            "98/98 [==============================] - 259s 3s/step - loss: 0.1724 - accuracy: 0.9974\n",
            "epoch  35 : setting learning rate to  0.01428571428571429\n",
            "Epoch 35/35\n",
            "98/98 [==============================] - 260s 3s/step - loss: 0.1701 - accuracy: 0.9979\n",
            "is_training False\n",
            "inside validation cycle\n",
            "===============\n",
            "\n",
            "is_training False\n",
            "inside validation cycle\n",
            "===============\n",
            "\n",
            "20/20 [==============================] - 3s 129ms/step - loss: 0.4876 - accuracy: 0.9159\n",
            "val accuracy score at the end of training model type  3 [0.4875758096575737, 0.915918]\n",
            "=========================================\n",
            "\n",
            "is_training True\n",
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 32, 32, 64)   1728        input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 32, 32, 64)   256         conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 32, 32, 64)   0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 32, 32, 128)  73728       activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 32, 32, 128)  512         conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 32, 32, 128)  0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2D)  (None, 16, 16, 128)  0           activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 16, 16, 128)  147456      max_pooling2d_9[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 16, 16, 128)  512         conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 16, 16, 128)  0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 16, 16, 128)  147456      activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 16, 16, 128)  512         conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 16, 16, 128)  0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_6 (TensorFlowOp [(None, 16, 16, 128) 0           max_pooling2d_9[0][0]            \n",
            "                                                                 activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 16, 16, 256)  294912      tf_op_layer_add_6[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 16, 16, 256)  1024        conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 16, 16, 256)  0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling2D) (None, 8, 8, 256)    0           activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_7 (Lambda)               (None, 8, 8, 256)    0           max_pooling2d_10[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 8, 8, 512)    1179648     lambda_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 8, 8, 512)    2048        conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 8, 8, 512)    0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling2D) (None, 4, 4, 512)    0           activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 4, 4, 512)    2359296     max_pooling2d_11[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 4, 4, 512)    2048        conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 4, 4, 512)    0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 4, 4, 512)    2359296     activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 4, 4, 512)    2048        conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 4, 4, 512)    0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_7 (TensorFlowOp [(None, 4, 4, 512)]  0           max_pooling2d_11[0][0]           \n",
            "                                                                 activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling2d_3 (GlobalM (None, 512)          0           tf_op_layer_add_7[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 10)           5120        global_max_pooling2d_3[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "lambda_8 (Lambda)               (None, 10)           0           dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 10)           0           lambda_8[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 6,577,600\n",
            "Trainable params: 6,573,120\n",
            "Non-trainable params: 4,480\n",
            "__________________________________________________________________________________________________\n",
            "Model will be trained with distortion after Res Blk 2\n",
            "=======================\n",
            "Train for 98.0 steps\n",
            "epoch  1 : setting learning rate to  0.025\n",
            "Epoch 1/35\n",
            "is_training True\n",
            "is_training True\n",
            "98/98 [==============================] - 154s 2s/step - loss: 1.5840 - accuracy: 0.4647\n",
            "epoch  2 : setting learning rate to  0.07857142857142857\n",
            "Epoch 2/35\n",
            "98/98 [==============================] - 151s 2s/step - loss: 1.0978 - accuracy: 0.6434\n",
            "epoch  3 : setting learning rate to  0.13214285714285715\n",
            "Epoch 3/35\n",
            "98/98 [==============================] - 147s 2s/step - loss: 0.8150 - accuracy: 0.7531\n",
            "epoch  4 : setting learning rate to  0.1857142857142857\n",
            "Epoch 4/35\n",
            "98/98 [==============================] - 151s 2s/step - loss: 0.6901 - accuracy: 0.8005\n",
            "epoch  5 : setting learning rate to  0.23928571428571427\n",
            "Epoch 5/35\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.6256 - accuracy: 0.8309\n",
            "epoch  6 : setting learning rate to  0.29285714285714287\n",
            "Epoch 6/35\n",
            "98/98 [==============================] - 151s 2s/step - loss: 0.5853 - accuracy: 0.8501\n",
            "epoch  7 : setting learning rate to  0.3464285714285714\n",
            "Epoch 7/35\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.5659 - accuracy: 0.8648\n",
            "epoch  8 : setting learning rate to  0.4\n",
            "Epoch 8/35\n",
            "98/98 [==============================] - 148s 2s/step - loss: 0.5525 - accuracy: 0.8781\n",
            "epoch  9 : setting learning rate to  0.38571428571428573\n",
            "Epoch 9/35\n",
            "98/98 [==============================] - 145s 1s/step - loss: 0.4986 - accuracy: 0.9030\n",
            "epoch  10 : setting learning rate to  0.37142857142857144\n",
            "Epoch 10/35\n",
            "98/98 [==============================] - 147s 1s/step - loss: 0.4633 - accuracy: 0.9182\n",
            "epoch  11 : setting learning rate to  0.35714285714285715\n",
            "Epoch 11/35\n",
            "98/98 [==============================] - 145s 1s/step - loss: 0.4257 - accuracy: 0.9322\n",
            "epoch  12 : setting learning rate to  0.34285714285714286\n",
            "Epoch 12/35\n",
            "98/98 [==============================] - 147s 2s/step - loss: 0.4120 - accuracy: 0.9391\n",
            "epoch  13 : setting learning rate to  0.32857142857142857\n",
            "Epoch 13/35\n",
            "98/98 [==============================] - 147s 1s/step - loss: 0.3855 - accuracy: 0.9487\n",
            "epoch  14 : setting learning rate to  0.3142857142857143\n",
            "Epoch 14/35\n",
            "98/98 [==============================] - 148s 2s/step - loss: 0.3643 - accuracy: 0.9567\n",
            "epoch  15 : setting learning rate to  0.30000000000000004\n",
            "Epoch 15/35\n",
            "98/98 [==============================] - 147s 1s/step - loss: 0.3466 - accuracy: 0.9631\n",
            "epoch  16 : setting learning rate to  0.2857142857142857\n",
            "Epoch 16/35\n",
            "98/98 [==============================] - 147s 2s/step - loss: 0.3334 - accuracy: 0.9673\n",
            "epoch  17 : setting learning rate to  0.27142857142857146\n",
            "Epoch 17/35\n",
            "98/98 [==============================] - 147s 2s/step - loss: 0.3173 - accuracy: 0.9718\n",
            "epoch  18 : setting learning rate to  0.2571428571428571\n",
            "Epoch 18/35\n",
            "98/98 [==============================] - 147s 2s/step - loss: 0.3054 - accuracy: 0.9740\n",
            "epoch  19 : setting learning rate to  0.24285714285714285\n",
            "Epoch 19/35\n",
            "98/98 [==============================] - 146s 1s/step - loss: 0.2870 - accuracy: 0.9782\n",
            "epoch  20 : setting learning rate to  0.2285714285714286\n",
            "Epoch 20/35\n",
            "98/98 [==============================] - 145s 1s/step - loss: 0.2738 - accuracy: 0.9811\n",
            "epoch  21 : setting learning rate to  0.2142857142857143\n",
            "Epoch 21/35\n",
            "98/98 [==============================] - 145s 1s/step - loss: 0.2657 - accuracy: 0.9819\n",
            "epoch  22 : setting learning rate to  0.2\n",
            "Epoch 22/35\n",
            "98/98 [==============================] - 145s 1s/step - loss: 0.2479 - accuracy: 0.9862\n",
            "epoch  23 : setting learning rate to  0.18571428571428572\n",
            "Epoch 23/35\n",
            "98/98 [==============================] - 145s 1s/step - loss: 0.2297 - accuracy: 0.9896\n",
            "epoch  24 : setting learning rate to  0.17142857142857143\n",
            "Epoch 24/35\n",
            "98/98 [==============================] - 146s 1s/step - loss: 0.2133 - accuracy: 0.9926\n",
            "epoch  25 : setting learning rate to  0.15714285714285714\n",
            "Epoch 25/35\n",
            "98/98 [==============================] - 146s 1s/step - loss: 0.2009 - accuracy: 0.9940\n",
            "epoch  26 : setting learning rate to  0.14285714285714285\n",
            "Epoch 26/35\n",
            "98/98 [==============================] - 146s 1s/step - loss: 0.1865 - accuracy: 0.9957\n",
            "epoch  27 : setting learning rate to  0.12857142857142856\n",
            "Epoch 27/35\n",
            "98/98 [==============================] - 147s 2s/step - loss: 0.1757 - accuracy: 0.9964\n",
            "epoch  28 : setting learning rate to  0.11428571428571427\n",
            "Epoch 28/35\n",
            "98/98 [==============================] - 146s 1s/step - loss: 0.1651 - accuracy: 0.9978\n",
            "epoch  29 : setting learning rate to  0.09999999999999998\n",
            "Epoch 29/35\n",
            "98/98 [==============================] - 146s 1s/step - loss: 0.1553 - accuracy: 0.9985\n",
            "epoch  30 : setting learning rate to  0.08571428571428569\n",
            "Epoch 30/35\n",
            "98/98 [==============================] - 146s 1s/step - loss: 0.1479 - accuracy: 0.9989\n",
            "epoch  31 : setting learning rate to  0.0714285714285714\n",
            "Epoch 31/35\n",
            "98/98 [==============================] - 146s 1s/step - loss: 0.1423 - accuracy: 0.9993\n",
            "epoch  32 : setting learning rate to  0.05714285714285716\n",
            "Epoch 32/35\n",
            "98/98 [==============================] - 145s 1s/step - loss: 0.1374 - accuracy: 0.9995\n",
            "epoch  33 : setting learning rate to  0.04285714285714287\n",
            "Epoch 33/35\n",
            "98/98 [==============================] - 145s 1s/step - loss: 0.1335 - accuracy: 0.9995\n",
            "epoch  34 : setting learning rate to  0.02857142857142858\n",
            "Epoch 34/35\n",
            "98/98 [==============================] - 145s 1s/step - loss: 0.1309 - accuracy: 0.9996\n",
            "epoch  35 : setting learning rate to  0.01428571428571429\n",
            "Epoch 35/35\n",
            "98/98 [==============================] - 146s 1s/step - loss: 0.1294 - accuracy: 0.9996\n",
            "is_training False\n",
            "inside validation cycle\n",
            "===============\n",
            "\n",
            "20/20 [==============================] - 3s 128ms/step - loss: 0.4712 - accuracy: 0.9126\n",
            "val accuracy score at the end of training model type  2 [0.4711788326501846, 0.91259766]\n",
            "=========================================\n",
            "\n",
            "is_training True\n",
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 32, 32, 64)   1728        input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 32, 32, 64)   256         conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 32, 32, 64)   0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 32, 32, 128)  73728       activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 32, 32, 128)  512         conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 32, 32, 128)  0           batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling2D) (None, 16, 16, 128)  0           activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 16, 16, 128)  147456      max_pooling2d_12[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 16, 16, 128)  512         conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 16, 16, 128)  0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 16, 16, 128)  147456      activation_38[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 16, 16, 128)  512         conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 16, 16, 128)  0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_8 (TensorFlowOp [(None, 16, 16, 128) 0           max_pooling2d_12[0][0]           \n",
            "                                                                 activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_9 (Lambda)               (None, 16, 16, 128)  0           tf_op_layer_add_8[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 16, 16, 256)  294912      lambda_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 16, 16, 256)  1024        conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 16, 16, 256)  0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling2D) (None, 8, 8, 256)    0           activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 8, 8, 512)    1179648     max_pooling2d_13[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 8, 8, 512)    2048        conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 8, 8, 512)    0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling2D) (None, 4, 4, 512)    0           activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 4, 4, 512)    2359296     max_pooling2d_14[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 4, 4, 512)    2048        conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 4, 4, 512)    0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 4, 4, 512)    2359296     activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 4, 4, 512)    2048        conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 4, 4, 512)    0           batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_9 (TensorFlowOp [(None, 4, 4, 512)]  0           max_pooling2d_14[0][0]           \n",
            "                                                                 activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling2d_4 (GlobalM (None, 512)          0           tf_op_layer_add_9[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 10)           5120        global_max_pooling2d_4[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "lambda_10 (Lambda)              (None, 10)           0           dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 10)           0           lambda_10[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 6,577,600\n",
            "Trainable params: 6,573,120\n",
            "Non-trainable params: 4,480\n",
            "__________________________________________________________________________________________________\n",
            "Model will be trained with distortion after Res Blk 1\n",
            "=======================\n",
            "Train for 98.0 steps\n",
            "epoch  1 : setting learning rate to  0.025\n",
            "Epoch 1/35\n",
            "is_training True\n",
            "is_training True\n",
            "98/98 [==============================] - 150s 2s/step - loss: 1.6027 - accuracy: 0.4565\n",
            "epoch  2 : setting learning rate to  0.07857142857142857\n",
            "Epoch 2/35\n",
            "98/98 [==============================] - 147s 1s/step - loss: 1.1532 - accuracy: 0.6250\n",
            "epoch  3 : setting learning rate to  0.13214285714285715\n",
            "Epoch 3/35\n",
            "98/98 [==============================] - 147s 2s/step - loss: 0.8519 - accuracy: 0.7386\n",
            "epoch  4 : setting learning rate to  0.1857142857142857\n",
            "Epoch 4/35\n",
            "98/98 [==============================] - 147s 2s/step - loss: 0.7160 - accuracy: 0.7931\n",
            "epoch  5 : setting learning rate to  0.23928571428571427\n",
            "Epoch 5/35\n",
            "98/98 [==============================] - 148s 2s/step - loss: 0.6601 - accuracy: 0.8183\n",
            "epoch  6 : setting learning rate to  0.29285714285714287\n",
            "Epoch 6/35\n",
            "98/98 [==============================] - 147s 1s/step - loss: 0.6114 - accuracy: 0.8421\n",
            "epoch  7 : setting learning rate to  0.3464285714285714\n",
            "Epoch 7/35\n",
            "98/98 [==============================] - 148s 2s/step - loss: 0.5965 - accuracy: 0.8547\n",
            "epoch  8 : setting learning rate to  0.4\n",
            "Epoch 8/35\n",
            "98/98 [==============================] - 149s 2s/step - loss: 0.5849 - accuracy: 0.8646\n",
            "epoch  9 : setting learning rate to  0.38571428571428573\n",
            "Epoch 9/35\n",
            "98/98 [==============================] - 149s 2s/step - loss: 0.5303 - accuracy: 0.8894\n",
            "epoch  10 : setting learning rate to  0.37142857142857144\n",
            "Epoch 10/35\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.5081 - accuracy: 0.8973\n",
            "epoch  11 : setting learning rate to  0.35714285714285715\n",
            "Epoch 11/35\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.4724 - accuracy: 0.9113\n",
            "epoch  12 : setting learning rate to  0.34285714285714286\n",
            "Epoch 12/35\n",
            "98/98 [==============================] - 149s 2s/step - loss: 0.4458 - accuracy: 0.9228\n",
            "epoch  13 : setting learning rate to  0.32857142857142857\n",
            "Epoch 13/35\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.4282 - accuracy: 0.9285\n",
            "epoch  14 : setting learning rate to  0.3142857142857143\n",
            "Epoch 14/35\n",
            "98/98 [==============================] - 149s 2s/step - loss: 0.4091 - accuracy: 0.9373\n",
            "epoch  15 : setting learning rate to  0.30000000000000004\n",
            "Epoch 15/35\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.3906 - accuracy: 0.9437\n",
            "epoch  16 : setting learning rate to  0.2857142857142857\n",
            "Epoch 16/35\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.3768 - accuracy: 0.9476\n",
            "epoch  17 : setting learning rate to  0.27142857142857146\n",
            "Epoch 17/35\n",
            "98/98 [==============================] - 149s 2s/step - loss: 0.3573 - accuracy: 0.9542\n",
            "epoch  18 : setting learning rate to  0.2571428571428571\n",
            "Epoch 18/35\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.3395 - accuracy: 0.9592\n",
            "epoch  19 : setting learning rate to  0.24285714285714285\n",
            "Epoch 19/35\n",
            "98/98 [==============================] - 151s 2s/step - loss: 0.3246 - accuracy: 0.9639\n",
            "epoch  20 : setting learning rate to  0.2285714285714286\n",
            "Epoch 20/35\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.3058 - accuracy: 0.9700\n",
            "epoch  21 : setting learning rate to  0.2142857142857143\n",
            "Epoch 21/35\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.2949 - accuracy: 0.9719\n",
            "epoch  22 : setting learning rate to  0.2\n",
            "Epoch 22/35\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.2779 - accuracy: 0.9759\n",
            "epoch  23 : setting learning rate to  0.18571428571428572\n",
            "Epoch 23/35\n",
            "98/98 [==============================] - 151s 2s/step - loss: 0.2651 - accuracy: 0.9786\n",
            "epoch  24 : setting learning rate to  0.17142857142857143\n",
            "Epoch 24/35\n",
            "98/98 [==============================] - 151s 2s/step - loss: 0.2525 - accuracy: 0.9818\n",
            "epoch  25 : setting learning rate to  0.15714285714285714\n",
            "Epoch 25/35\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.2351 - accuracy: 0.9857\n",
            "epoch  26 : setting learning rate to  0.14285714285714285\n",
            "Epoch 26/35\n",
            "98/98 [==============================] - 149s 2s/step - loss: 0.2204 - accuracy: 0.9890\n",
            "epoch  27 : setting learning rate to  0.12857142857142856\n",
            "Epoch 27/35\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.2069 - accuracy: 0.9910\n",
            "epoch  28 : setting learning rate to  0.11428571428571427\n",
            "Epoch 28/35\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.1933 - accuracy: 0.9941\n",
            "epoch  29 : setting learning rate to  0.09999999999999998\n",
            "Epoch 29/35\n",
            "98/98 [==============================] - 151s 2s/step - loss: 0.1813 - accuracy: 0.9959\n",
            "epoch  30 : setting learning rate to  0.08571428571428569\n",
            "Epoch 30/35\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.1706 - accuracy: 0.9972\n",
            "epoch  31 : setting learning rate to  0.0714285714285714\n",
            "Epoch 31/35\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.1638 - accuracy: 0.9976\n",
            "epoch  32 : setting learning rate to  0.05714285714285716\n",
            "Epoch 32/35\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.1563 - accuracy: 0.9987\n",
            "epoch  33 : setting learning rate to  0.04285714285714287\n",
            "Epoch 33/35\n",
            "98/98 [==============================] - 149s 2s/step - loss: 0.1517 - accuracy: 0.9989\n",
            "epoch  34 : setting learning rate to  0.02857142857142858\n",
            "Epoch 34/35\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.1484 - accuracy: 0.9991\n",
            "epoch  35 : setting learning rate to  0.01428571428571429\n",
            "Epoch 35/35\n",
            "98/98 [==============================] - 148s 2s/step - loss: 0.1460 - accuracy: 0.9993\n",
            "is_training False\n",
            "inside validation cycle\n",
            "===============\n",
            "\n",
            "20/20 [==============================] - 3s 127ms/step - loss: 0.4431 - accuracy: 0.9199\n",
            "val accuracy score at the end of training model type  1 [0.44310151487588884, 0.9199219]\n",
            "=========================================\n",
            "\n",
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_6 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 32, 32, 64)   1728        input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 32, 32, 64)   256         conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 32, 32, 64)   0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 32, 32, 128)  73728       activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 32, 32, 128)  512         conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 32, 32, 128)  0           batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_15 (MaxPooling2D) (None, 16, 16, 128)  0           activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 16, 16, 128)  147456      max_pooling2d_15[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 16, 16, 128)  512         conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 16, 16, 128)  0           batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 16, 16, 128)  147456      activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 16, 16, 128)  512         conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 16, 16, 128)  0           batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_10 (TensorFlowO [(None, 16, 16, 128) 0           max_pooling2d_15[0][0]           \n",
            "                                                                 activation_48[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 16, 16, 256)  294912      tf_op_layer_add_10[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 16, 16, 256)  1024        conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 16, 16, 256)  0           batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_16 (MaxPooling2D) (None, 8, 8, 256)    0           activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 8, 8, 512)    1179648     max_pooling2d_16[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 8, 8, 512)    2048        conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 8, 8, 512)    0           batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_17 (MaxPooling2D) (None, 4, 4, 512)    0           activation_50[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 4, 4, 512)    2359296     max_pooling2d_17[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 4, 4, 512)    2048        conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 4, 4, 512)    0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 4, 4, 512)    2359296     activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 4, 4, 512)    2048        conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 4, 4, 512)    0           batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_11 (TensorFlowO [(None, 4, 4, 512)]  0           max_pooling2d_17[0][0]           \n",
            "                                                                 activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling2d_5 (GlobalM (None, 512)          0           tf_op_layer_add_11[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 10)           5120        global_max_pooling2d_5[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "lambda_11 (Lambda)              (None, 10)           0           dense_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_53 (Activation)      (None, 10)           0           lambda_11[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 6,577,600\n",
            "Trainable params: 6,573,120\n",
            "Non-trainable params: 4,480\n",
            "__________________________________________________________________________________________________\n",
            "Model will be trained with image augmentation in put layer\n",
            "====================\n",
            "Train for 98.0 steps\n",
            "epoch  1 : setting learning rate to  0.025\n",
            "Epoch 1/35\n",
            "98/98 [==============================] - 41s 421ms/step - loss: 1.6492 - accuracy: 0.4393\n",
            "epoch  2 : setting learning rate to  0.07857142857142857\n",
            "Epoch 2/35\n",
            "98/98 [==============================] - 39s 402ms/step - loss: 1.1737 - accuracy: 0.6138\n",
            "epoch  3 : setting learning rate to  0.13214285714285715\n",
            "Epoch 3/35\n",
            "98/98 [==============================] - 39s 402ms/step - loss: 0.8947 - accuracy: 0.7226\n",
            "epoch  4 : setting learning rate to  0.1857142857142857\n",
            "Epoch 4/35\n",
            "98/98 [==============================] - 39s 402ms/step - loss: 0.7847 - accuracy: 0.7668\n",
            "epoch  5 : setting learning rate to  0.23928571428571427\n",
            "Epoch 5/35\n",
            "98/98 [==============================] - 39s 400ms/step - loss: 0.7359 - accuracy: 0.7925\n",
            "epoch  6 : setting learning rate to  0.29285714285714287\n",
            "Epoch 6/35\n",
            "98/98 [==============================] - 39s 402ms/step - loss: 0.7203 - accuracy: 0.8054\n",
            "epoch  7 : setting learning rate to  0.3464285714285714\n",
            "Epoch 7/35\n",
            "98/98 [==============================] - 39s 402ms/step - loss: 0.6988 - accuracy: 0.8199\n",
            "epoch  8 : setting learning rate to  0.4\n",
            "Epoch 8/35\n",
            "98/98 [==============================] - 39s 401ms/step - loss: 0.6878 - accuracy: 0.8323\n",
            "epoch  9 : setting learning rate to  0.38571428571428573\n",
            "Epoch 9/35\n",
            "98/98 [==============================] - 39s 401ms/step - loss: 0.6287 - accuracy: 0.8589\n",
            "epoch  10 : setting learning rate to  0.37142857142857144\n",
            "Epoch 10/35\n",
            "98/98 [==============================] - 39s 402ms/step - loss: 0.6052 - accuracy: 0.8695\n",
            "epoch  11 : setting learning rate to  0.35714285714285715\n",
            "Epoch 11/35\n",
            "98/98 [==============================] - 39s 401ms/step - loss: 0.5776 - accuracy: 0.8798\n",
            "epoch  12 : setting learning rate to  0.34285714285714286\n",
            "Epoch 12/35\n",
            "98/98 [==============================] - 39s 401ms/step - loss: 0.5556 - accuracy: 0.8885\n",
            "epoch  13 : setting learning rate to  0.32857142857142857\n",
            "Epoch 13/35\n",
            "98/98 [==============================] - 39s 401ms/step - loss: 0.5363 - accuracy: 0.8956\n",
            "epoch  14 : setting learning rate to  0.3142857142857143\n",
            "Epoch 14/35\n",
            "98/98 [==============================] - 39s 400ms/step - loss: 0.5213 - accuracy: 0.9018\n",
            "epoch  15 : setting learning rate to  0.30000000000000004\n",
            "Epoch 15/35\n",
            "98/98 [==============================] - 39s 401ms/step - loss: 0.5069 - accuracy: 0.9069\n",
            "epoch  16 : setting learning rate to  0.2857142857142857\n",
            "Epoch 16/35\n",
            "98/98 [==============================] - 39s 401ms/step - loss: 0.4925 - accuracy: 0.9123\n",
            "epoch  17 : setting learning rate to  0.27142857142857146\n",
            "Epoch 17/35\n",
            "98/98 [==============================] - 39s 400ms/step - loss: 0.4740 - accuracy: 0.9177\n",
            "epoch  18 : setting learning rate to  0.2571428571428571\n",
            "Epoch 18/35\n",
            "98/98 [==============================] - 39s 401ms/step - loss: 0.4595 - accuracy: 0.9221\n",
            "epoch  19 : setting learning rate to  0.24285714285714285\n",
            "Epoch 19/35\n",
            "98/98 [==============================] - 39s 400ms/step - loss: 0.4475 - accuracy: 0.9281\n",
            "epoch  20 : setting learning rate to  0.2285714285714286\n",
            "Epoch 20/35\n",
            "98/98 [==============================] - 39s 400ms/step - loss: 0.4291 - accuracy: 0.9315\n",
            "epoch  21 : setting learning rate to  0.2142857142857143\n",
            "Epoch 21/35\n",
            "98/98 [==============================] - 39s 401ms/step - loss: 0.4171 - accuracy: 0.9358\n",
            "epoch  22 : setting learning rate to  0.2\n",
            "Epoch 22/35\n",
            "98/98 [==============================] - 39s 400ms/step - loss: 0.4001 - accuracy: 0.9419\n",
            "epoch  23 : setting learning rate to  0.18571428571428572\n",
            "Epoch 23/35\n",
            "98/98 [==============================] - 39s 400ms/step - loss: 0.3868 - accuracy: 0.9437\n",
            "epoch  24 : setting learning rate to  0.17142857142857143\n",
            "Epoch 24/35\n",
            "98/98 [==============================] - 39s 401ms/step - loss: 0.3704 - accuracy: 0.9490\n",
            "epoch  25 : setting learning rate to  0.15714285714285714\n",
            "Epoch 25/35\n",
            "98/98 [==============================] - 39s 400ms/step - loss: 0.3555 - accuracy: 0.9531\n",
            "epoch  26 : setting learning rate to  0.14285714285714285\n",
            "Epoch 26/35\n",
            "98/98 [==============================] - 39s 401ms/step - loss: 0.3414 - accuracy: 0.9570\n",
            "epoch  27 : setting learning rate to  0.12857142857142856\n",
            "Epoch 27/35\n",
            "98/98 [==============================] - 39s 400ms/step - loss: 0.3276 - accuracy: 0.9601\n",
            "epoch  28 : setting learning rate to  0.11428571428571427\n",
            "Epoch 28/35\n",
            "98/98 [==============================] - 39s 401ms/step - loss: 0.3095 - accuracy: 0.9663\n",
            "epoch  29 : setting learning rate to  0.09999999999999998\n",
            "Epoch 29/35\n",
            "98/98 [==============================] - 39s 400ms/step - loss: 0.2923 - accuracy: 0.9694\n",
            "epoch  30 : setting learning rate to  0.08571428571428569\n",
            "Epoch 30/35\n",
            "98/98 [==============================] - 39s 400ms/step - loss: 0.2803 - accuracy: 0.9724\n",
            "epoch  31 : setting learning rate to  0.0714285714285714\n",
            "Epoch 31/35\n",
            "98/98 [==============================] - 39s 401ms/step - loss: 0.2642 - accuracy: 0.9773\n",
            "epoch  32 : setting learning rate to  0.05714285714285716\n",
            "Epoch 32/35\n",
            "98/98 [==============================] - 39s 401ms/step - loss: 0.2500 - accuracy: 0.9811\n",
            "epoch  33 : setting learning rate to  0.04285714285714287\n",
            "Epoch 33/35\n",
            "98/98 [==============================] - 39s 400ms/step - loss: 0.2387 - accuracy: 0.9847\n",
            "epoch  34 : setting learning rate to  0.02857142857142858\n",
            "Epoch 34/35\n",
            "98/98 [==============================] - 39s 401ms/step - loss: 0.2281 - accuracy: 0.9877\n",
            "epoch  35 : setting learning rate to  0.01428571428571429\n",
            "Epoch 35/35\n",
            "98/98 [==============================] - 39s 402ms/step - loss: 0.2197 - accuracy: 0.9903\n",
            "20/20 [==============================] - 3s 145ms/step - loss: 0.3742 - accuracy: 0.9415\n",
            "val accuracy score at the end of training model type  0 [0.37424543499946594, 0.9415039]\n",
            "=========================================\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqkyspQ4R1C6",
        "colab_type": "text"
      },
      "source": [
        "### Summary of Test Results \n",
        "Hyperparameters : Epochs:35, max_lr:0.4, momentum:0.9, L2-wt_decay on Conv Layers :1.25e-4 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzOpzOMqq-gQ",
        "colab_type": "text"
      },
      "source": [
        "| Trial | Augmentation strategy | Train accuracy |Test Accuracy | Hyperparameters |Comments |\n",
        "| :--- | :---: | :---: | :---: | :---: | :--- |\n",
        "| 1 | cutout(flip_lr(pad4_random_crop(inp_image),8)  |99.03  | 94.15 | as above | train-test acc gap 4.88 |\n",
        "| 2 | flip_lr(random_crop(cutout(pad2(res_blk1),4)))  |99.93  | 91.99 | \" | Lower Val accuracy , Tendency to overfit . Perhaps try a more stringent Aug policy ? |\n",
        "| 3 | flip_lr(random_crop(cutout(pad1(res_blk1),2))) | 99.96 | 91.26 | \" | Lowest Val Accuracy, Tendency to overfit. Try a more stringent aug policy? |\n",
        "| 4 | augmentation of trial 2 + augmentation of trial 3  | 99.79  | 91.59 | \" | Lower Val Accuracy, Tendency to overfit. Try a more stringent aug policy? |\n",
        "| 5 | augmentations of trial 1 + augmentation of trial 2  | 97.62  | 93.66 |\"| train-test acc gap 3.96 |\n",
        "| 6 | augmentations of trial 1 + augmentation of trial 3  | 97.70 | 93.80 |\"|  Train-Test acc gap : 3.90 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7EJpOdo09FV",
        "colab_type": "text"
      },
      "source": [
        "**Validation accuracy went down when only Distortion was used in the mid/lower layers (after Res Blk1 and Res Blk2 ) But when used in combination with usual Image augmentation , there seems to be better Regularization and perhaps we could explore such an option. If we were to pursue only Distortion of middle or lower layers (with a good enough size of channels ) , we may need to try more stringent augmentation strategies to overcome the problem of overfitting**"
      ]
    }
  ]
}