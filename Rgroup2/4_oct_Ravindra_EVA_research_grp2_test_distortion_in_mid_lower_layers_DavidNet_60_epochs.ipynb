{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4-oct_Ravindra_EVA_research_grp2_test_distortion_in_mid_lower_layers_DavidNet-60-epochs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ravindrabharathi/Project1/blob/master/Rgroup2/4_oct_Ravindra_EVA_research_grp2_test_distortion_in_mid_lower_layers_DavidNet_60_epochs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynqEXKR8Quk9",
        "colab_type": "text"
      },
      "source": [
        "### Install tf2 modules "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgLSEQaclSMi",
        "colab_type": "text"
      },
      "source": [
        "### install/import tensorflow_addons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3R6SSn4lb2D",
        "colab_type": "code",
        "outputId": "108ecf9d-4df5-447f-b008-b14397bbe1e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "!pip install tensorflow==2.0\n",
        "!pip install tensorflow-gpu==2.0\n",
        "!pip install tensorflow_addons\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import math\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/0f/7bd55361168bb32796b360ad15a25de6966c9c1beb58a8e30c01c8279862/tensorflow-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (86.3MB)\n",
            "\u001b[K     |████████████████████████████████| 86.3MB 35.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.0.8)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (0.8.0)\n",
            "Collecting gast==0.2.2 (from tensorflow==2.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (3.7.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.12.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.16.5)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (0.33.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (3.0.1)\n",
            "Collecting tensorflow-estimator<2.1.0,>=2.0.0 (from tensorflow==2.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/00/5e6cdf86190a70d7382d320b2b04e4ff0f8191a37d90a422a2f8ff0705bb/tensorflow_estimator-2.0.0-py2.py3-none-any.whl (449kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 42.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.11.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.15.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (0.8.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (0.1.7)\n",
            "Collecting tensorboard<2.1.0,>=2.0.0 (from tensorflow==2.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/a6/e8ffa4e2ddb216449d34cfcb825ebb38206bee5c4553d69e7bc8bc2c5d64/tensorboard-2.0.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 36.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==2.0) (41.2.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (0.16.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (3.1.1)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=6e22927ccf5650edc68635d04ecc6888b9236ebb8b13b716a4a5911e29f4fd0d\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "Installing collected packages: gast, tensorflow-estimator, tensorboard, tensorflow\n",
            "  Found existing installation: gast 0.3.2\n",
            "    Uninstalling gast-0.3.2:\n",
            "      Successfully uninstalled gast-0.3.2\n",
            "  Found existing installation: tensorflow-estimator 1.14.0\n",
            "    Uninstalling tensorflow-estimator-1.14.0:\n",
            "      Successfully uninstalled tensorflow-estimator-1.14.0\n",
            "  Found existing installation: tensorboard 1.14.0\n",
            "    Uninstalling tensorboard-1.14.0:\n",
            "      Successfully uninstalled tensorboard-1.14.0\n",
            "  Found existing installation: tensorflow 1.14.0\n",
            "    Uninstalling tensorflow-1.14.0:\n",
            "      Successfully uninstalled tensorflow-1.14.0\n",
            "Successfully installed gast-0.2.2 tensorboard-2.0.0 tensorflow-2.0.0 tensorflow-estimator-2.0.0\n",
            "Collecting tensorflow-gpu==2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/44/47f0722aea081697143fbcf5d2aa60d1aee4aaacb5869aee2b568974777b/tensorflow_gpu-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (380.8MB)\n",
            "\u001b[K     |████████████████████████████████| 380.8MB 82kB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (3.7.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (1.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (0.8.0)\n",
            "Requirement already satisfied: tensorboard<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (2.0.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (1.0.8)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (0.8.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (1.16.5)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (1.11.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (0.33.6)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (0.1.7)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (0.2.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (2.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (3.0.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0) (41.2.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (0.16.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (3.1.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==2.0) (2.8.0)\n",
            "Installing collected packages: tensorflow-gpu\n",
            "Successfully installed tensorflow-gpu-2.0.0\n",
            "Collecting tensorflow_addons\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/b6/574034405ad527eec40ac426081694f50da9766db6b81c2b899041c44ef2/tensorflow_addons-0.5.2-cp36-cp36m-manylinux2010_x86_64.whl (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 6.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-gpu==2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_addons) (2.0.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_addons) (1.12.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (1.11.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (1.16.5)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (0.33.6)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (0.8.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (0.2.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (3.7.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (2.0.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (1.0.8)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (0.8.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (0.1.7)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (1.1.0)\n",
            "Requirement already satisfied: tensorboard<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (2.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (3.0.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0->tensorflow_addons) (41.2.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==2.0.0->tensorflow_addons) (2.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0->tensorflow_addons) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0->tensorflow_addons) (0.16.0)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.5.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFPFRpPDQ2e6",
        "colab_type": "text"
      },
      "source": [
        "### Install tf_utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0yHfmeMv-rI",
        "colab_type": "code",
        "outputId": "424d15da-e1bc-40ee-ac02-0d1f32bb6997",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "!pip install --upgrade git+https://github.com/ravindrabharathi/tf_utils "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/ravindrabharathi/tf_utils\n",
            "  Cloning https://github.com/ravindrabharathi/tf_utils to /tmp/pip-req-build-s0ejlgnr\n",
            "  Running command git clone -q https://github.com/ravindrabharathi/tf_utils /tmp/pip-req-build-s0ejlgnr\n",
            "Building wheels for collected packages: tf-utils\n",
            "  Building wheel for tf-utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tf-utils: filename=tf_utils-0.2-cp36-none-any.whl size=8945 sha256=c25a6d02f35737de35b04aac2bebf8c4e5bfaa9720b1dc4f157cb7a38cc86a2d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-qveeyhdf/wheels/95/af/bb/690b94c65a5aad47a5c39e75f158a2b043448e908c5c121791\n",
            "Successfully built tf-utils\n",
            "Installing collected packages: tf-utils\n",
            "Successfully installed tf-utils-0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VH-5ffTMQ8ms",
        "colab_type": "text"
      },
      "source": [
        "### import the data module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBcelJMpwYoX",
        "colab_type": "code",
        "outputId": "86c0c6f3-0fb0-476d-c5f9-8ec6ce424db6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tf_utils.data as ds"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished 'get_cpu_num' in 0.0000 secs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vb0kbJalRCPJ",
        "colab_type": "text"
      },
      "source": [
        "### set batch size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJTA3CO8xOtg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size=512\n",
        "ds.batch_size=batch_size\n",
        "EPOCHS=60"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTroxaquRE2Z",
        "colab_type": "text"
      },
      "source": [
        "### downlaod data and create tf records"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zF3qoDnbykb6",
        "colab_type": "code",
        "outputId": "c0a45e23-069e-4245-a80a-79c95e325f24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "ds.get_cifar10_and_create_tfrecords()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "62b3915b2e1746d99b5e257dbe91408c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='cifar-10-python.tar.gz', max=170498071, style=ProgressStyle(d…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Finished 'download_file' in 6.5949 secs\n",
            "Finished 'download_cifar10_files' in 6.5955 secs\n",
            "Done\n",
            "Finished 'extract_cifar10_files' in 1.8980 secs\n",
            "Finished '_get_file_names' in 0.0000 secs\n",
            "Generating ./train.tfrecords\n",
            "Finished 'read_pickle_from_file' in 0.1492 secs\n",
            "Finished 'read_pickle_from_file' in 0.1437 secs\n",
            "Finished 'read_pickle_from_file' in 0.1336 secs\n",
            "Finished 'read_pickle_from_file' in 0.1461 secs\n",
            "Finished 'read_pickle_from_file' in 0.1339 secs\n",
            "Finished 'convert_to_tfrecord' in 3.1494 secs\n",
            "Done!\n",
            "Generating ./eval.tfrecords\n",
            "Finished 'read_pickle_from_file' in 0.1379 secs\n",
            "Finished 'convert_to_tfrecord' in 0.6191 secs\n",
            "Done!\n",
            "Finished 'create_tf_records' in 3.7707 secs\n",
            "Finished 'get_cifar10_and_create_tfrecords' in 12.2647 secs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7U-_CmC6RKDF",
        "colab_type": "text"
      },
      "source": [
        "### create train and test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lYOp7UKyoG5",
        "colab_type": "code",
        "outputId": "7f83ae19-d70d-4d41-bb77-33b41005a5ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "def aug_fn(image):\n",
        "  return ds.cutout(ds.flip_left_right(ds.random_pad_crop(image)))\n",
        "\n",
        "train_ds1=ds.get_train_ds(batch_size=batch_size,shuffle=True,distort=True,distort_fn=aug_fn)\n",
        "train_ds2=ds.get_train_ds(batch_size=batch_size,shuffle=True,distort=False)\n",
        "\n",
        "test_ds=ds.get_eval_ds(batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "distorting...\n",
            "Finished 'get_tf_dataset_2' in 2.7414 secs\n",
            "Finished 'get_tf_dataset_in_batches' in 2.7418 secs\n",
            "Finished 'get_train_ds' in 2.7420 secs\n",
            "Finished 'get_tf_dataset' in 0.1619 secs\n",
            "Finished 'get_tf_dataset_in_batches' in 0.1621 secs\n",
            "Finished 'get_train_ds' in 0.1625 secs\n",
            "Finished 'get_tf_dataset' in 0.0262 secs\n",
            "Finished 'get_tf_dataset_in_batches' in 0.0263 secs\n",
            "Finished 'get_eval_ds' in 0.0264 secs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7z9t4e5RRbH",
        "colab_type": "text"
      },
      "source": [
        "### import visualization module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHq2g5VjzmDz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tf_utils.visualize as vz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utItaz0RRXft",
        "colab_type": "text"
      },
      "source": [
        "### plot images from train dataset1 , train dataset by default uses image augmenttation of cutout,flip-left-right,random-pad-crop "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvonf0IP0uOX",
        "colab_type": "code",
        "outputId": "97c8a6e2-8598-4165-abc9-babf381cc8e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "vz.plot_cifar10_files(train_ds1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/IRECoOfkEhh8BMSKQx+QiLl/wBR/1OyeBNP8AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n2QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKnsdKUUovds",
        "colab_type": "code",
        "outputId": "e8ad1f3b-6a8c-482d-c0d2-8deab7544390",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "vz.plot_cifar10_files(train_ds2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nX4hEUfALkSgKfiESRcEvRKIo+IVIlP8L+j73elgLRHIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nheInJFIofkIiheInJFL+F54drSVY/0HiAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qN5z9vjRsvL",
        "colab_type": "text"
      },
      "source": [
        "### Training - Build model , compile and train "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m26XDkJi1YyS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, GlobalAveragePooling2D,  Activation, GlobalMaxPooling2D\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization , Dense, Lambda\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam,SGD\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "WEIGHT_DECAY=1.25e-4\n",
        "reg=tf.keras.regularizers.l2(WEIGHT_DECAY)\n",
        "def init_pytorch(shape, dtype=tf.float32, partition_info=None):\n",
        "  fan = np.prod(shape[:-1])\n",
        "  bound = 1 / math.sqrt(fan)\n",
        "  return tf.random.uniform(shape, minval=-bound, maxval=bound, dtype=dtype)\n",
        "\n",
        "def conv(inp,f=32,k=3):\n",
        "  conv_layer=Conv2D(f,k,use_bias=False,padding='same',kernel_initializer=init_pytorch, kernel_regularizer=reg)(inp)\n",
        "  conv_layer=BatchNormalization(momentum=0.9, epsilon=1e-5)(conv_layer)\n",
        "  conv_layer=Activation('relu')(conv_layer)\n",
        "  return conv_layer\n",
        "def resBlk(inp,f=32,k=3,residual=True) :\n",
        "  res1=conv(inp,f,k)\n",
        "  res1=MaxPooling2D(pool_size=(2,2))(res1)\n",
        "  if residual:\n",
        "    res2=conv(res1,f,k)\n",
        "    res3=conv(res2,f,k)\n",
        "    return res1+res3\n",
        "  else:\n",
        "    return res1  \n",
        "\n",
        "def apply_weight(x):\n",
        "  return x*0.125  \n",
        "\n",
        "def random_pad_crop(image,padding=2):\n",
        "  shp=tf.shape(image)\n",
        "  \n",
        "  image=tf.pad(image,[(0, 0), (padding, padding), (padding, padding), (0, 0)], mode='reflect')\n",
        "  \n",
        "  image=tf.image.random_crop(image,size=shp)\n",
        "  return image  \n",
        "\n",
        "def flip_left_right(image):\n",
        "  return tf.image.random_flip_left_right(image)   \n",
        "\n",
        "def aug_fn1(img):\n",
        "  shp=img.get_shape()\n",
        "  #print(shp)\n",
        "  #out_shp=[shp[0],shp[1],shp[2]]\n",
        "  return ds.flip_left_right(ds.random_crop(ds.cutout(ds.pad_img(img,padding=2),100,size=4),out_shape=shp))\n",
        "\n",
        "def aug_fn2(img):\n",
        "  shp=img.get_shape()\n",
        "  #print(shp)\n",
        "  #out_shp=[shp[0],shp[1],shp[2]]\n",
        "  return ds.flip_left_right(ds.random_crop(ds.cutout(ds.pad_img(img,padding=1),100,size=2),out_shape=shp))\n",
        "\n",
        "def aug1(image):\n",
        "  print('is_training',is_training)\n",
        "  if is_training:\n",
        "    \n",
        "    #return tf.map_fn(lambda img: aug_fn1(img),image,parallel_iterations=ds.CPU_CORES,back_prop=is_training) \n",
        "    return tf.map_fn(lambda img: aug_fn1(img) ,image,back_prop=is_training)    \n",
        "  else:\n",
        "    print('inside validation cycle\\n===============\\n')\n",
        "    return image  \n",
        "\n",
        "def aug2(image):\n",
        "  \n",
        "  print('is_training',is_training)\n",
        "  if is_training:\n",
        "    \n",
        "    return tf.map_fn(lambda img: aug_fn2(img) ,image,back_prop=is_training)\n",
        "  else:\n",
        "    print('inside validation cycle\\n===============\\n')\n",
        "    return image        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVqdMyzkx4tY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(distort_param=0):\n",
        "  f=64\n",
        "  inp=Input(shape=(32,32,3))\n",
        "  layer1=conv(inp,f,3)\n",
        "  res1=resBlk(layer1,f*2,3)\n",
        "  if (distort_param in [1,3,4]):\n",
        "    res1=Lambda(aug1)(res1)\n",
        "  res2=resBlk(res1,f*4,3,False)\n",
        "  if (distort_param in [2,3,5]):\n",
        "    res2=Lambda(aug2)(res2)\n",
        "  res3=resBlk(res2,f*8,3)\n",
        "  \n",
        "  layer2=GlobalMaxPooling2D()(res3)\n",
        "  layer3=Dense(10, kernel_initializer=init_pytorch, use_bias=False,kernel_regularizer=reg)(layer2)\n",
        "  layer4=Lambda(lambda x: x*0.125)(layer3)\n",
        "  out=Activation('softmax')(layer4)\n",
        "  model=Model(inputs=[inp],outputs=[out])\n",
        "  model.summary()\n",
        "  return model "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nng9bFm1rQq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "\n",
        "def lr_schedule():\n",
        "    \n",
        "    def schedule(epoch):\n",
        "\n",
        "      lr=lr1=np.interp([epoch],[0, EPOCHS//3,EPOCHS], [0.025, 0.4, 0])[0]\n",
        "      print('epoch ', epoch+1, ': setting learning rate to ',lr1)\n",
        "      return lr\n",
        "    \n",
        "    return LearningRateScheduler(schedule)\n",
        "\n",
        "lr_sched = lr_schedule()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaDK2E2h81vV",
        "colab_type": "code",
        "outputId": "4555cc6f-3858-4e48-9639-f3145b8fee58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "for model_params in [5,4,3,2,1,0]:\n",
        "  is_training=True\n",
        "  #global_step = tf.train.get_or_create_global_step()\n",
        "  model=model=build_model(model_params)\n",
        "  opt=SGD(lr=0.025,momentum=0.9,nesterov=True)\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,metrics=['accuracy']\n",
        "              )\n",
        "  batch_size=512\n",
        "  \n",
        "  if model_params in [0,4,5]:\n",
        "    train_ds=train_ds1\n",
        "    print('Model will be trained with image augmentation in put layer\\n====================')\n",
        "    if model_params==4:\n",
        "      print('Model will also include distortion after Res Blk 1\\n=======================')\n",
        "    elif model_params==5:\n",
        "      print('Model will also include distortion after Res Blk 2\\n=======================')\n",
        "      \n",
        "  else:\n",
        "    if model_params==1:\n",
        "      print('Model will be trained with distortion after Res Blk 1\\n=======================')\n",
        "    elif model_params==2:\n",
        "      print('Model will be trained with distortion after Res Blk 2\\n=======================')\n",
        "    elif model_params==3:\n",
        "      print('Model will be trained with distortion after Res Blk 1 and Res Blk 2\\n=======================')    \n",
        "    train_ds=train_ds2  \n",
        "  model.fit(train_ds,epochs=EPOCHS, steps_per_epoch=np.ceil(50000/batch_size), \n",
        "          callbacks=[lr_sched],\n",
        "          verbose=1)\n",
        "  is_training=False\n",
        "  score=model.evaluate(test_ds, steps =np.ceil(10000/batch_size), verbose=1)\n",
        "\n",
        "  del(model)\n",
        "  del(train_ds)\n",
        "  \n",
        "  print('val accuracy score at the end of training model type ',model_params, score)\n",
        "  print(\"=========================================\\n\")\n",
        "\n",
        "#validation_data=test_ds, validation_steps=np.ceil(10000/batch_size),"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "is_training True\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 32, 32, 64)   1728        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 32, 32, 64)   256         conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 32, 32, 64)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 128)  73728       activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 128)  512         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 128)  0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 16, 16, 128)  0           activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 16, 16, 128)  147456      max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 16, 16, 128)  512         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 16, 16, 128)  0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 16, 16, 128)  147456      activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 16, 16, 128)  512         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 16, 16, 128)  0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add (TensorFlowOpLa [(None, 16, 16, 128) 0           max_pooling2d[0][0]              \n",
            "                                                                 activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 16, 16, 256)  294912      tf_op_layer_add[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 16, 16, 256)  1024        conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 16, 16, 256)  0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 8, 8, 256)    0           activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, 8, 8, 256)    0           max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 8, 8, 512)    1179648     lambda[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 8, 8, 512)    2048        conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 8, 8, 512)    0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 4, 4, 512)    0           activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 4, 4, 512)    2359296     max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 4, 4, 512)    2048        conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 4, 4, 512)    0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 4, 4, 512)    2359296     activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 4, 4, 512)    2048        conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 4, 4, 512)    0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_1 (TensorFlowOp [(None, 4, 4, 512)]  0           max_pooling2d_2[0][0]            \n",
            "                                                                 activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling2d (GlobalMax (None, 512)          0           tf_op_layer_add_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 10)           5120        global_max_pooling2d[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 10)           0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 10)           0           lambda_1[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 6,577,600\n",
            "Trainable params: 6,573,120\n",
            "Non-trainable params: 4,480\n",
            "__________________________________________________________________________________________________\n",
            "Model will be trained with image augmentation in put layer\n",
            "====================\n",
            "Model will also include distortion after Res Blk 2\n",
            "=======================\n",
            "Train for 98.0 steps\n",
            "epoch  1 : setting learning rate to  0.025\n",
            "Epoch 1/60\n",
            "is_training True\n",
            "is_training True\n",
            "98/98 [==============================] - 168s 2s/step - loss: 1.7239 - accuracy: 0.4068\n",
            "epoch  2 : setting learning rate to  0.04375\n",
            "Epoch 2/60\n",
            "98/98 [==============================] - 158s 2s/step - loss: 1.2058 - accuracy: 0.6007\n",
            "epoch  3 : setting learning rate to  0.0625\n",
            "Epoch 3/60\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.9791 - accuracy: 0.6881\n",
            "epoch  4 : setting learning rate to  0.08124999999999999\n",
            "Epoch 4/60\n",
            "98/98 [==============================] - 158s 2s/step - loss: 0.8458 - accuracy: 0.7399\n",
            "epoch  5 : setting learning rate to  0.1\n",
            "Epoch 5/60\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.7677 - accuracy: 0.7699\n",
            "epoch  6 : setting learning rate to  0.11875\n",
            "Epoch 6/60\n",
            "98/98 [==============================] - 160s 2s/step - loss: 0.7115 - accuracy: 0.7959\n",
            "epoch  7 : setting learning rate to  0.13749999999999998\n",
            "Epoch 7/60\n",
            "98/98 [==============================] - 159s 2s/step - loss: 0.6739 - accuracy: 0.8104\n",
            "epoch  8 : setting learning rate to  0.15625\n",
            "Epoch 8/60\n",
            "98/98 [==============================] - 159s 2s/step - loss: 0.6519 - accuracy: 0.8209\n",
            "epoch  9 : setting learning rate to  0.175\n",
            "Epoch 9/60\n",
            "98/98 [==============================] - 160s 2s/step - loss: 0.6281 - accuracy: 0.8324\n",
            "epoch  10 : setting learning rate to  0.19374999999999998\n",
            "Epoch 10/60\n",
            "98/98 [==============================] - 159s 2s/step - loss: 0.6182 - accuracy: 0.8385\n",
            "epoch  11 : setting learning rate to  0.2125\n",
            "Epoch 11/60\n",
            "98/98 [==============================] - 159s 2s/step - loss: 0.6056 - accuracy: 0.8480\n",
            "epoch  12 : setting learning rate to  0.23124999999999998\n",
            "Epoch 12/60\n",
            "98/98 [==============================] - 161s 2s/step - loss: 0.6059 - accuracy: 0.8504\n",
            "epoch  13 : setting learning rate to  0.24999999999999997\n",
            "Epoch 13/60\n",
            "98/98 [==============================] - 158s 2s/step - loss: 0.5946 - accuracy: 0.8593\n",
            "epoch  14 : setting learning rate to  0.26875\n",
            "Epoch 14/60\n",
            "98/98 [==============================] - 158s 2s/step - loss: 0.6017 - accuracy: 0.8590\n",
            "epoch  15 : setting learning rate to  0.28750000000000003\n",
            "Epoch 15/60\n",
            "98/98 [==============================] - 162s 2s/step - loss: 0.6004 - accuracy: 0.8638\n",
            "epoch  16 : setting learning rate to  0.30625\n",
            "Epoch 16/60\n",
            "98/98 [==============================] - 163s 2s/step - loss: 0.6004 - accuracy: 0.8668\n",
            "epoch  17 : setting learning rate to  0.325\n",
            "Epoch 17/60\n",
            "98/98 [==============================] - 164s 2s/step - loss: 0.6024 - accuracy: 0.8703\n",
            "epoch  18 : setting learning rate to  0.34375\n",
            "Epoch 18/60\n",
            "98/98 [==============================] - 167s 2s/step - loss: 0.6101 - accuracy: 0.8704\n",
            "epoch  19 : setting learning rate to  0.3625\n",
            "Epoch 19/60\n",
            "98/98 [==============================] - 159s 2s/step - loss: 0.6105 - accuracy: 0.8748\n",
            "epoch  20 : setting learning rate to  0.38125000000000003\n",
            "Epoch 20/60\n",
            "98/98 [==============================] - 160s 2s/step - loss: 0.6187 - accuracy: 0.8761\n",
            "epoch  21 : setting learning rate to  0.4\n",
            "Epoch 21/60\n",
            "98/98 [==============================] - 159s 2s/step - loss: 0.6262 - accuracy: 0.8754\n",
            "epoch  22 : setting learning rate to  0.39\n",
            "Epoch 22/60\n",
            "98/98 [==============================] - 158s 2s/step - loss: 0.6222 - accuracy: 0.8795\n",
            "epoch  23 : setting learning rate to  0.38\n",
            "Epoch 23/60\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.6045 - accuracy: 0.8883\n",
            "epoch  24 : setting learning rate to  0.37\n",
            "Epoch 24/60\n",
            "98/98 [==============================] - 160s 2s/step - loss: 0.5947 - accuracy: 0.8911\n",
            "epoch  25 : setting learning rate to  0.36000000000000004\n",
            "Epoch 25/60\n",
            "98/98 [==============================] - 162s 2s/step - loss: 0.5775 - accuracy: 0.8982\n",
            "epoch  26 : setting learning rate to  0.35000000000000003\n",
            "Epoch 26/60\n",
            "98/98 [==============================] - 160s 2s/step - loss: 0.5734 - accuracy: 0.8994\n",
            "epoch  27 : setting learning rate to  0.34\n",
            "Epoch 27/60\n",
            "98/98 [==============================] - 160s 2s/step - loss: 0.5559 - accuracy: 0.9052\n",
            "epoch  28 : setting learning rate to  0.33\n",
            "Epoch 28/60\n",
            "98/98 [==============================] - 160s 2s/step - loss: 0.5563 - accuracy: 0.9057\n",
            "epoch  29 : setting learning rate to  0.32\n",
            "Epoch 29/60\n",
            "98/98 [==============================] - 160s 2s/step - loss: 0.5410 - accuracy: 0.9097\n",
            "epoch  30 : setting learning rate to  0.31000000000000005\n",
            "Epoch 30/60\n",
            "98/98 [==============================] - 159s 2s/step - loss: 0.5406 - accuracy: 0.9094\n",
            "epoch  31 : setting learning rate to  0.30000000000000004\n",
            "Epoch 31/60\n",
            "98/98 [==============================] - 160s 2s/step - loss: 0.5166 - accuracy: 0.9181\n",
            "epoch  32 : setting learning rate to  0.29000000000000004\n",
            "Epoch 32/60\n",
            "98/98 [==============================] - 160s 2s/step - loss: 0.5140 - accuracy: 0.9188\n",
            "epoch  33 : setting learning rate to  0.28\n",
            "Epoch 33/60\n",
            "98/98 [==============================] - 159s 2s/step - loss: 0.5078 - accuracy: 0.9206\n",
            "epoch  34 : setting learning rate to  0.27\n",
            "Epoch 34/60\n",
            "98/98 [==============================] - 160s 2s/step - loss: 0.4999 - accuracy: 0.9228\n",
            "epoch  35 : setting learning rate to  0.26\n",
            "Epoch 35/60\n",
            "98/98 [==============================] - 160s 2s/step - loss: 0.4918 - accuracy: 0.9247\n",
            "epoch  36 : setting learning rate to  0.25\n",
            "Epoch 36/60\n",
            "98/98 [==============================] - 159s 2s/step - loss: 0.4775 - accuracy: 0.9281\n",
            "epoch  37 : setting learning rate to  0.24000000000000002\n",
            "Epoch 37/60\n",
            "98/98 [==============================] - 158s 2s/step - loss: 0.4712 - accuracy: 0.9298\n",
            "epoch  38 : setting learning rate to  0.23\n",
            "Epoch 38/60\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.4709 - accuracy: 0.9299\n",
            "epoch  39 : setting learning rate to  0.22000000000000003\n",
            "Epoch 39/60\n",
            "98/98 [==============================] - 159s 2s/step - loss: 0.4544 - accuracy: 0.9344\n",
            "epoch  40 : setting learning rate to  0.21000000000000002\n",
            "Epoch 40/60\n",
            "98/98 [==============================] - 158s 2s/step - loss: 0.4385 - accuracy: 0.9399\n",
            "epoch  41 : setting learning rate to  0.2\n",
            "Epoch 41/60\n",
            "98/98 [==============================] - 158s 2s/step - loss: 0.4326 - accuracy: 0.9386\n",
            "epoch  42 : setting learning rate to  0.19000000000000003\n",
            "Epoch 42/60\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.4181 - accuracy: 0.9449\n",
            "epoch  43 : setting learning rate to  0.18000000000000002\n",
            "Epoch 43/60\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.4091 - accuracy: 0.9459\n",
            "epoch  44 : setting learning rate to  0.17\n",
            "Epoch 44/60\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.4066 - accuracy: 0.9463\n",
            "epoch  45 : setting learning rate to  0.16000000000000003\n",
            "Epoch 45/60\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.3870 - accuracy: 0.9519\n",
            "epoch  46 : setting learning rate to  0.15000000000000002\n",
            "Epoch 46/60\n",
            "98/98 [==============================] - 156s 2s/step - loss: 0.3790 - accuracy: 0.9524\n",
            "epoch  47 : setting learning rate to  0.14\n",
            "Epoch 47/60\n",
            "98/98 [==============================] - 156s 2s/step - loss: 0.3628 - accuracy: 0.9580\n",
            "epoch  48 : setting learning rate to  0.13\n",
            "Epoch 48/60\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.3540 - accuracy: 0.9594\n",
            "epoch  49 : setting learning rate to  0.12\n",
            "Epoch 49/60\n",
            "98/98 [==============================] - 159s 2s/step - loss: 0.3413 - accuracy: 0.9618\n",
            "epoch  50 : setting learning rate to  0.11000000000000004\n",
            "Epoch 50/60\n",
            "98/98 [==============================] - 161s 2s/step - loss: 0.3262 - accuracy: 0.9659\n",
            "epoch  51 : setting learning rate to  0.10000000000000003\n",
            "Epoch 51/60\n",
            "98/98 [==============================] - 159s 2s/step - loss: 0.3136 - accuracy: 0.9688\n",
            "epoch  52 : setting learning rate to  0.09000000000000002\n",
            "Epoch 52/60\n",
            "98/98 [==============================] - 161s 2s/step - loss: 0.3023 - accuracy: 0.9716\n",
            "epoch  53 : setting learning rate to  0.08000000000000002\n",
            "Epoch 53/60\n",
            "98/98 [==============================] - 161s 2s/step - loss: 0.2919 - accuracy: 0.9736\n",
            "epoch  54 : setting learning rate to  0.07\n",
            "Epoch 54/60\n",
            "98/98 [==============================] - 161s 2s/step - loss: 0.2777 - accuracy: 0.9777\n",
            "epoch  55 : setting learning rate to  0.06\n",
            "Epoch 55/60\n",
            "98/98 [==============================] - 158s 2s/step - loss: 0.2679 - accuracy: 0.9792\n",
            "epoch  56 : setting learning rate to  0.04999999999999999\n",
            "Epoch 56/60\n",
            "98/98 [==============================] - 159s 2s/step - loss: 0.2568 - accuracy: 0.9819\n",
            "epoch  57 : setting learning rate to  0.040000000000000036\n",
            "Epoch 57/60\n",
            "98/98 [==============================] - 160s 2s/step - loss: 0.2453 - accuracy: 0.9853\n",
            "epoch  58 : setting learning rate to  0.030000000000000027\n",
            "Epoch 58/60\n",
            "98/98 [==============================] - 160s 2s/step - loss: 0.2380 - accuracy: 0.9870\n",
            "epoch  59 : setting learning rate to  0.020000000000000018\n",
            "Epoch 59/60\n",
            "98/98 [==============================] - 160s 2s/step - loss: 0.2301 - accuracy: 0.9892\n",
            "epoch  60 : setting learning rate to  0.010000000000000009\n",
            "Epoch 60/60\n",
            "98/98 [==============================] - 161s 2s/step - loss: 0.2270 - accuracy: 0.9896\n",
            "is_training False\n",
            "inside validation cycle\n",
            "===============\n",
            "\n",
            "20/20 [==============================] - 3s 135ms/step - loss: 0.3800 - accuracy: 0.9472\n",
            "val accuracy score at the end of training model type  5 [0.37996586710214614, 0.947168]\n",
            "=========================================\n",
            "\n",
            "is_training True\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 32, 32, 64)   1728        input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 32, 32, 64)   256         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 32, 32, 64)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 32, 32, 128)  73728       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 32, 32, 128)  512         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 32, 32, 128)  0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 16, 16, 128)  0           activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 16, 16, 128)  147456      max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 16, 16, 128)  512         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 16, 16, 128)  0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 16, 16, 128)  147456      activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 16, 16, 128)  512         conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 16, 16, 128)  0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_2 (TensorFlowOp [(None, 16, 16, 128) 0           max_pooling2d_3[0][0]            \n",
            "                                                                 activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_2 (Lambda)               (None, 16, 16, 128)  0           tf_op_layer_add_2[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 16, 16, 256)  294912      lambda_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 16, 16, 256)  1024        conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 16, 16, 256)  0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 8, 8, 256)    0           activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 8, 8, 512)    1179648     max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 8, 8, 512)    2048        conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 8, 8, 512)    0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2D)  (None, 4, 4, 512)    0           activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 4, 4, 512)    2359296     max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 4, 4, 512)    2048        conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 4, 4, 512)    0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 4, 4, 512)    2359296     activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 4, 4, 512)    2048        conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 4, 4, 512)    0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_3 (TensorFlowOp [(None, 4, 4, 512)]  0           max_pooling2d_5[0][0]            \n",
            "                                                                 activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling2d_1 (GlobalM (None, 512)          0           tf_op_layer_add_3[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 10)           5120        global_max_pooling2d_1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "lambda_3 (Lambda)               (None, 10)           0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 10)           0           lambda_3[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 6,577,600\n",
            "Trainable params: 6,573,120\n",
            "Non-trainable params: 4,480\n",
            "__________________________________________________________________________________________________\n",
            "Model will be trained with image augmentation in put layer\n",
            "====================\n",
            "Model will also include distortion after Res Blk 1\n",
            "=======================\n",
            "Train for 98.0 steps\n",
            "epoch  1 : setting learning rate to  0.025\n",
            "Epoch 1/60\n",
            "is_training True\n",
            "is_training True\n",
            "98/98 [==============================] - 168s 2s/step - loss: 1.7390 - accuracy: 0.4004\n",
            "epoch  2 : setting learning rate to  0.04375\n",
            "Epoch 2/60\n",
            "98/98 [==============================] - 165s 2s/step - loss: 1.2087 - accuracy: 0.6002\n",
            "epoch  3 : setting learning rate to  0.0625\n",
            "Epoch 3/60\n",
            "98/98 [==============================] - 163s 2s/step - loss: 0.9749 - accuracy: 0.6914\n",
            "epoch  4 : setting learning rate to  0.08124999999999999\n",
            "Epoch 4/60\n",
            "98/98 [==============================] - 162s 2s/step - loss: 0.8494 - accuracy: 0.7390\n",
            "epoch  5 : setting learning rate to  0.1\n",
            "Epoch 5/60\n",
            "98/98 [==============================] - 163s 2s/step - loss: 0.7741 - accuracy: 0.7689\n",
            "epoch  6 : setting learning rate to  0.11875\n",
            "Epoch 6/60\n",
            "98/98 [==============================] - 162s 2s/step - loss: 0.7247 - accuracy: 0.7889\n",
            "epoch  7 : setting learning rate to  0.13749999999999998\n",
            "Epoch 7/60\n",
            "98/98 [==============================] - 163s 2s/step - loss: 0.6829 - accuracy: 0.8061\n",
            "epoch  8 : setting learning rate to  0.15625\n",
            "Epoch 8/60\n",
            "98/98 [==============================] - 162s 2s/step - loss: 0.6573 - accuracy: 0.8166\n",
            "epoch  9 : setting learning rate to  0.175\n",
            "Epoch 9/60\n",
            "98/98 [==============================] - 161s 2s/step - loss: 0.6353 - accuracy: 0.8294\n",
            "epoch  10 : setting learning rate to  0.19374999999999998\n",
            "Epoch 10/60\n",
            "98/98 [==============================] - 161s 2s/step - loss: 0.6247 - accuracy: 0.8373\n",
            "epoch  11 : setting learning rate to  0.2125\n",
            "Epoch 11/60\n",
            "98/98 [==============================] - 162s 2s/step - loss: 0.6100 - accuracy: 0.8456\n",
            "epoch  12 : setting learning rate to  0.23124999999999998\n",
            "Epoch 12/60\n",
            "98/98 [==============================] - 162s 2s/step - loss: 0.6074 - accuracy: 0.8490\n",
            "epoch  13 : setting learning rate to  0.24999999999999997\n",
            "Epoch 13/60\n",
            "98/98 [==============================] - 161s 2s/step - loss: 0.6048 - accuracy: 0.8549\n",
            "epoch  14 : setting learning rate to  0.26875\n",
            "Epoch 14/60\n",
            "98/98 [==============================] - 163s 2s/step - loss: 0.6098 - accuracy: 0.8560\n",
            "epoch  15 : setting learning rate to  0.28750000000000003\n",
            "Epoch 15/60\n",
            "98/98 [==============================] - 165s 2s/step - loss: 0.6049 - accuracy: 0.8619\n",
            "epoch  16 : setting learning rate to  0.30625\n",
            "Epoch 16/60\n",
            "98/98 [==============================] - 164s 2s/step - loss: 0.6056 - accuracy: 0.8628\n",
            "epoch  17 : setting learning rate to  0.325\n",
            "Epoch 17/60\n",
            "98/98 [==============================] - 165s 2s/step - loss: 0.6174 - accuracy: 0.8655\n",
            "epoch  18 : setting learning rate to  0.34375\n",
            "Epoch 18/60\n",
            "98/98 [==============================] - 163s 2s/step - loss: 0.6198 - accuracy: 0.8680\n",
            "epoch  19 : setting learning rate to  0.3625\n",
            "Epoch 19/60\n",
            "98/98 [==============================] - 164s 2s/step - loss: 0.6258 - accuracy: 0.8706\n",
            "epoch  20 : setting learning rate to  0.38125000000000003\n",
            "Epoch 20/60\n",
            "98/98 [==============================] - 163s 2s/step - loss: 0.6350 - accuracy: 0.8695\n",
            "epoch  21 : setting learning rate to  0.4\n",
            "Epoch 21/60\n",
            "98/98 [==============================] - 163s 2s/step - loss: 0.6329 - accuracy: 0.8753\n",
            "epoch  22 : setting learning rate to  0.39\n",
            "Epoch 22/60\n",
            "98/98 [==============================] - 162s 2s/step - loss: 0.6203 - accuracy: 0.8814\n",
            "epoch  23 : setting learning rate to  0.38\n",
            "Epoch 23/60\n",
            "98/98 [==============================] - 162s 2s/step - loss: 0.6202 - accuracy: 0.8826\n",
            "epoch  24 : setting learning rate to  0.37\n",
            "Epoch 24/60\n",
            "98/98 [==============================] - 161s 2s/step - loss: 0.5928 - accuracy: 0.8940\n",
            "epoch  25 : setting learning rate to  0.36000000000000004\n",
            "Epoch 25/60\n",
            "98/98 [==============================] - 163s 2s/step - loss: 0.5899 - accuracy: 0.8953\n",
            "epoch  26 : setting learning rate to  0.35000000000000003\n",
            "Epoch 26/60\n",
            "98/98 [==============================] - 164s 2s/step - loss: 0.5732 - accuracy: 0.8999\n",
            "epoch  27 : setting learning rate to  0.34\n",
            "Epoch 27/60\n",
            "98/98 [==============================] - 164s 2s/step - loss: 0.5701 - accuracy: 0.9015\n",
            "epoch  28 : setting learning rate to  0.33\n",
            "Epoch 28/60\n",
            "98/98 [==============================] - 163s 2s/step - loss: 0.5517 - accuracy: 0.9080\n",
            "epoch  29 : setting learning rate to  0.32\n",
            "Epoch 29/60\n",
            "98/98 [==============================] - 163s 2s/step - loss: 0.5512 - accuracy: 0.9073\n",
            "epoch  30 : setting learning rate to  0.31000000000000005\n",
            "Epoch 30/60\n",
            "98/98 [==============================] - 162s 2s/step - loss: 0.5362 - accuracy: 0.9123\n",
            "epoch  31 : setting learning rate to  0.30000000000000004\n",
            "Epoch 31/60\n",
            "98/98 [==============================] - 163s 2s/step - loss: 0.5265 - accuracy: 0.9142\n",
            "epoch  32 : setting learning rate to  0.29000000000000004\n",
            "Epoch 32/60\n",
            "98/98 [==============================] - 164s 2s/step - loss: 0.5202 - accuracy: 0.9168\n",
            "epoch  33 : setting learning rate to  0.28\n",
            "Epoch 33/60\n",
            "98/98 [==============================] - 165s 2s/step - loss: 0.5093 - accuracy: 0.9189\n",
            "epoch  34 : setting learning rate to  0.27\n",
            "Epoch 34/60\n",
            "98/98 [==============================] - 166s 2s/step - loss: 0.5000 - accuracy: 0.9224\n",
            "epoch  35 : setting learning rate to  0.26\n",
            "Epoch 35/60\n",
            "98/98 [==============================] - 164s 2s/step - loss: 0.4926 - accuracy: 0.9243\n",
            "epoch  36 : setting learning rate to  0.25\n",
            "Epoch 36/60\n",
            "98/98 [==============================] - 164s 2s/step - loss: 0.4891 - accuracy: 0.9253\n",
            "epoch  37 : setting learning rate to  0.24000000000000002\n",
            "Epoch 37/60\n",
            "98/98 [==============================] - 164s 2s/step - loss: 0.4733 - accuracy: 0.9311\n",
            "epoch  38 : setting learning rate to  0.23\n",
            "Epoch 38/60\n",
            "98/98 [==============================] - 164s 2s/step - loss: 0.4631 - accuracy: 0.9332\n",
            "epoch  39 : setting learning rate to  0.22000000000000003\n",
            "Epoch 39/60\n",
            "98/98 [==============================] - 163s 2s/step - loss: 0.4543 - accuracy: 0.9357\n",
            "epoch  40 : setting learning rate to  0.21000000000000002\n",
            "Epoch 40/60\n",
            "98/98 [==============================] - 164s 2s/step - loss: 0.4480 - accuracy: 0.9361\n",
            "epoch  41 : setting learning rate to  0.2\n",
            "Epoch 41/60\n",
            "98/98 [==============================] - 164s 2s/step - loss: 0.4325 - accuracy: 0.9405\n",
            "epoch  42 : setting learning rate to  0.19000000000000003\n",
            "Epoch 42/60\n",
            "98/98 [==============================] - 164s 2s/step - loss: 0.4241 - accuracy: 0.9425\n",
            "epoch  43 : setting learning rate to  0.18000000000000002\n",
            "Epoch 43/60\n",
            "98/98 [==============================] - 164s 2s/step - loss: 0.4135 - accuracy: 0.9452\n",
            "epoch  44 : setting learning rate to  0.17\n",
            "Epoch 44/60\n",
            "98/98 [==============================] - 163s 2s/step - loss: 0.4038 - accuracy: 0.9479\n",
            "epoch  45 : setting learning rate to  0.16000000000000003\n",
            "Epoch 45/60\n",
            "98/98 [==============================] - 161s 2s/step - loss: 0.3919 - accuracy: 0.9496\n",
            "epoch  46 : setting learning rate to  0.15000000000000002\n",
            "Epoch 46/60\n",
            "98/98 [==============================] - 164s 2s/step - loss: 0.3794 - accuracy: 0.9535\n",
            "epoch  47 : setting learning rate to  0.14\n",
            "Epoch 47/60\n",
            "98/98 [==============================] - 165s 2s/step - loss: 0.3674 - accuracy: 0.9563\n",
            "epoch  48 : setting learning rate to  0.13\n",
            "Epoch 48/60\n",
            "98/98 [==============================] - 165s 2s/step - loss: 0.3517 - accuracy: 0.9595\n",
            "epoch  49 : setting learning rate to  0.12\n",
            "Epoch 49/60\n",
            "98/98 [==============================] - 164s 2s/step - loss: 0.3415 - accuracy: 0.9629\n",
            "epoch  50 : setting learning rate to  0.11000000000000004\n",
            "Epoch 50/60\n",
            "98/98 [==============================] - 163s 2s/step - loss: 0.3304 - accuracy: 0.9647\n",
            "epoch  51 : setting learning rate to  0.10000000000000003\n",
            "Epoch 51/60\n",
            "98/98 [==============================] - 164s 2s/step - loss: 0.3131 - accuracy: 0.9691\n",
            "epoch  52 : setting learning rate to  0.09000000000000002\n",
            "Epoch 52/60\n",
            "98/98 [==============================] - 163s 2s/step - loss: 0.3021 - accuracy: 0.9713\n",
            "epoch  53 : setting learning rate to  0.08000000000000002\n",
            "Epoch 53/60\n",
            "98/98 [==============================] - 163s 2s/step - loss: 0.2885 - accuracy: 0.9751\n",
            "epoch  54 : setting learning rate to  0.07\n",
            "Epoch 54/60\n",
            "98/98 [==============================] - 164s 2s/step - loss: 0.2778 - accuracy: 0.9771\n",
            "epoch  55 : setting learning rate to  0.06\n",
            "Epoch 55/60\n",
            "98/98 [==============================] - 166s 2s/step - loss: 0.2667 - accuracy: 0.9802\n",
            "epoch  56 : setting learning rate to  0.04999999999999999\n",
            "Epoch 56/60\n",
            "98/98 [==============================] - 164s 2s/step - loss: 0.2582 - accuracy: 0.9820\n",
            "epoch  57 : setting learning rate to  0.040000000000000036\n",
            "Epoch 57/60\n",
            "98/98 [==============================] - 163s 2s/step - loss: 0.2490 - accuracy: 0.9838\n",
            "epoch  58 : setting learning rate to  0.030000000000000027\n",
            "Epoch 58/60\n",
            "98/98 [==============================] - 162s 2s/step - loss: 0.2377 - accuracy: 0.9875\n",
            "epoch  59 : setting learning rate to  0.020000000000000018\n",
            "Epoch 59/60\n",
            "98/98 [==============================] - 161s 2s/step - loss: 0.2304 - accuracy: 0.9893\n",
            "epoch  60 : setting learning rate to  0.010000000000000009\n",
            "Epoch 60/60\n",
            "98/98 [==============================] - 160s 2s/step - loss: 0.2270 - accuracy: 0.9904\n",
            "is_training False\n",
            "inside validation cycle\n",
            "===============\n",
            "\n",
            "20/20 [==============================] - 3s 125ms/step - loss: 0.3849 - accuracy: 0.9441\n",
            "val accuracy score at the end of training model type  4 [0.38492451012134554, 0.9441406]\n",
            "=========================================\n",
            "\n",
            "is_training True\n",
            "is_training True\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 32, 32, 64)   1728        input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 32, 32, 64)   256         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 32, 32, 64)   0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 32, 32, 128)  73728       activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 32, 32, 128)  512         conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 32, 32, 128)  0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2D)  (None, 16, 16, 128)  0           activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 16, 16, 128)  147456      max_pooling2d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 16, 16, 128)  512         conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 16, 16, 128)  0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 16, 16, 128)  147456      activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 16, 16, 128)  512         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 16, 16, 128)  0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_4 (TensorFlowOp [(None, 16, 16, 128) 0           max_pooling2d_6[0][0]            \n",
            "                                                                 activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_4 (Lambda)               (None, 16, 16, 128)  0           tf_op_layer_add_4[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 16, 16, 256)  294912      lambda_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 16, 16, 256)  1024        conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 16, 16, 256)  0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2D)  (None, 8, 8, 256)    0           activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_5 (Lambda)               (None, 8, 8, 256)    0           max_pooling2d_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 8, 8, 512)    1179648     lambda_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 8, 8, 512)    2048        conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 8, 8, 512)    0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2D)  (None, 4, 4, 512)    0           activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 4, 4, 512)    2359296     max_pooling2d_8[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 4, 4, 512)    2048        conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 4, 4, 512)    0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 4, 4, 512)    2359296     activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 4, 4, 512)    2048        conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 4, 4, 512)    0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_5 (TensorFlowOp [(None, 4, 4, 512)]  0           max_pooling2d_8[0][0]            \n",
            "                                                                 activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling2d_2 (GlobalM (None, 512)          0           tf_op_layer_add_5[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 10)           5120        global_max_pooling2d_2[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "lambda_6 (Lambda)               (None, 10)           0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 10)           0           lambda_6[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 6,577,600\n",
            "Trainable params: 6,573,120\n",
            "Non-trainable params: 4,480\n",
            "__________________________________________________________________________________________________\n",
            "Model will be trained with distortion after Res Blk 1 and Res Blk 2\n",
            "=======================\n",
            "Train for 98.0 steps\n",
            "epoch  1 : setting learning rate to  0.025\n",
            "Epoch 1/60\n",
            "is_training True\n",
            "is_training True\n",
            "is_training True\n",
            "is_training True\n",
            "98/98 [==============================] - 265s 3s/step - loss: 1.6646 - accuracy: 0.4302\n",
            "epoch  2 : setting learning rate to  0.04375\n",
            "Epoch 2/60\n",
            "98/98 [==============================] - 261s 3s/step - loss: 1.0891 - accuracy: 0.6453\n",
            "epoch  3 : setting learning rate to  0.0625\n",
            "Epoch 3/60\n",
            "98/98 [==============================] - 263s 3s/step - loss: 0.8461 - accuracy: 0.7365\n",
            "epoch  4 : setting learning rate to  0.08124999999999999\n",
            "Epoch 4/60\n",
            "98/98 [==============================] - 262s 3s/step - loss: 0.7241 - accuracy: 0.7848\n",
            "epoch  5 : setting learning rate to  0.1\n",
            "Epoch 5/60\n",
            "98/98 [==============================] - 260s 3s/step - loss: 0.6472 - accuracy: 0.8138\n",
            "epoch  6 : setting learning rate to  0.11875\n",
            "Epoch 6/60\n",
            "98/98 [==============================] - 262s 3s/step - loss: 0.5982 - accuracy: 0.8335\n",
            "epoch  7 : setting learning rate to  0.13749999999999998\n",
            "Epoch 7/60\n",
            "98/98 [==============================] - 262s 3s/step - loss: 0.5607 - accuracy: 0.8490\n",
            "epoch  8 : setting learning rate to  0.15625\n",
            "Epoch 8/60\n",
            "98/98 [==============================] - 259s 3s/step - loss: 0.5351 - accuracy: 0.8612\n",
            "epoch  9 : setting learning rate to  0.175\n",
            "Epoch 9/60\n",
            "98/98 [==============================] - 259s 3s/step - loss: 0.5103 - accuracy: 0.8734\n",
            "epoch  10 : setting learning rate to  0.19374999999999998\n",
            "Epoch 10/60\n",
            "98/98 [==============================] - 260s 3s/step - loss: 0.4937 - accuracy: 0.8810\n",
            "epoch  11 : setting learning rate to  0.2125\n",
            "Epoch 11/60\n",
            "98/98 [==============================] - 258s 3s/step - loss: 0.4896 - accuracy: 0.8872\n",
            "epoch  12 : setting learning rate to  0.23124999999999998\n",
            "Epoch 12/60\n",
            "98/98 [==============================] - 259s 3s/step - loss: 0.4719 - accuracy: 0.8964\n",
            "epoch  13 : setting learning rate to  0.24999999999999997\n",
            "Epoch 13/60\n",
            "98/98 [==============================] - 259s 3s/step - loss: 0.4669 - accuracy: 0.9012\n",
            "epoch  14 : setting learning rate to  0.26875\n",
            "Epoch 14/60\n",
            "98/98 [==============================] - 259s 3s/step - loss: 0.4744 - accuracy: 0.9023\n",
            "epoch  15 : setting learning rate to  0.28750000000000003\n",
            "Epoch 15/60\n",
            "98/98 [==============================] - 260s 3s/step - loss: 0.4776 - accuracy: 0.9040\n",
            "epoch  16 : setting learning rate to  0.30625\n",
            "Epoch 16/60\n",
            "98/98 [==============================] - 262s 3s/step - loss: 0.4766 - accuracy: 0.9098\n",
            "epoch  17 : setting learning rate to  0.325\n",
            "Epoch 17/60\n",
            "98/98 [==============================] - 263s 3s/step - loss: 0.4757 - accuracy: 0.9137\n",
            "epoch  18 : setting learning rate to  0.34375\n",
            "Epoch 18/60\n",
            "98/98 [==============================] - 263s 3s/step - loss: 0.4793 - accuracy: 0.9158\n",
            "epoch  19 : setting learning rate to  0.3625\n",
            "Epoch 19/60\n",
            "98/98 [==============================] - 262s 3s/step - loss: 0.4909 - accuracy: 0.9139\n",
            "epoch  20 : setting learning rate to  0.38125000000000003\n",
            "Epoch 20/60\n",
            "98/98 [==============================] - 262s 3s/step - loss: 0.5031 - accuracy: 0.9157\n",
            "epoch  21 : setting learning rate to  0.4\n",
            "Epoch 21/60\n",
            "98/98 [==============================] - 261s 3s/step - loss: 0.5098 - accuracy: 0.9170\n",
            "epoch  22 : setting learning rate to  0.39\n",
            "Epoch 22/60\n",
            "98/98 [==============================] - 261s 3s/step - loss: 0.4924 - accuracy: 0.9245\n",
            "epoch  23 : setting learning rate to  0.38\n",
            "Epoch 23/60\n",
            "98/98 [==============================] - 258s 3s/step - loss: 0.4827 - accuracy: 0.9296\n",
            "epoch  24 : setting learning rate to  0.37\n",
            "Epoch 24/60\n",
            "98/98 [==============================] - 261s 3s/step - loss: 0.4745 - accuracy: 0.9342\n",
            "epoch  25 : setting learning rate to  0.36000000000000004\n",
            "Epoch 25/60\n",
            "98/98 [==============================] - 263s 3s/step - loss: 0.4602 - accuracy: 0.9389\n",
            "epoch  26 : setting learning rate to  0.35000000000000003\n",
            "Epoch 26/60\n",
            "98/98 [==============================] - 262s 3s/step - loss: 0.4531 - accuracy: 0.9416\n",
            "epoch  27 : setting learning rate to  0.34\n",
            "Epoch 27/60\n",
            "98/98 [==============================] - 260s 3s/step - loss: 0.4533 - accuracy: 0.9426\n",
            "epoch  28 : setting learning rate to  0.33\n",
            "Epoch 28/60\n",
            "98/98 [==============================] - 262s 3s/step - loss: 0.4291 - accuracy: 0.9491\n",
            "epoch  29 : setting learning rate to  0.32\n",
            "Epoch 29/60\n",
            "98/98 [==============================] - 259s 3s/step - loss: 0.4234 - accuracy: 0.9497\n",
            "epoch  30 : setting learning rate to  0.31000000000000005\n",
            "Epoch 30/60\n",
            "98/98 [==============================] - 260s 3s/step - loss: 0.4172 - accuracy: 0.9510\n",
            "epoch  31 : setting learning rate to  0.30000000000000004\n",
            "Epoch 31/60\n",
            "98/98 [==============================] - 259s 3s/step - loss: 0.4065 - accuracy: 0.9540\n",
            "epoch  32 : setting learning rate to  0.29000000000000004\n",
            "Epoch 32/60\n",
            "98/98 [==============================] - 259s 3s/step - loss: 0.3969 - accuracy: 0.9569\n",
            "epoch  33 : setting learning rate to  0.28\n",
            "Epoch 33/60\n",
            "98/98 [==============================] - 260s 3s/step - loss: 0.3865 - accuracy: 0.9606\n",
            "epoch  34 : setting learning rate to  0.27\n",
            "Epoch 34/60\n",
            "98/98 [==============================] - 261s 3s/step - loss: 0.3779 - accuracy: 0.9620\n",
            "epoch  35 : setting learning rate to  0.26\n",
            "Epoch 35/60\n",
            "98/98 [==============================] - 261s 3s/step - loss: 0.3698 - accuracy: 0.9621\n",
            "epoch  36 : setting learning rate to  0.25\n",
            "Epoch 36/60\n",
            "98/98 [==============================] - 262s 3s/step - loss: 0.3543 - accuracy: 0.9664\n",
            "epoch  37 : setting learning rate to  0.24000000000000002\n",
            "Epoch 37/60\n",
            "98/98 [==============================] - 264s 3s/step - loss: 0.3445 - accuracy: 0.9686\n",
            "epoch  38 : setting learning rate to  0.23\n",
            "Epoch 38/60\n",
            "98/98 [==============================] - 265s 3s/step - loss: 0.3385 - accuracy: 0.9689\n",
            "epoch  39 : setting learning rate to  0.22000000000000003\n",
            "Epoch 39/60\n",
            "98/98 [==============================] - 262s 3s/step - loss: 0.3275 - accuracy: 0.9721\n",
            "epoch  40 : setting learning rate to  0.21000000000000002\n",
            "Epoch 40/60\n",
            "98/98 [==============================] - 262s 3s/step - loss: 0.3109 - accuracy: 0.9754\n",
            "epoch  41 : setting learning rate to  0.2\n",
            "Epoch 41/60\n",
            "98/98 [==============================] - 260s 3s/step - loss: 0.3018 - accuracy: 0.9766\n",
            "epoch  42 : setting learning rate to  0.19000000000000003\n",
            "Epoch 42/60\n",
            "98/98 [==============================] - 261s 3s/step - loss: 0.2934 - accuracy: 0.9776\n",
            "epoch  43 : setting learning rate to  0.18000000000000002\n",
            "Epoch 43/60\n",
            "98/98 [==============================] - 261s 3s/step - loss: 0.2808 - accuracy: 0.9799\n",
            "epoch  44 : setting learning rate to  0.17\n",
            "Epoch 44/60\n",
            "98/98 [==============================] - 261s 3s/step - loss: 0.2682 - accuracy: 0.9822\n",
            "epoch  45 : setting learning rate to  0.16000000000000003\n",
            "Epoch 45/60\n",
            "98/98 [==============================] - 262s 3s/step - loss: 0.2573 - accuracy: 0.9839\n",
            "epoch  46 : setting learning rate to  0.15000000000000002\n",
            "Epoch 46/60\n",
            "98/98 [==============================] - 260s 3s/step - loss: 0.2477 - accuracy: 0.9855\n",
            "epoch  47 : setting learning rate to  0.14\n",
            "Epoch 47/60\n",
            "98/98 [==============================] - 261s 3s/step - loss: 0.2354 - accuracy: 0.9880\n",
            "epoch  48 : setting learning rate to  0.13\n",
            "Epoch 48/60\n",
            "98/98 [==============================] - 263s 3s/step - loss: 0.2285 - accuracy: 0.9874\n",
            "epoch  49 : setting learning rate to  0.12\n",
            "Epoch 49/60\n",
            "98/98 [==============================] - 264s 3s/step - loss: 0.2188 - accuracy: 0.9889\n",
            "epoch  50 : setting learning rate to  0.11000000000000004\n",
            "Epoch 50/60\n",
            "98/98 [==============================] - 263s 3s/step - loss: 0.2063 - accuracy: 0.9915\n",
            "epoch  51 : setting learning rate to  0.10000000000000003\n",
            "Epoch 51/60\n",
            "98/98 [==============================] - 264s 3s/step - loss: 0.1981 - accuracy: 0.9924\n",
            "epoch  52 : setting learning rate to  0.09000000000000002\n",
            "Epoch 52/60\n",
            "98/98 [==============================] - 261s 3s/step - loss: 0.1880 - accuracy: 0.9941\n",
            "epoch  53 : setting learning rate to  0.08000000000000002\n",
            "Epoch 53/60\n",
            "98/98 [==============================] - 265s 3s/step - loss: 0.1799 - accuracy: 0.9953\n",
            "epoch  54 : setting learning rate to  0.07\n",
            "Epoch 54/60\n",
            "98/98 [==============================] - 263s 3s/step - loss: 0.1715 - accuracy: 0.9966\n",
            "epoch  55 : setting learning rate to  0.06\n",
            "Epoch 55/60\n",
            "98/98 [==============================] - 263s 3s/step - loss: 0.1642 - accuracy: 0.9976\n",
            "epoch  56 : setting learning rate to  0.04999999999999999\n",
            "Epoch 56/60\n",
            "98/98 [==============================] - 265s 3s/step - loss: 0.1583 - accuracy: 0.9981\n",
            "epoch  57 : setting learning rate to  0.040000000000000036\n",
            "Epoch 57/60\n",
            "98/98 [==============================] - 263s 3s/step - loss: 0.1541 - accuracy: 0.9986\n",
            "epoch  58 : setting learning rate to  0.030000000000000027\n",
            "Epoch 58/60\n",
            "98/98 [==============================] - 262s 3s/step - loss: 0.1518 - accuracy: 0.9983\n",
            "epoch  59 : setting learning rate to  0.020000000000000018\n",
            "Epoch 59/60\n",
            "98/98 [==============================] - 261s 3s/step - loss: 0.1493 - accuracy: 0.9985\n",
            "epoch  60 : setting learning rate to  0.010000000000000009\n",
            "Epoch 60/60\n",
            "98/98 [==============================] - 260s 3s/step - loss: 0.1468 - accuracy: 0.9992\n",
            "is_training False\n",
            "inside validation cycle\n",
            "===============\n",
            "\n",
            "is_training False\n",
            "inside validation cycle\n",
            "===============\n",
            "\n",
            "20/20 [==============================] - 3s 125ms/step - loss: 0.4580 - accuracy: 0.9240\n",
            "val accuracy score at the end of training model type  3 [0.4580035135149956, 0.92402345]\n",
            "=========================================\n",
            "\n",
            "is_training True\n",
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 32, 32, 64)   1728        input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 32, 32, 64)   256         conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 32, 32, 64)   0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 32, 32, 128)  73728       activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 32, 32, 128)  512         conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 32, 32, 128)  0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2D)  (None, 16, 16, 128)  0           activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 16, 16, 128)  147456      max_pooling2d_9[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 16, 16, 128)  512         conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 16, 16, 128)  0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 16, 16, 128)  147456      activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 16, 16, 128)  512         conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 16, 16, 128)  0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_6 (TensorFlowOp [(None, 16, 16, 128) 0           max_pooling2d_9[0][0]            \n",
            "                                                                 activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 16, 16, 256)  294912      tf_op_layer_add_6[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 16, 16, 256)  1024        conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 16, 16, 256)  0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling2D) (None, 8, 8, 256)    0           activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_7 (Lambda)               (None, 8, 8, 256)    0           max_pooling2d_10[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 8, 8, 512)    1179648     lambda_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 8, 8, 512)    2048        conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 8, 8, 512)    0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling2D) (None, 4, 4, 512)    0           activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 4, 4, 512)    2359296     max_pooling2d_11[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 4, 4, 512)    2048        conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 4, 4, 512)    0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 4, 4, 512)    2359296     activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 4, 4, 512)    2048        conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 4, 4, 512)    0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_7 (TensorFlowOp [(None, 4, 4, 512)]  0           max_pooling2d_11[0][0]           \n",
            "                                                                 activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling2d_3 (GlobalM (None, 512)          0           tf_op_layer_add_7[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 10)           5120        global_max_pooling2d_3[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "lambda_8 (Lambda)               (None, 10)           0           dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 10)           0           lambda_8[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 6,577,600\n",
            "Trainable params: 6,573,120\n",
            "Non-trainable params: 4,480\n",
            "__________________________________________________________________________________________________\n",
            "Model will be trained with distortion after Res Blk 2\n",
            "=======================\n",
            "Train for 98.0 steps\n",
            "epoch  1 : setting learning rate to  0.025\n",
            "Epoch 1/60\n",
            "is_training True\n",
            "is_training True\n",
            "98/98 [==============================] - 151s 2s/step - loss: 1.5962 - accuracy: 0.4592\n",
            "epoch  2 : setting learning rate to  0.04375\n",
            "Epoch 2/60\n",
            "98/98 [==============================] - 149s 2s/step - loss: 1.0163 - accuracy: 0.6743\n",
            "epoch  3 : setting learning rate to  0.0625\n",
            "Epoch 3/60\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.7610 - accuracy: 0.7700\n",
            "epoch  4 : setting learning rate to  0.08124999999999999\n",
            "Epoch 4/60\n",
            "98/98 [==============================] - 149s 2s/step - loss: 0.6380 - accuracy: 0.8160\n",
            "epoch  5 : setting learning rate to  0.1\n",
            "Epoch 5/60\n",
            "98/98 [==============================] - 149s 2s/step - loss: 0.5664 - accuracy: 0.8423\n",
            "epoch  6 : setting learning rate to  0.11875\n",
            "Epoch 6/60\n",
            "98/98 [==============================] - 149s 2s/step - loss: 0.5120 - accuracy: 0.8638\n",
            "epoch  7 : setting learning rate to  0.13749999999999998\n",
            "Epoch 7/60\n",
            "98/98 [==============================] - 149s 2s/step - loss: 0.4708 - accuracy: 0.8815\n",
            "epoch  8 : setting learning rate to  0.15625\n",
            "Epoch 8/60\n",
            "98/98 [==============================] - 149s 2s/step - loss: 0.4392 - accuracy: 0.8950\n",
            "epoch  9 : setting learning rate to  0.175\n",
            "Epoch 9/60\n",
            "98/98 [==============================] - 149s 2s/step - loss: 0.4203 - accuracy: 0.9059\n",
            "epoch  10 : setting learning rate to  0.19374999999999998\n",
            "Epoch 10/60\n",
            "98/98 [==============================] - 148s 2s/step - loss: 0.4034 - accuracy: 0.9146\n",
            "epoch  11 : setting learning rate to  0.2125\n",
            "Epoch 11/60\n",
            "98/98 [==============================] - 148s 2s/step - loss: 0.3922 - accuracy: 0.9229\n",
            "epoch  12 : setting learning rate to  0.23124999999999998\n",
            "Epoch 12/60\n",
            "98/98 [==============================] - 147s 2s/step - loss: 0.3895 - accuracy: 0.9278\n",
            "epoch  13 : setting learning rate to  0.24999999999999997\n",
            "Epoch 13/60\n",
            "98/98 [==============================] - 148s 2s/step - loss: 0.3843 - accuracy: 0.9337\n",
            "epoch  14 : setting learning rate to  0.26875\n",
            "Epoch 14/60\n",
            "98/98 [==============================] - 147s 2s/step - loss: 0.3868 - accuracy: 0.9378\n",
            "epoch  15 : setting learning rate to  0.28750000000000003\n",
            "Epoch 15/60\n",
            "98/98 [==============================] - 149s 2s/step - loss: 0.3831 - accuracy: 0.9424\n",
            "epoch  16 : setting learning rate to  0.30625\n",
            "Epoch 16/60\n",
            "98/98 [==============================] - 148s 2s/step - loss: 0.3939 - accuracy: 0.9424\n",
            "epoch  17 : setting learning rate to  0.325\n",
            "Epoch 17/60\n",
            "98/98 [==============================] - 149s 2s/step - loss: 0.3926 - accuracy: 0.9483\n",
            "epoch  18 : setting learning rate to  0.34375\n",
            "Epoch 18/60\n",
            "98/98 [==============================] - 149s 2s/step - loss: 0.4093 - accuracy: 0.9458\n",
            "epoch  19 : setting learning rate to  0.3625\n",
            "Epoch 19/60\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.4170 - accuracy: 0.9469\n",
            "epoch  20 : setting learning rate to  0.38125000000000003\n",
            "Epoch 20/60\n",
            "98/98 [==============================] - 149s 2s/step - loss: 0.4308 - accuracy: 0.9458\n",
            "epoch  21 : setting learning rate to  0.4\n",
            "Epoch 21/60\n",
            "98/98 [==============================] - 149s 2s/step - loss: 0.4495 - accuracy: 0.9450\n",
            "epoch  22 : setting learning rate to  0.39\n",
            "Epoch 22/60\n",
            "98/98 [==============================] - 148s 2s/step - loss: 0.4323 - accuracy: 0.9541\n",
            "epoch  23 : setting learning rate to  0.38\n",
            "Epoch 23/60\n",
            "98/98 [==============================] - 147s 2s/step - loss: 0.4142 - accuracy: 0.9601\n",
            "epoch  24 : setting learning rate to  0.37\n",
            "Epoch 24/60\n",
            "98/98 [==============================] - 145s 1s/step - loss: 0.3989 - accuracy: 0.9651\n",
            "epoch  25 : setting learning rate to  0.36000000000000004\n",
            "Epoch 25/60\n",
            "98/98 [==============================] - 141s 1s/step - loss: 0.3995 - accuracy: 0.9628\n",
            "epoch  26 : setting learning rate to  0.35000000000000003\n",
            "Epoch 26/60\n",
            "98/98 [==============================] - 139s 1s/step - loss: 0.3925 - accuracy: 0.9655\n",
            "epoch  27 : setting learning rate to  0.34\n",
            "Epoch 27/60\n",
            "98/98 [==============================] - 139s 1s/step - loss: 0.3683 - accuracy: 0.9723\n",
            "epoch  28 : setting learning rate to  0.33\n",
            "Epoch 28/60\n",
            "98/98 [==============================] - 139s 1s/step - loss: 0.3630 - accuracy: 0.9724\n",
            "epoch  29 : setting learning rate to  0.32\n",
            "Epoch 29/60\n",
            "98/98 [==============================] - 144s 1s/step - loss: 0.3515 - accuracy: 0.9737\n",
            "epoch  30 : setting learning rate to  0.31000000000000005\n",
            "Epoch 30/60\n",
            "98/98 [==============================] - 143s 1s/step - loss: 0.3466 - accuracy: 0.9742\n",
            "epoch  31 : setting learning rate to  0.30000000000000004\n",
            "Epoch 31/60\n",
            "98/98 [==============================] - 140s 1s/step - loss: 0.3282 - accuracy: 0.9793\n",
            "epoch  32 : setting learning rate to  0.29000000000000004\n",
            "Epoch 32/60\n",
            "98/98 [==============================] - 139s 1s/step - loss: 0.3230 - accuracy: 0.9782\n",
            "epoch  33 : setting learning rate to  0.28\n",
            "Epoch 33/60\n",
            "98/98 [==============================] - 139s 1s/step - loss: 0.3128 - accuracy: 0.9801\n",
            "epoch  34 : setting learning rate to  0.27\n",
            "Epoch 34/60\n",
            "98/98 [==============================] - 141s 1s/step - loss: 0.3062 - accuracy: 0.9803\n",
            "epoch  35 : setting learning rate to  0.26\n",
            "Epoch 35/60\n",
            "98/98 [==============================] - 142s 1s/step - loss: 0.2971 - accuracy: 0.9818\n",
            "epoch  36 : setting learning rate to  0.25\n",
            "Epoch 36/60\n",
            "98/98 [==============================] - 140s 1s/step - loss: 0.2829 - accuracy: 0.9840\n",
            "epoch  37 : setting learning rate to  0.24000000000000002\n",
            "Epoch 37/60\n",
            "98/98 [==============================] - 140s 1s/step - loss: 0.2733 - accuracy: 0.9860\n",
            "epoch  38 : setting learning rate to  0.23\n",
            "Epoch 38/60\n",
            "98/98 [==============================] - 139s 1s/step - loss: 0.2626 - accuracy: 0.9863\n",
            "epoch  39 : setting learning rate to  0.22000000000000003\n",
            "Epoch 39/60\n",
            "98/98 [==============================] - 138s 1s/step - loss: 0.2546 - accuracy: 0.9867\n",
            "epoch  40 : setting learning rate to  0.21000000000000002\n",
            "Epoch 40/60\n",
            "98/98 [==============================] - 137s 1s/step - loss: 0.2470 - accuracy: 0.9877\n",
            "epoch  41 : setting learning rate to  0.2\n",
            "Epoch 41/60\n",
            "98/98 [==============================] - 138s 1s/step - loss: 0.2344 - accuracy: 0.9899\n",
            "epoch  42 : setting learning rate to  0.19000000000000003\n",
            "Epoch 42/60\n",
            "98/98 [==============================] - 144s 1s/step - loss: 0.2259 - accuracy: 0.9901\n",
            "epoch  43 : setting learning rate to  0.18000000000000002\n",
            "Epoch 43/60\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.2156 - accuracy: 0.9918\n",
            "epoch  44 : setting learning rate to  0.17\n",
            "Epoch 44/60\n",
            "98/98 [==============================] - 149s 2s/step - loss: 0.2040 - accuracy: 0.9933\n",
            "epoch  45 : setting learning rate to  0.16000000000000003\n",
            "Epoch 45/60\n",
            "98/98 [==============================] - 149s 2s/step - loss: 0.1895 - accuracy: 0.9952\n",
            "epoch  46 : setting learning rate to  0.15000000000000002\n",
            "Epoch 46/60\n",
            "98/98 [==============================] - 143s 1s/step - loss: 0.1822 - accuracy: 0.9950\n",
            "epoch  47 : setting learning rate to  0.14\n",
            "Epoch 47/60\n",
            "98/98 [==============================] - 141s 1s/step - loss: 0.1719 - accuracy: 0.9963\n",
            "epoch  48 : setting learning rate to  0.13\n",
            "Epoch 48/60\n",
            "98/98 [==============================] - 140s 1s/step - loss: 0.1615 - accuracy: 0.9973\n",
            "epoch  49 : setting learning rate to  0.12\n",
            "Epoch 49/60\n",
            "98/98 [==============================] - 139s 1s/step - loss: 0.1532 - accuracy: 0.9979\n",
            "epoch  50 : setting learning rate to  0.11000000000000004\n",
            "Epoch 50/60\n",
            "98/98 [==============================] - 141s 1s/step - loss: 0.1442 - accuracy: 0.9987\n",
            "epoch  51 : setting learning rate to  0.10000000000000003\n",
            "Epoch 51/60\n",
            "98/98 [==============================] - 140s 1s/step - loss: 0.1368 - accuracy: 0.9990\n",
            "epoch  52 : setting learning rate to  0.09000000000000002\n",
            "Epoch 52/60\n",
            "98/98 [==============================] - 139s 1s/step - loss: 0.1297 - accuracy: 0.9995\n",
            "epoch  53 : setting learning rate to  0.08000000000000002\n",
            "Epoch 53/60\n",
            "98/98 [==============================] - 138s 1s/step - loss: 0.1249 - accuracy: 0.9994\n",
            "epoch  54 : setting learning rate to  0.07\n",
            "Epoch 54/60\n",
            "98/98 [==============================] - 140s 1s/step - loss: 0.1204 - accuracy: 0.9995\n",
            "epoch  55 : setting learning rate to  0.06\n",
            "Epoch 55/60\n",
            "98/98 [==============================] - 137s 1s/step - loss: 0.1166 - accuracy: 0.9996\n",
            "epoch  56 : setting learning rate to  0.04999999999999999\n",
            "Epoch 56/60\n",
            "98/98 [==============================] - 138s 1s/step - loss: 0.1135 - accuracy: 0.9997\n",
            "epoch  57 : setting learning rate to  0.040000000000000036\n",
            "Epoch 57/60\n",
            "20/98 [=====>........................] - ETA: 1:50 - loss: 0.1118 - accuracy: 0.9997"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfQwoED0hhm3",
        "colab_type": "text"
      },
      "source": [
        "### Runtime disconnected . So we will run the interrupted test combinations again - only the ones that were missed out"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e08d75e1-21e8-488b-8113-89485cd8ed47",
        "id": "KL7VHyVwfz-3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "for model_params in [0,1,2]:\n",
        "  is_training=True\n",
        "  #global_step = tf.train.get_or_create_global_step()\n",
        "  model=model=build_model(model_params)\n",
        "  opt=SGD(lr=0.025,momentum=0.9,nesterov=True)\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,metrics=['accuracy']\n",
        "              )\n",
        "  batch_size=512\n",
        "  \n",
        "  if model_params in [0,4,5]:\n",
        "    train_ds=train_ds1\n",
        "    print('Model will be trained with image augmentation in put layer\\n====================')\n",
        "    if model_params==4:\n",
        "      print('Model will also include distortion after Res Blk 1\\n=======================')\n",
        "    elif model_params==5:\n",
        "      print('Model will also include distortion after Res Blk 2\\n=======================')\n",
        "      \n",
        "  else:\n",
        "    if model_params==1:\n",
        "      print('Model will be trained with distortion after Res Blk 1\\n=======================')\n",
        "    elif model_params==2:\n",
        "      print('Model will be trained with distortion after Res Blk 2\\n=======================')\n",
        "    elif model_params==3:\n",
        "      print('Model will be trained with distortion after Res Blk 1 and Res Blk 2\\n=======================')    \n",
        "    train_ds=train_ds2  \n",
        "  model.fit(train_ds,epochs=EPOCHS, steps_per_epoch=np.ceil(50000/batch_size), \n",
        "          callbacks=[lr_sched],\n",
        "          verbose=1)\n",
        "  is_training=False\n",
        "  score=model.evaluate(test_ds, steps =np.ceil(10000/batch_size), verbose=1)\n",
        "\n",
        "  del(model)\n",
        "  del(train_ds)\n",
        "  \n",
        "  print('val accuracy score at the end of training model type ',model_params, score)\n",
        "  print(\"=========================================\\n\")\n",
        "\n",
        "#validation_data=test_ds, validation_steps=np.ceil(10000/batch_size),"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 32, 32, 64)   1728        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 32, 32, 64)   256         conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 32, 32, 64)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 128)  73728       activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 128)  512         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 128)  0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 16, 16, 128)  0           activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 16, 16, 128)  147456      max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 16, 16, 128)  512         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 16, 16, 128)  0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 16, 16, 128)  147456      activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 16, 16, 128)  512         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 16, 16, 128)  0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add (TensorFlowOpLa [(None, 16, 16, 128) 0           max_pooling2d[0][0]              \n",
            "                                                                 activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 16, 16, 256)  294912      tf_op_layer_add[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 16, 16, 256)  1024        conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 16, 16, 256)  0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 8, 8, 256)    0           activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 8, 8, 512)    1179648     max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 8, 8, 512)    2048        conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 8, 8, 512)    0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 4, 4, 512)    0           activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 4, 4, 512)    2359296     max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 4, 4, 512)    2048        conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 4, 4, 512)    0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 4, 4, 512)    2359296     activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 4, 4, 512)    2048        conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 4, 4, 512)    0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_1 (TensorFlowOp [(None, 4, 4, 512)]  0           max_pooling2d_2[0][0]            \n",
            "                                                                 activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling2d (GlobalMax (None, 512)          0           tf_op_layer_add_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 10)           5120        global_max_pooling2d[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, 10)           0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 10)           0           lambda[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 6,577,600\n",
            "Trainable params: 6,573,120\n",
            "Non-trainable params: 4,480\n",
            "__________________________________________________________________________________________________\n",
            "Model will be trained with image augmentation in put layer\n",
            "====================\n",
            "Train for 98.0 steps\n",
            "epoch  1 : setting learning rate to  0.025\n",
            "Epoch 1/60\n",
            "98/98 [==============================] - 48s 494ms/step - loss: 1.6773 - accuracy: 0.4271\n",
            "epoch  2 : setting learning rate to  0.04375\n",
            "Epoch 2/60\n",
            "98/98 [==============================] - 38s 392ms/step - loss: 1.1183 - accuracy: 0.6350\n",
            "epoch  3 : setting learning rate to  0.0625\n",
            "Epoch 3/60\n",
            "98/98 [==============================] - 39s 394ms/step - loss: 0.8717 - accuracy: 0.7302\n",
            "epoch  4 : setting learning rate to  0.08124999999999999\n",
            "Epoch 4/60\n",
            "98/98 [==============================] - 39s 394ms/step - loss: 0.7545 - accuracy: 0.7760\n",
            "epoch  5 : setting learning rate to  0.1\n",
            "Epoch 5/60\n",
            "98/98 [==============================] - 39s 394ms/step - loss: 0.6812 - accuracy: 0.8004\n",
            "epoch  6 : setting learning rate to  0.11875\n",
            "Epoch 6/60\n",
            "98/98 [==============================] - 39s 394ms/step - loss: 0.6370 - accuracy: 0.8211\n",
            "epoch  7 : setting learning rate to  0.13749999999999998\n",
            "Epoch 7/60\n",
            "98/98 [==============================] - 39s 393ms/step - loss: 0.6006 - accuracy: 0.8363\n",
            "epoch  8 : setting learning rate to  0.15625\n",
            "Epoch 8/60\n",
            "98/98 [==============================] - 39s 393ms/step - loss: 0.5772 - accuracy: 0.8465\n",
            "epoch  9 : setting learning rate to  0.175\n",
            "Epoch 9/60\n",
            "98/98 [==============================] - 39s 393ms/step - loss: 0.5591 - accuracy: 0.8570\n",
            "epoch  10 : setting learning rate to  0.19374999999999998\n",
            "Epoch 10/60\n",
            "98/98 [==============================] - 39s 393ms/step - loss: 0.5485 - accuracy: 0.8648\n",
            "epoch  11 : setting learning rate to  0.2125\n",
            "Epoch 11/60\n",
            "98/98 [==============================] - 39s 393ms/step - loss: 0.5376 - accuracy: 0.8730\n",
            "epoch  12 : setting learning rate to  0.23124999999999998\n",
            "Epoch 12/60\n",
            "98/98 [==============================] - 39s 393ms/step - loss: 0.5403 - accuracy: 0.8749\n",
            "epoch  13 : setting learning rate to  0.24999999999999997\n",
            "Epoch 13/60\n",
            "98/98 [==============================] - 39s 393ms/step - loss: 0.5405 - accuracy: 0.8783\n",
            "epoch  14 : setting learning rate to  0.26875\n",
            "Epoch 14/60\n",
            "98/98 [==============================] - 39s 393ms/step - loss: 0.5339 - accuracy: 0.8842\n",
            "epoch  15 : setting learning rate to  0.28750000000000003\n",
            "Epoch 15/60\n",
            "98/98 [==============================] - 39s 393ms/step - loss: 0.5450 - accuracy: 0.8847\n",
            "epoch  16 : setting learning rate to  0.30625\n",
            "Epoch 16/60\n",
            "98/98 [==============================] - 39s 393ms/step - loss: 0.5379 - accuracy: 0.8903\n",
            "epoch  17 : setting learning rate to  0.325\n",
            "Epoch 17/60\n",
            "98/98 [==============================] - 39s 394ms/step - loss: 0.5470 - accuracy: 0.8933\n",
            "epoch  18 : setting learning rate to  0.34375\n",
            "Epoch 18/60\n",
            "98/98 [==============================] - 38s 393ms/step - loss: 0.5531 - accuracy: 0.8921\n",
            "epoch  19 : setting learning rate to  0.3625\n",
            "Epoch 19/60\n",
            "98/98 [==============================] - 39s 393ms/step - loss: 0.5577 - accuracy: 0.8965\n",
            "epoch  20 : setting learning rate to  0.38125000000000003\n",
            "Epoch 20/60\n",
            "98/98 [==============================] - 39s 393ms/step - loss: 0.5708 - accuracy: 0.8941\n",
            "epoch  21 : setting learning rate to  0.4\n",
            "Epoch 21/60\n",
            "98/98 [==============================] - 39s 393ms/step - loss: 0.5693 - accuracy: 0.8994\n",
            "epoch  22 : setting learning rate to  0.39\n",
            "Epoch 22/60\n",
            "98/98 [==============================] - 39s 393ms/step - loss: 0.5613 - accuracy: 0.9051\n",
            "epoch  23 : setting learning rate to  0.38\n",
            "Epoch 23/60\n",
            "98/98 [==============================] - 39s 393ms/step - loss: 0.5476 - accuracy: 0.9103\n",
            "epoch  24 : setting learning rate to  0.37\n",
            "Epoch 24/60\n",
            "98/98 [==============================] - 39s 393ms/step - loss: 0.5349 - accuracy: 0.9144\n",
            "epoch  25 : setting learning rate to  0.36000000000000004\n",
            "Epoch 25/60\n",
            "98/98 [==============================] - 39s 393ms/step - loss: 0.5302 - accuracy: 0.9162\n",
            "epoch  26 : setting learning rate to  0.35000000000000003\n",
            "Epoch 26/60\n",
            "98/98 [==============================] - 39s 393ms/step - loss: 0.5218 - accuracy: 0.9180\n",
            "epoch  27 : setting learning rate to  0.34\n",
            "Epoch 27/60\n",
            "98/98 [==============================] - 39s 393ms/step - loss: 0.5113 - accuracy: 0.9231\n",
            "epoch  28 : setting learning rate to  0.33\n",
            "Epoch 28/60\n",
            "98/98 [==============================] - 39s 393ms/step - loss: 0.5037 - accuracy: 0.9252\n",
            "epoch  29 : setting learning rate to  0.32\n",
            "Epoch 29/60\n",
            "98/98 [==============================] - 38s 393ms/step - loss: 0.4858 - accuracy: 0.9315\n",
            "epoch  30 : setting learning rate to  0.31000000000000005\n",
            "Epoch 30/60\n",
            "98/98 [==============================] - 39s 393ms/step - loss: 0.4871 - accuracy: 0.9314\n",
            "epoch  31 : setting learning rate to  0.30000000000000004\n",
            "Epoch 31/60\n",
            "98/98 [==============================] - 38s 393ms/step - loss: 0.4750 - accuracy: 0.9356\n",
            "epoch  32 : setting learning rate to  0.29000000000000004\n",
            "Epoch 32/60\n",
            "98/98 [==============================] - 39s 393ms/step - loss: 0.4655 - accuracy: 0.9371\n",
            "epoch  33 : setting learning rate to  0.28\n",
            "Epoch 33/60\n",
            "98/98 [==============================] - 38s 393ms/step - loss: 0.4590 - accuracy: 0.9396\n",
            "epoch  34 : setting learning rate to  0.27\n",
            "Epoch 34/60\n",
            "98/98 [==============================] - 39s 393ms/step - loss: 0.4501 - accuracy: 0.9436\n",
            "epoch  35 : setting learning rate to  0.26\n",
            "Epoch 35/60\n",
            "98/98 [==============================] - 39s 393ms/step - loss: 0.4383 - accuracy: 0.9447\n",
            "epoch  36 : setting learning rate to  0.25\n",
            "Epoch 36/60\n",
            "98/98 [==============================] - 39s 393ms/step - loss: 0.4271 - accuracy: 0.9481\n",
            "epoch  37 : setting learning rate to  0.24000000000000002\n",
            "Epoch 37/60\n",
            "98/98 [==============================] - 38s 393ms/step - loss: 0.4216 - accuracy: 0.9494\n",
            "epoch  38 : setting learning rate to  0.23\n",
            "Epoch 38/60\n",
            "98/98 [==============================] - 39s 393ms/step - loss: 0.4071 - accuracy: 0.9535\n",
            "epoch  39 : setting learning rate to  0.22000000000000003\n",
            "Epoch 39/60\n",
            "98/98 [==============================] - 39s 393ms/step - loss: 0.4028 - accuracy: 0.9536\n",
            "epoch  40 : setting learning rate to  0.21000000000000002\n",
            "Epoch 40/60\n",
            "98/98 [==============================] - 38s 393ms/step - loss: 0.3900 - accuracy: 0.9561\n",
            "epoch  41 : setting learning rate to  0.2\n",
            "Epoch 41/60\n",
            "98/98 [==============================] - 39s 393ms/step - loss: 0.3808 - accuracy: 0.9585\n",
            "epoch  42 : setting learning rate to  0.19000000000000003\n",
            "Epoch 42/60\n",
            "98/98 [==============================] - 39s 394ms/step - loss: 0.3707 - accuracy: 0.9609\n",
            "epoch  43 : setting learning rate to  0.18000000000000002\n",
            "Epoch 43/60\n",
            "98/98 [==============================] - 38s 393ms/step - loss: 0.3590 - accuracy: 0.9636\n",
            "epoch  44 : setting learning rate to  0.17\n",
            "Epoch 44/60\n",
            "98/98 [==============================] - 38s 393ms/step - loss: 0.3513 - accuracy: 0.9649\n",
            "epoch  45 : setting learning rate to  0.16000000000000003\n",
            "Epoch 45/60\n",
            "98/98 [==============================] - 39s 393ms/step - loss: 0.3393 - accuracy: 0.9671\n",
            "epoch  46 : setting learning rate to  0.15000000000000002\n",
            "Epoch 46/60\n",
            "98/98 [==============================] - 38s 393ms/step - loss: 0.3279 - accuracy: 0.9708\n",
            "epoch  47 : setting learning rate to  0.14\n",
            "Epoch 47/60\n",
            "98/98 [==============================] - 38s 393ms/step - loss: 0.3150 - accuracy: 0.9740\n",
            "epoch  48 : setting learning rate to  0.13\n",
            "Epoch 48/60\n",
            "98/98 [==============================] - 39s 393ms/step - loss: 0.3004 - accuracy: 0.9765\n",
            "epoch  49 : setting learning rate to  0.12\n",
            "Epoch 49/60\n",
            "98/98 [==============================] - 38s 393ms/step - loss: 0.2896 - accuracy: 0.9784\n",
            "epoch  50 : setting learning rate to  0.11000000000000004\n",
            "Epoch 50/60\n",
            "98/98 [==============================] - 39s 393ms/step - loss: 0.2807 - accuracy: 0.9786\n",
            "epoch  51 : setting learning rate to  0.10000000000000003\n",
            "Epoch 51/60\n",
            "98/98 [==============================] - 39s 393ms/step - loss: 0.2677 - accuracy: 0.9822\n",
            "epoch  52 : setting learning rate to  0.09000000000000002\n",
            "Epoch 52/60\n",
            "98/98 [==============================] - 38s 393ms/step - loss: 0.2556 - accuracy: 0.9853\n",
            "epoch  53 : setting learning rate to  0.08000000000000002\n",
            "Epoch 53/60\n",
            "98/98 [==============================] - 39s 393ms/step - loss: 0.2468 - accuracy: 0.9862\n",
            "epoch  54 : setting learning rate to  0.07\n",
            "Epoch 54/60\n",
            "98/98 [==============================] - 38s 393ms/step - loss: 0.2353 - accuracy: 0.9887\n",
            "epoch  55 : setting learning rate to  0.06\n",
            "Epoch 55/60\n",
            "98/98 [==============================] - 38s 393ms/step - loss: 0.2249 - accuracy: 0.9911\n",
            "epoch  56 : setting learning rate to  0.04999999999999999\n",
            "Epoch 56/60\n",
            "98/98 [==============================] - 38s 393ms/step - loss: 0.2172 - accuracy: 0.9926\n",
            "epoch  57 : setting learning rate to  0.040000000000000036\n",
            "Epoch 57/60\n",
            "98/98 [==============================] - 38s 393ms/step - loss: 0.2094 - accuracy: 0.9943\n",
            "epoch  58 : setting learning rate to  0.030000000000000027\n",
            "Epoch 58/60\n",
            "98/98 [==============================] - 39s 393ms/step - loss: 0.2052 - accuracy: 0.9944\n",
            "epoch  59 : setting learning rate to  0.020000000000000018\n",
            "Epoch 59/60\n",
            "98/98 [==============================] - 38s 393ms/step - loss: 0.1999 - accuracy: 0.9955\n",
            "epoch  60 : setting learning rate to  0.010000000000000009\n",
            "Epoch 60/60\n",
            "98/98 [==============================] - 38s 393ms/step - loss: 0.1969 - accuracy: 0.9962\n",
            "20/20 [==============================] - 3s 135ms/step - loss: 0.3697 - accuracy: 0.9444\n",
            "val accuracy score at the end of training model type  0 [0.36970300078392027, 0.94443357]\n",
            "=========================================\n",
            "\n",
            "is_training True\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 32, 32, 64)   1728        input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 32, 32, 64)   256         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 32, 32, 64)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 32, 32, 128)  73728       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 32, 32, 128)  512         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 32, 32, 128)  0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 16, 16, 128)  0           activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 16, 16, 128)  147456      max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 16, 16, 128)  512         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 16, 16, 128)  0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 16, 16, 128)  147456      activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 16, 16, 128)  512         conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 16, 16, 128)  0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_2 (TensorFlowOp [(None, 16, 16, 128) 0           max_pooling2d_3[0][0]            \n",
            "                                                                 activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 16, 16, 128)  0           tf_op_layer_add_2[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 16, 16, 256)  294912      lambda_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 16, 16, 256)  1024        conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 16, 16, 256)  0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 8, 8, 256)    0           activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 8, 8, 512)    1179648     max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 8, 8, 512)    2048        conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 8, 8, 512)    0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2D)  (None, 4, 4, 512)    0           activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 4, 4, 512)    2359296     max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 4, 4, 512)    2048        conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 4, 4, 512)    0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 4, 4, 512)    2359296     activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 4, 4, 512)    2048        conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 4, 4, 512)    0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_3 (TensorFlowOp [(None, 4, 4, 512)]  0           max_pooling2d_5[0][0]            \n",
            "                                                                 activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling2d_1 (GlobalM (None, 512)          0           tf_op_layer_add_3[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 10)           5120        global_max_pooling2d_1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "lambda_2 (Lambda)               (None, 10)           0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 10)           0           lambda_2[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 6,577,600\n",
            "Trainable params: 6,573,120\n",
            "Non-trainable params: 4,480\n",
            "__________________________________________________________________________________________________\n",
            "Model will be trained with distortion after Res Blk 1\n",
            "=======================\n",
            "Train for 98.0 steps\n",
            "epoch  1 : setting learning rate to  0.025\n",
            "Epoch 1/60\n",
            "is_training True\n",
            "is_training True\n",
            "98/98 [==============================] - 152s 2s/step - loss: 1.5859 - accuracy: 0.4641\n",
            "epoch  2 : setting learning rate to  0.04375\n",
            "Epoch 2/60\n",
            "98/98 [==============================] - 149s 2s/step - loss: 1.0111 - accuracy: 0.6746\n",
            "epoch  3 : setting learning rate to  0.0625\n",
            "Epoch 3/60\n",
            "98/98 [==============================] - 149s 2s/step - loss: 0.7808 - accuracy: 0.7600\n",
            "epoch  4 : setting learning rate to  0.08124999999999999\n",
            "Epoch 4/60\n",
            "98/98 [==============================] - 149s 2s/step - loss: 0.6626 - accuracy: 0.8054\n",
            "epoch  5 : setting learning rate to  0.1\n",
            "Epoch 5/60\n",
            "98/98 [==============================] - 151s 2s/step - loss: 0.5863 - accuracy: 0.8360\n",
            "epoch  6 : setting learning rate to  0.11875\n",
            "Epoch 6/60\n",
            "98/98 [==============================] - 151s 2s/step - loss: 0.5377 - accuracy: 0.8566\n",
            "epoch  7 : setting learning rate to  0.13749999999999998\n",
            "Epoch 7/60\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.5063 - accuracy: 0.8690\n",
            "epoch  8 : setting learning rate to  0.15625\n",
            "Epoch 8/60\n",
            "98/98 [==============================] - 149s 2s/step - loss: 0.4789 - accuracy: 0.8815\n",
            "epoch  9 : setting learning rate to  0.175\n",
            "Epoch 9/60\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.4606 - accuracy: 0.8899\n",
            "epoch  10 : setting learning rate to  0.19374999999999998\n",
            "Epoch 10/60\n",
            "98/98 [==============================] - 149s 2s/step - loss: 0.4441 - accuracy: 0.8995\n",
            "epoch  11 : setting learning rate to  0.2125\n",
            "Epoch 11/60\n",
            "98/98 [==============================] - 148s 2s/step - loss: 0.4396 - accuracy: 0.9035\n",
            "epoch  12 : setting learning rate to  0.23124999999999998\n",
            "Epoch 12/60\n",
            "98/98 [==============================] - 149s 2s/step - loss: 0.4299 - accuracy: 0.9089\n",
            "epoch  13 : setting learning rate to  0.24999999999999997\n",
            "Epoch 13/60\n",
            "98/98 [==============================] - 149s 2s/step - loss: 0.4262 - accuracy: 0.9171\n",
            "epoch  14 : setting learning rate to  0.26875\n",
            "Epoch 14/60\n",
            "98/98 [==============================] - 149s 2s/step - loss: 0.4270 - accuracy: 0.9189\n",
            "epoch  15 : setting learning rate to  0.28750000000000003\n",
            "Epoch 15/60\n",
            "98/98 [==============================] - 149s 2s/step - loss: 0.4264 - accuracy: 0.9232\n",
            "epoch  16 : setting learning rate to  0.30625\n",
            "Epoch 16/60\n",
            "98/98 [==============================] - 148s 2s/step - loss: 0.4276 - accuracy: 0.9267\n",
            "epoch  17 : setting learning rate to  0.325\n",
            "Epoch 17/60\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.4415 - accuracy: 0.9258\n",
            "epoch  18 : setting learning rate to  0.34375\n",
            "Epoch 18/60\n",
            "98/98 [==============================] - 149s 2s/step - loss: 0.4494 - accuracy: 0.9258\n",
            "epoch  19 : setting learning rate to  0.3625\n",
            "Epoch 19/60\n",
            "98/98 [==============================] - 151s 2s/step - loss: 0.4480 - accuracy: 0.9305\n",
            "epoch  20 : setting learning rate to  0.38125000000000003\n",
            "Epoch 20/60\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.4564 - accuracy: 0.9309\n",
            "epoch  21 : setting learning rate to  0.4\n",
            "Epoch 21/60\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.4636 - accuracy: 0.9333\n",
            "epoch  22 : setting learning rate to  0.39\n",
            "Epoch 22/60\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.4547 - accuracy: 0.9388\n",
            "epoch  23 : setting learning rate to  0.38\n",
            "Epoch 23/60\n",
            "98/98 [==============================] - 151s 2s/step - loss: 0.4519 - accuracy: 0.9410\n",
            "epoch  24 : setting learning rate to  0.37\n",
            "Epoch 24/60\n",
            "98/98 [==============================] - 149s 2s/step - loss: 0.4272 - accuracy: 0.9503\n",
            "epoch  25 : setting learning rate to  0.36000000000000004\n",
            "Epoch 25/60\n",
            "98/98 [==============================] - 149s 2s/step - loss: 0.4221 - accuracy: 0.9512\n",
            "epoch  26 : setting learning rate to  0.35000000000000003\n",
            "Epoch 26/60\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.4078 - accuracy: 0.9556\n",
            "epoch  27 : setting learning rate to  0.34\n",
            "Epoch 27/60\n",
            "98/98 [==============================] - 151s 2s/step - loss: 0.3976 - accuracy: 0.9576\n",
            "epoch  28 : setting learning rate to  0.33\n",
            "Epoch 28/60\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.3930 - accuracy: 0.9584\n",
            "epoch  29 : setting learning rate to  0.32\n",
            "Epoch 29/60\n",
            "98/98 [==============================] - 151s 2s/step - loss: 0.3837 - accuracy: 0.9613\n",
            "epoch  30 : setting learning rate to  0.31000000000000005\n",
            "Epoch 30/60\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.3670 - accuracy: 0.9658\n",
            "epoch  31 : setting learning rate to  0.30000000000000004\n",
            "Epoch 31/60\n",
            "98/98 [==============================] - 148s 2s/step - loss: 0.3676 - accuracy: 0.9646\n",
            "epoch  32 : setting learning rate to  0.29000000000000004\n",
            "Epoch 32/60\n",
            "98/98 [==============================] - 149s 2s/step - loss: 0.3565 - accuracy: 0.9681\n",
            "epoch  33 : setting learning rate to  0.28\n",
            "Epoch 33/60\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.3396 - accuracy: 0.9715\n",
            "epoch  34 : setting learning rate to  0.27\n",
            "Epoch 34/60\n",
            "98/98 [==============================] - 149s 2s/step - loss: 0.3314 - accuracy: 0.9732\n",
            "epoch  35 : setting learning rate to  0.26\n",
            "Epoch 35/60\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.3217 - accuracy: 0.9749\n",
            "epoch  36 : setting learning rate to  0.25\n",
            "Epoch 36/60\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.3154 - accuracy: 0.9749\n",
            "epoch  37 : setting learning rate to  0.24000000000000002\n",
            "Epoch 37/60\n",
            "98/98 [==============================] - 148s 2s/step - loss: 0.3080 - accuracy: 0.9765\n",
            "epoch  38 : setting learning rate to  0.23\n",
            "Epoch 38/60\n",
            "98/98 [==============================] - 148s 2s/step - loss: 0.2967 - accuracy: 0.9787\n",
            "epoch  39 : setting learning rate to  0.22000000000000003\n",
            "Epoch 39/60\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.2811 - accuracy: 0.9819\n",
            "epoch  40 : setting learning rate to  0.21000000000000002\n",
            "Epoch 40/60\n",
            "98/98 [==============================] - 149s 2s/step - loss: 0.2738 - accuracy: 0.9824\n",
            "epoch  41 : setting learning rate to  0.2\n",
            "Epoch 41/60\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.2597 - accuracy: 0.9849\n",
            "epoch  42 : setting learning rate to  0.19000000000000003\n",
            "Epoch 42/60\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.2499 - accuracy: 0.9859\n",
            "epoch  43 : setting learning rate to  0.18000000000000002\n",
            "Epoch 43/60\n",
            "98/98 [==============================] - 149s 2s/step - loss: 0.2470 - accuracy: 0.9849\n",
            "epoch  44 : setting learning rate to  0.17\n",
            "Epoch 44/60\n",
            "98/98 [==============================] - 147s 2s/step - loss: 0.2322 - accuracy: 0.9887\n",
            "epoch  45 : setting learning rate to  0.16000000000000003\n",
            "Epoch 45/60\n",
            "98/98 [==============================] - 149s 2s/step - loss: 0.2206 - accuracy: 0.9903\n",
            "epoch  46 : setting learning rate to  0.15000000000000002\n",
            "Epoch 46/60\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.2081 - accuracy: 0.9926\n",
            "epoch  47 : setting learning rate to  0.14\n",
            "Epoch 47/60\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.2037 - accuracy: 0.9911\n",
            "epoch  48 : setting learning rate to  0.13\n",
            "Epoch 48/60\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.1893 - accuracy: 0.9944\n",
            "epoch  49 : setting learning rate to  0.12\n",
            "Epoch 49/60\n",
            "98/98 [==============================] - 149s 2s/step - loss: 0.1804 - accuracy: 0.9951\n",
            "epoch  50 : setting learning rate to  0.11000000000000004\n",
            "Epoch 50/60\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.1705 - accuracy: 0.9964\n",
            "epoch  51 : setting learning rate to  0.10000000000000003\n",
            "Epoch 51/60\n",
            "98/98 [==============================] - 151s 2s/step - loss: 0.1618 - accuracy: 0.9973\n",
            "epoch  52 : setting learning rate to  0.09000000000000002\n",
            "Epoch 52/60\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.1531 - accuracy: 0.9982\n",
            "epoch  53 : setting learning rate to  0.08000000000000002\n",
            "Epoch 53/60\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.1467 - accuracy: 0.9986\n",
            "epoch  54 : setting learning rate to  0.07\n",
            "Epoch 54/60\n",
            "98/98 [==============================] - 151s 2s/step - loss: 0.1410 - accuracy: 0.9987\n",
            "epoch  55 : setting learning rate to  0.06\n",
            "Epoch 55/60\n",
            "98/98 [==============================] - 149s 2s/step - loss: 0.1369 - accuracy: 0.9988\n",
            "epoch  56 : setting learning rate to  0.04999999999999999\n",
            "Epoch 56/60\n",
            "98/98 [==============================] - 151s 2s/step - loss: 0.1323 - accuracy: 0.9995\n",
            "epoch  57 : setting learning rate to  0.040000000000000036\n",
            "Epoch 57/60\n",
            "98/98 [==============================] - 149s 2s/step - loss: 0.1289 - accuracy: 0.9994\n",
            "epoch  58 : setting learning rate to  0.030000000000000027\n",
            "Epoch 58/60\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.1263 - accuracy: 0.9996\n",
            "epoch  59 : setting learning rate to  0.020000000000000018\n",
            "Epoch 59/60\n",
            "98/98 [==============================] - 149s 2s/step - loss: 0.1249 - accuracy: 0.9995\n",
            "epoch  60 : setting learning rate to  0.010000000000000009\n",
            "Epoch 60/60\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.1234 - accuracy: 0.9998\n",
            "is_training False\n",
            "inside validation cycle\n",
            "===============\n",
            "\n",
            "20/20 [==============================] - 3s 126ms/step - loss: 0.4273 - accuracy: 0.9236\n",
            "val accuracy score at the end of training model type  1 [0.4273167386651039, 0.9236328]\n",
            "=========================================\n",
            "\n",
            "is_training True\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 32, 32, 64)   1728        input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 32, 32, 64)   256         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 32, 32, 64)   0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 32, 32, 128)  73728       activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 32, 32, 128)  512         conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 32, 32, 128)  0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2D)  (None, 16, 16, 128)  0           activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 16, 16, 128)  147456      max_pooling2d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 16, 16, 128)  512         conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 16, 16, 128)  0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 16, 16, 128)  147456      activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 16, 16, 128)  512         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 16, 16, 128)  0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_4 (TensorFlowOp [(None, 16, 16, 128) 0           max_pooling2d_6[0][0]            \n",
            "                                                                 activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 16, 16, 256)  294912      tf_op_layer_add_4[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 16, 16, 256)  1024        conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 16, 16, 256)  0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2D)  (None, 8, 8, 256)    0           activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_3 (Lambda)               (None, 8, 8, 256)    0           max_pooling2d_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 8, 8, 512)    1179648     lambda_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 8, 8, 512)    2048        conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 8, 8, 512)    0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2D)  (None, 4, 4, 512)    0           activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 4, 4, 512)    2359296     max_pooling2d_8[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 4, 4, 512)    2048        conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 4, 4, 512)    0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 4, 4, 512)    2359296     activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 4, 4, 512)    2048        conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 4, 4, 512)    0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_5 (TensorFlowOp [(None, 4, 4, 512)]  0           max_pooling2d_8[0][0]            \n",
            "                                                                 activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling2d_2 (GlobalM (None, 512)          0           tf_op_layer_add_5[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 10)           5120        global_max_pooling2d_2[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "lambda_4 (Lambda)               (None, 10)           0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 10)           0           lambda_4[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 6,577,600\n",
            "Trainable params: 6,573,120\n",
            "Non-trainable params: 4,480\n",
            "__________________________________________________________________________________________________\n",
            "Model will be trained with distortion after Res Blk 2\n",
            "=======================\n",
            "Train for 98.0 steps\n",
            "epoch  1 : setting learning rate to  0.025\n",
            "Epoch 1/60\n",
            "is_training True\n",
            "is_training True\n",
            "98/98 [==============================] - 153s 2s/step - loss: 1.6167 - accuracy: 0.4479\n",
            "epoch  2 : setting learning rate to  0.04375\n",
            "Epoch 2/60\n",
            "98/98 [==============================] - 151s 2s/step - loss: 1.0309 - accuracy: 0.6672\n",
            "epoch  3 : setting learning rate to  0.0625\n",
            "Epoch 3/60\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.7701 - accuracy: 0.7667\n",
            "epoch  4 : setting learning rate to  0.08124999999999999\n",
            "Epoch 4/60\n",
            "98/98 [==============================] - 151s 2s/step - loss: 0.6400 - accuracy: 0.8169\n",
            "epoch  5 : setting learning rate to  0.1\n",
            "Epoch 5/60\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.5664 - accuracy: 0.8420\n",
            "epoch  6 : setting learning rate to  0.11875\n",
            "Epoch 6/60\n",
            "98/98 [==============================] - 151s 2s/step - loss: 0.5091 - accuracy: 0.8658\n",
            "epoch  7 : setting learning rate to  0.13749999999999998\n",
            "Epoch 7/60\n",
            "98/98 [==============================] - 151s 2s/step - loss: 0.4697 - accuracy: 0.8831\n",
            "epoch  8 : setting learning rate to  0.15625\n",
            "Epoch 8/60\n",
            "98/98 [==============================] - 151s 2s/step - loss: 0.4464 - accuracy: 0.8931\n",
            "epoch  9 : setting learning rate to  0.175\n",
            "Epoch 9/60\n",
            "98/98 [==============================] - 149s 2s/step - loss: 0.4193 - accuracy: 0.9073\n",
            "epoch  10 : setting learning rate to  0.19374999999999998\n",
            "Epoch 10/60\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.4043 - accuracy: 0.9140\n",
            "epoch  11 : setting learning rate to  0.2125\n",
            "Epoch 11/60\n",
            "98/98 [==============================] - 149s 2s/step - loss: 0.3968 - accuracy: 0.9209\n",
            "epoch  12 : setting learning rate to  0.23124999999999998\n",
            "Epoch 12/60\n",
            "98/98 [==============================] - 149s 2s/step - loss: 0.3878 - accuracy: 0.9278\n",
            "epoch  13 : setting learning rate to  0.24999999999999997\n",
            "Epoch 13/60\n",
            "98/98 [==============================] - 149s 2s/step - loss: 0.3890 - accuracy: 0.9323\n",
            "epoch  14 : setting learning rate to  0.26875\n",
            "Epoch 14/60\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.3880 - accuracy: 0.9377\n",
            "epoch  15 : setting learning rate to  0.28750000000000003\n",
            "Epoch 15/60\n",
            "98/98 [==============================] - 149s 2s/step - loss: 0.3834 - accuracy: 0.9422\n",
            "epoch  16 : setting learning rate to  0.30625\n",
            "Epoch 16/60\n",
            "98/98 [==============================] - 149s 2s/step - loss: 0.3943 - accuracy: 0.9434\n",
            "epoch  17 : setting learning rate to  0.325\n",
            "Epoch 17/60\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.3972 - accuracy: 0.9468\n",
            "epoch  18 : setting learning rate to  0.34375\n",
            "Epoch 18/60\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.4047 - accuracy: 0.9480\n",
            "epoch  19 : setting learning rate to  0.3625\n",
            "Epoch 19/60\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.4197 - accuracy: 0.9463\n",
            "epoch  20 : setting learning rate to  0.38125000000000003\n",
            "Epoch 20/60\n",
            "98/98 [==============================] - 149s 2s/step - loss: 0.4244 - accuracy: 0.9483\n",
            "epoch  21 : setting learning rate to  0.4\n",
            "Epoch 21/60\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.4417 - accuracy: 0.9471\n",
            "epoch  22 : setting learning rate to  0.39\n",
            "Epoch 22/60\n",
            "98/98 [==============================] - 149s 2s/step - loss: 0.4322 - accuracy: 0.9531\n",
            "epoch  23 : setting learning rate to  0.38\n",
            "Epoch 23/60\n",
            "98/98 [==============================] - 149s 2s/step - loss: 0.4133 - accuracy: 0.9595\n",
            "epoch  24 : setting learning rate to  0.37\n",
            "Epoch 24/60\n",
            "98/98 [==============================] - 147s 1s/step - loss: 0.4061 - accuracy: 0.9624\n",
            "epoch  25 : setting learning rate to  0.36000000000000004\n",
            "Epoch 25/60\n",
            "98/98 [==============================] - 147s 1s/step - loss: 0.3933 - accuracy: 0.9668\n",
            "epoch  26 : setting learning rate to  0.35000000000000003\n",
            "Epoch 26/60\n",
            "98/98 [==============================] - 148s 2s/step - loss: 0.3841 - accuracy: 0.9689\n",
            "epoch  27 : setting learning rate to  0.34\n",
            "Epoch 27/60\n",
            "98/98 [==============================] - 148s 2s/step - loss: 0.3723 - accuracy: 0.9707\n",
            "epoch  28 : setting learning rate to  0.33\n",
            "Epoch 28/60\n",
            "98/98 [==============================] - 149s 2s/step - loss: 0.3560 - accuracy: 0.9742\n",
            "epoch  29 : setting learning rate to  0.32\n",
            "Epoch 29/60\n",
            "98/98 [==============================] - 149s 2s/step - loss: 0.3516 - accuracy: 0.9733\n",
            "epoch  30 : setting learning rate to  0.31000000000000005\n",
            "Epoch 30/60\n",
            "98/98 [==============================] - 149s 2s/step - loss: 0.3450 - accuracy: 0.9754\n",
            "epoch  31 : setting learning rate to  0.30000000000000004\n",
            "Epoch 31/60\n",
            "98/98 [==============================] - 147s 2s/step - loss: 0.3300 - accuracy: 0.9779\n",
            "epoch  32 : setting learning rate to  0.29000000000000004\n",
            "Epoch 32/60\n",
            "98/98 [==============================] - 147s 2s/step - loss: 0.3297 - accuracy: 0.9761\n",
            "epoch  33 : setting learning rate to  0.28\n",
            "Epoch 33/60\n",
            "98/98 [==============================] - 147s 2s/step - loss: 0.3178 - accuracy: 0.9796\n",
            "epoch  34 : setting learning rate to  0.27\n",
            "Epoch 34/60\n",
            "98/98 [==============================] - 148s 2s/step - loss: 0.3079 - accuracy: 0.9807\n",
            "epoch  35 : setting learning rate to  0.26\n",
            "Epoch 35/60\n",
            "98/98 [==============================] - 148s 2s/step - loss: 0.2960 - accuracy: 0.9821\n",
            "epoch  36 : setting learning rate to  0.25\n",
            "Epoch 36/60\n",
            "98/98 [==============================] - 149s 2s/step - loss: 0.2862 - accuracy: 0.9838\n",
            "epoch  37 : setting learning rate to  0.24000000000000002\n",
            "Epoch 37/60\n",
            "98/98 [==============================] - 149s 2s/step - loss: 0.2759 - accuracy: 0.9845\n",
            "epoch  38 : setting learning rate to  0.23\n",
            "Epoch 38/60\n",
            "98/98 [==============================] - 148s 2s/step - loss: 0.2647 - accuracy: 0.9866\n",
            "epoch  39 : setting learning rate to  0.22000000000000003\n",
            "Epoch 39/60\n",
            "98/98 [==============================] - 148s 2s/step - loss: 0.2532 - accuracy: 0.9878\n",
            "epoch  40 : setting learning rate to  0.21000000000000002\n",
            "Epoch 40/60\n",
            "98/98 [==============================] - 149s 2s/step - loss: 0.2472 - accuracy: 0.9875\n",
            "epoch  41 : setting learning rate to  0.2\n",
            "Epoch 41/60\n",
            "98/98 [==============================] - 148s 2s/step - loss: 0.2371 - accuracy: 0.9891\n",
            "epoch  42 : setting learning rate to  0.19000000000000003\n",
            "Epoch 42/60\n",
            "98/98 [==============================] - 147s 2s/step - loss: 0.2246 - accuracy: 0.9906\n",
            "epoch  43 : setting learning rate to  0.18000000000000002\n",
            "Epoch 43/60\n",
            "98/98 [==============================] - 148s 2s/step - loss: 0.2116 - accuracy: 0.9929\n",
            "epoch  44 : setting learning rate to  0.17\n",
            "Epoch 44/60\n",
            "98/98 [==============================] - 148s 2s/step - loss: 0.2061 - accuracy: 0.9918\n",
            "epoch  45 : setting learning rate to  0.16000000000000003\n",
            "Epoch 45/60\n",
            "98/98 [==============================] - 148s 2s/step - loss: 0.1904 - accuracy: 0.9952\n",
            "epoch  46 : setting learning rate to  0.15000000000000002\n",
            "Epoch 46/60\n",
            "98/98 [==============================] - 148s 2s/step - loss: 0.1801 - accuracy: 0.9959\n",
            "epoch  47 : setting learning rate to  0.14\n",
            "Epoch 47/60\n",
            "98/98 [==============================] - 147s 1s/step - loss: 0.1706 - accuracy: 0.9969\n",
            "epoch  48 : setting learning rate to  0.13\n",
            "Epoch 48/60\n",
            "98/98 [==============================] - 148s 2s/step - loss: 0.1599 - accuracy: 0.9975\n",
            "epoch  49 : setting learning rate to  0.12\n",
            "Epoch 49/60\n",
            "98/98 [==============================] - 148s 2s/step - loss: 0.1532 - accuracy: 0.9977\n",
            "epoch  50 : setting learning rate to  0.11000000000000004\n",
            "Epoch 50/60\n",
            "98/98 [==============================] - 148s 2s/step - loss: 0.1452 - accuracy: 0.9983\n",
            "epoch  51 : setting learning rate to  0.10000000000000003\n",
            "Epoch 51/60\n",
            "98/98 [==============================] - 148s 2s/step - loss: 0.1371 - accuracy: 0.9990\n",
            "epoch  52 : setting learning rate to  0.09000000000000002\n",
            "Epoch 52/60\n",
            "98/98 [==============================] - 147s 2s/step - loss: 0.1303 - accuracy: 0.9994\n",
            "epoch  53 : setting learning rate to  0.08000000000000002\n",
            "Epoch 53/60\n",
            "98/98 [==============================] - 148s 2s/step - loss: 0.1254 - accuracy: 0.9993\n",
            "epoch  54 : setting learning rate to  0.07\n",
            "Epoch 54/60\n",
            "98/98 [==============================] - 149s 2s/step - loss: 0.1203 - accuracy: 0.9996\n",
            "epoch  55 : setting learning rate to  0.06\n",
            "Epoch 55/60\n",
            "98/98 [==============================] - 148s 2s/step - loss: 0.1165 - accuracy: 0.9996\n",
            "epoch  56 : setting learning rate to  0.04999999999999999\n",
            "Epoch 56/60\n",
            "98/98 [==============================] - 148s 2s/step - loss: 0.1135 - accuracy: 0.9996\n",
            "epoch  57 : setting learning rate to  0.040000000000000036\n",
            "Epoch 57/60\n",
            "98/98 [==============================] - 147s 1s/step - loss: 0.1107 - accuracy: 0.9998\n",
            "epoch  58 : setting learning rate to  0.030000000000000027\n",
            "Epoch 58/60\n",
            "98/98 [==============================] - 147s 2s/step - loss: 0.1087 - accuracy: 0.9998\n",
            "epoch  59 : setting learning rate to  0.020000000000000018\n",
            "Epoch 59/60\n",
            "98/98 [==============================] - 149s 2s/step - loss: 0.1072 - accuracy: 0.9998\n",
            "epoch  60 : setting learning rate to  0.010000000000000009\n",
            "Epoch 60/60\n",
            "98/98 [==============================] - 148s 2s/step - loss: 0.1063 - accuracy: 0.9998\n",
            "is_training False\n",
            "inside validation cycle\n",
            "===============\n",
            "\n",
            "20/20 [==============================] - 3s 126ms/step - loss: 0.4492 - accuracy: 0.9118\n",
            "val accuracy score at the end of training model type  2 [0.4492311418056488, 0.9118164]\n",
            "=========================================\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqkyspQ4R1C6",
        "colab_type": "text"
      },
      "source": [
        "### Summary of Test Results \n",
        "Hyperparameters : Epochs:60, max_lr:0.4, momentum:0.9, L2-wt_decay on Conv Layers :1.25e-4 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzOpzOMqq-gQ",
        "colab_type": "text"
      },
      "source": [
        "| Trial | Augmentation strategy | Train accuracy |Test Accuracy | Hyperparameters |Comments |\n",
        "| :--- | :---: | :---: | :---: | :---: | :--- |\n",
        "| 1 | cutout(flip_lr(pad4_random_crop(inp_image),8)  |99.62  | 94.44 | as above | train-test acc gap 5.18 |\n",
        "| 2 | flip_lr(random_crop(cutout(pad2(res_blk1),4)))  |99.98  | 92.36 | \" | Lower Val accuracy , Tendency to overfit . Perhaps try a more stringent Aug policy ? |\n",
        "| 3 | flip_lr(random_crop(cutout(pad2(res_blk1),4)))  | 99.98 | 91.18 | \" | Lower Val Accuracy, Tendency to overfit.  |\n",
        "| 4 | augmentation of trial 2 + augmentation of trial 3  | 99.92  | 92.40 | \" | Lower Val Accuracy, Tendency to overfit.  |\n",
        "| 5 | augmentations of trial 1 + augmentation of trial 2  | 99.04  | 94.41 |\"| train-test acc gap 4.63 |\n",
        "| 6 | augmentations of trial 1 + augmentation of trial 3  | 98.96 | **94.72** |\"| Train-Test acc gap : 4.24 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7EJpOdo09FV",
        "colab_type": "text"
      },
      "source": [
        "**Validation accuracy went down when only Distortion was used in the mid/lower layers (after Res Blk1 and Res Blk2 ) But when used in combination with usual Image augmentation , there seems to be better Regularization and perhaps we could explore such an option. If we were to pursue only Distortion of middle or lower layers (with a good enough size of channels ) , we may need to try more stringent augmentation strategies to overcome the problem of overfitting**"
      ]
    }
  ]
}