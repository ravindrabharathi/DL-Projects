{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Oct_3_EVA_research_grp2_test_distortion_in_mid_lower_layers_DavidNet_50-epochs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ravindrabharathi/Project1/blob/master/Rgroup2/Oct_3_EVA_research_grp2_test_distortion_in_mid_lower_layers_DavidNet_50_epochs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9J-YBCINFzxw",
        "colab_type": "text"
      },
      "source": [
        "# Effect of distortions/transformations in mid/lower layers of a Convolutional Neural Network\n",
        "\n",
        "This exercise is to find out of there is any beneficial effect due to  performing distortions/transformation on the activation outputs of mid/lower layers (similar to augmenations on input images ) of a convolutional Neural Network . We will look at gain in test accuracy , regularization effects as seen in smaller gaps of train vs test accuracy , etc. Here we are using a reference model loosely based on the one defined by David Page in his DawnBench submission and explained very well in his series of blog posts (insert link here) . The Model is essentially a 3 block , nine layer modified Resnet Model .\n",
        "\n",
        "We will compare the accuracies achieved using the following methods \n",
        "\n",
        "1. Base Model with image augmnetation on input images alone \n",
        "2. Base Model without input image augmenation but with distortions after first resnet block \n",
        "3. Base Model without input image augmenation but with distortions after second resnet block \n",
        "4. Base Model without input image augmenation but with distortions after first and second resnet blocks \n",
        "5. Base Model with input image augmenation and with distortions after first resnet block \n",
        "6. Base Model without input image augmenation and with distortions after second resnet block \n",
        "\n",
        "Augmentations for input images as well as activation channels will be a combination of pad/random_crop , flip_left_right and cutout "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgLSEQaclSMi",
        "colab_type": "text"
      },
      "source": [
        "### install/import tensorflow_addons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3R6SSn4lb2D",
        "colab_type": "code",
        "outputId": "b966949c-fa82-4a48-e3a4-4a2408fdebc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "!pip install tensorflow==2.0\n",
        "!pip install tensorflow-gpu==2.0\n",
        "!pip install tensorflow_addons\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import math\n",
        "import random"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/0f/7bd55361168bb32796b360ad15a25de6966c9c1beb58a8e30c01c8279862/tensorflow-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (86.3MB)\n",
            "\u001b[K     |████████████████████████████████| 86.3MB 56.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (0.33.6)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.15.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (3.0.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (0.1.7)\n",
            "Collecting tensorboard<2.1.0,>=2.0.0 (from tensorflow==2.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/a6/e8ffa4e2ddb216449d34cfcb825ebb38206bee5c4553d69e7bc8bc2c5d64/tensorboard-2.0.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 32.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (0.8.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.0.8)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.11.2)\n",
            "Collecting tensorflow-estimator<2.1.0,>=2.0.0 (from tensorflow==2.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/00/5e6cdf86190a70d7382d320b2b04e4ff0f8191a37d90a422a2f8ff0705bb/tensorflow_estimator-2.0.0-py2.py3-none-any.whl (449kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 34.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (3.7.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (0.8.0)\n",
            "Collecting gast==0.2.2 (from tensorflow==2.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.12.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.16.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (0.16.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (41.2.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0) (2.8.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=7e6b9c8b6c4b7018c6caa7bcf60878927aaff8cd34ae5099a4155f72d215a90f\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "Installing collected packages: tensorboard, tensorflow-estimator, gast, tensorflow\n",
            "  Found existing installation: tensorboard 1.14.0\n",
            "    Uninstalling tensorboard-1.14.0:\n",
            "      Successfully uninstalled tensorboard-1.14.0\n",
            "  Found existing installation: tensorflow-estimator 1.14.0\n",
            "    Uninstalling tensorflow-estimator-1.14.0:\n",
            "      Successfully uninstalled tensorflow-estimator-1.14.0\n",
            "  Found existing installation: gast 0.3.2\n",
            "    Uninstalling gast-0.3.2:\n",
            "      Successfully uninstalled gast-0.3.2\n",
            "  Found existing installation: tensorflow 1.14.0\n",
            "    Uninstalling tensorflow-1.14.0:\n",
            "      Successfully uninstalled tensorflow-1.14.0\n",
            "Successfully installed gast-0.2.2 tensorboard-2.0.0 tensorflow-2.0.0 tensorflow-estimator-2.0.0\n",
            "Collecting tensorflow-gpu==2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/44/47f0722aea081697143fbcf5d2aa60d1aee4aaacb5869aee2b568974777b/tensorflow_gpu-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (380.8MB)\n",
            "\u001b[K     |████████████████████████████████| 380.8MB 74kB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (0.8.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (3.7.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (1.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (1.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (1.16.5)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (1.0.8)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (1.11.2)\n",
            "Requirement already satisfied: tensorboard<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (2.0.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (2.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (3.0.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (0.33.6)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (0.1.7)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (0.2.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (0.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0) (41.2.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==2.0) (2.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (0.16.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (3.1.1)\n",
            "Installing collected packages: tensorflow-gpu\n",
            "Successfully installed tensorflow-gpu-2.0.0\n",
            "Collecting tensorflow_addons\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/b6/574034405ad527eec40ac426081694f50da9766db6b81c2b899041c44ef2/tensorflow_addons-0.5.2-cp36-cp36m-manylinux2010_x86_64.whl (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_addons) (1.12.0)\n",
            "Requirement already satisfied: tensorflow-gpu==2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_addons) (2.0.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (0.8.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (2.0.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (0.33.6)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (0.1.7)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (0.8.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (3.0.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (1.16.5)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (1.0.8)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (3.7.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (1.11.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (1.15.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (0.2.2)\n",
            "Requirement already satisfied: tensorboard<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (2.0.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==2.0.0->tensorflow_addons) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0->tensorflow_addons) (41.2.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0->tensorflow_addons) (0.16.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0->tensorflow_addons) (3.1.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.5.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFPFRpPDQ2e6",
        "colab_type": "text"
      },
      "source": [
        "### Install tf_utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0yHfmeMv-rI",
        "colab_type": "code",
        "outputId": "87fcd089-7337-4d72-c08b-f6cace26c86e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "source": [
        "!pip install --upgrade git+https://github.com/ravindrabharathi/tf_utils "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/ravindrabharathi/tf_utils\n",
            "  Cloning https://github.com/ravindrabharathi/tf_utils to /tmp/pip-req-build-uyfb8itl\n",
            "  Running command git clone -q https://github.com/ravindrabharathi/tf_utils /tmp/pip-req-build-uyfb8itl\n",
            "Building wheels for collected packages: tf-utils\n",
            "  Building wheel for tf-utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tf-utils: filename=tf_utils-0.2-cp36-none-any.whl size=8945 sha256=ea87a408c9c13fe88d7f0b6c35213bfbe6fe38bdd3b5f3099c6ea55b9e756c52\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-dpo2sf9j/wheels/95/af/bb/690b94c65a5aad47a5c39e75f158a2b043448e908c5c121791\n",
            "Successfully built tf-utils\n",
            "Installing collected packages: tf-utils\n",
            "Successfully installed tf-utils-0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VH-5ffTMQ8ms",
        "colab_type": "text"
      },
      "source": [
        "### import the data module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBcelJMpwYoX",
        "colab_type": "code",
        "outputId": "4915b2bb-75fc-4b99-a55d-16c8ce6af991",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tf_utils.data as ds"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished 'get_cpu_num' in 0.0000 secs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vb0kbJalRCPJ",
        "colab_type": "text"
      },
      "source": [
        "### set batch size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJTA3CO8xOtg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size=512\n",
        "ds.batch_size=batch_size\n",
        "EPOCHS=50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTroxaquRE2Z",
        "colab_type": "text"
      },
      "source": [
        "### downlaod data and create tf records"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zF3qoDnbykb6",
        "colab_type": "code",
        "outputId": "89f4a749-8e43-4992-b1ee-f1421b877380",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "source": [
        "ds.get_cifar10_and_create_tfrecords()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d3b996b304dd479491fa7cf8a3cc113e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='cifar-10-python.tar.gz', max=170498071, style=ProgressStyle(d…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Finished 'download_file' in 2.6523 secs\n",
            "Finished 'download_cifar10_files' in 2.6526 secs\n",
            "Done\n",
            "Finished 'extract_cifar10_files' in 1.9446 secs\n",
            "Finished '_get_file_names' in 0.0000 secs\n",
            "Generating ./train.tfrecords\n",
            "Finished 'read_pickle_from_file' in 0.1616 secs\n",
            "Finished 'read_pickle_from_file' in 0.1509 secs\n",
            "Finished 'read_pickle_from_file' in 0.1448 secs\n",
            "Finished 'read_pickle_from_file' in 0.1454 secs\n",
            "Finished 'read_pickle_from_file' in 0.1538 secs\n",
            "Finished 'convert_to_tfrecord' in 3.5251 secs\n",
            "Done!\n",
            "Generating ./eval.tfrecords\n",
            "Finished 'read_pickle_from_file' in 0.1533 secs\n",
            "Finished 'convert_to_tfrecord' in 0.6795 secs\n",
            "Done!\n",
            "Finished 'create_tf_records' in 4.2061 secs\n",
            "Finished 'get_cifar10_and_create_tfrecords' in 8.8036 secs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7U-_CmC6RKDF",
        "colab_type": "text"
      },
      "source": [
        "### create train and test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lYOp7UKyoG5",
        "colab_type": "code",
        "outputId": "9cfa464c-6ced-42af-b354-4dd9bbf81e49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "def aug_fn(image):\n",
        "  return ds.cutout(ds.flip_left_right(ds.random_pad_crop(image)))\n",
        "\n",
        "train_ds1=ds.get_train_ds(batch_size=batch_size,shuffle=True,distort=True,distort_fn=aug_fn)\n",
        "train_ds2=ds.get_train_ds(batch_size=batch_size,shuffle=True,distort=False)\n",
        "\n",
        "test_ds=ds.get_eval_ds(batch_size=batch_size)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "distorting...\n",
            "Finished 'get_tf_dataset_2' in 2.9943 secs\n",
            "Finished 'get_tf_dataset_in_batches' in 2.9947 secs\n",
            "Finished 'get_train_ds' in 2.9948 secs\n",
            "Finished 'get_tf_dataset' in 0.1808 secs\n",
            "Finished 'get_tf_dataset_in_batches' in 0.1811 secs\n",
            "Finished 'get_train_ds' in 0.1813 secs\n",
            "Finished 'get_tf_dataset' in 0.0349 secs\n",
            "Finished 'get_tf_dataset_in_batches' in 0.0351 secs\n",
            "Finished 'get_eval_ds' in 0.0353 secs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7z9t4e5RRbH",
        "colab_type": "text"
      },
      "source": [
        "### import visualization module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHq2g5VjzmDz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tf_utils.visualize as vz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utItaz0RRXft",
        "colab_type": "text"
      },
      "source": [
        "### plot images from train dataset1 , train dataset by default uses image augmenttation of cutout,flip-left-right,random-pad-crop "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvonf0IP0uOX",
        "colab_type": "code",
        "outputId": "428b82e9-1a32-48e6-95c4-e3b13e41fa59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "vz.plot_cifar10_files(train_ds1)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nasT69YIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/EJEioJfiEj533gbDgFQTk5+AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAHTBJREFUeJztnXuMnGeV5p9Tl763L223707sOCYh\nMBkTmixMYCYEBYUsIslolIXVoIzEjEerQVqkWa0iRhpYaf9gVguIP1aszBJNhuUShovIsGiHJAOb\nQTMb0glJHGKHJI6D7fjutt1td3fdzvxR5V2neZ/T5b5U23mfn2S5+j31ft+pt7/TX9X71DnH3B1C\niPwoLLUDQoilQcEvRKYo+IXIFAW/EJmi4BciUxT8QmSKgl+ITFHwC5EpCn4hMqU0n8lmdgeALwEo\nAvgf7v656PldPQPeMzBEDhZMJF9CnMOUeU7sIBY5GUC+sRm/LG61cCI3OnV/bq/L5jiPrccsJ1tw\n3BuBkfnBHWGmqYkxVKbOtfUK5hz8ZlYE8N8A3A7gIIAnzexhd3+BzekZGMK7PvIfk7ZCIfC3nl64\nEr/CUIuu2iJ/w9NoBPOYbY4XiwWvuVAszumY9Xo9Od4IArXeqFFbNznehaMyKsX0a7PgzWYUH6VC\neS5uAM5eG1+PQoH7aEFANhp8rWq16WBe+gUUg2ugUEyH7pP/60t0zm8co+1n/iY3A3jZ3fe5ewXA\ntwDcNY/jCSE6yHyCfyOAAxf9fLA1JoS4Alj0DT8z22lmo2Y2WpmaWOzTCSHaZD7BfwjA5ot+3tQa\newPuvsvdR9x9pKtnYB6nE0IsJPMJ/icBbDezrWbWBeCjAB5eGLeEEIvNnHf73b1mZp8E8PdoSn0P\nuPsvwzlwNMjO8vR0lc4rk53NOlEBAMCDHf1qhe9ul8t8V5ntvtZr0Y54QCAsRNJQtONcq5HXFqgf\nxVDP4+eqB/eOuqdtRb70sGAdC8YnFoOFrCB9zHrwmtnuOwDExW+i31mw/uS6ivyoEvUgVKtmMC+d\n391/BOBH8zmGEGJp0Df8hMgUBb8QmaLgFyJTFPxCZIqCX4hMmddu/yXjjlo9LekVSCIIADiRUCL5\nhM2Z7VzMPwCo1dK2SJKJkkRKQeJGrcqlrUgGLBSYbMSPN12ZpLaqBwk1JW6rFUh2YZ2/5ihRy4Pf\nS3QdFMrkfEFSVZRQGUl9kQoYrX+lUrnkczVY9uYlZDHqzi9Epij4hcgUBb8QmaLgFyJTFPxCZEpH\nd/vdg8STYPe1THaw2S4pAJS6u6itGpRboqW6ABTINvBcd4CrlcCPYGIxUBCq1XTCh5EEFwAoWZTI\nQk2oBopEvZg+ZtX5JVcOttnr4Lv99eD3WSHVs+rhrnhQ09Ci+yVfx7kkC4VKUSm9jlHS128cv+1n\nCiHeVCj4hcgUBb8QmaLgFyJTFPxCZIqCX4hM6WxiD5zKGvVACqlV0zLP6N/+5YJ4dTnyrrs/Q21h\nRxlqiRJjuM3BE3GKgaxU9LQMO93VTedE5ee8xM8VKaaNWvq68jnW6bNQFp1bXcC5dOxhZ1JijxBi\nVhT8QmSKgl+ITFHwC5EpCn4hMkXBL0SmzEvqM7P9AMYB1AHU3H0ker4DaHg6E8wD+arhc2yHdQVT\nCrIcS0ErsgKRhyyQgOrTvIZfvRFkTgYttLx+hlj66JwqafEFANNRmy/jl7FVicQW6YphYlxU+y8q\n/td+tt0FInmwVidxFNR3nMlC6Pzvd/cTC3AcIUQH0dt+ITJlvsHvAH5sZk+Z2c6FcEgI0Rnm+7b/\nve5+yMzWAHjEzPa6++MXP6H1R2EnAHT1LZ/n6YQQC8W87vzufqj1/zEA3wdwc+I5u9x9xN1HSt39\n8zmdEGIBmXPwm1m/mQ1eeAzggwCeXyjHhBCLy3ze9q8F8P2WvFEC8A13/9/RhHq9hjNnTydtjaCF\nViksmvjmpDo9RW2VUM5Jr2MhkOXWDS+jtqlJLol11bkM2FVM245MnKNzpoPinsXyILVFMrERm/GE\nuTAzrkiKyQJxcc9AtQvmBJPaT96jzDn43X0fgN+evwtCiKUgv1uqEAKAgl+IbFHwC5EpCn4hMkXB\nL0SmdLSAZ6FQQF9fb9JWD6Q+WwBZ40qjXI5+NUGGXj1tG161ks7ZevU6atu75xVqq06MUdtNb12T\nnrP3VX68Iu+vOBZIhPUSv4d5Vzk5Xgvk0rkU2wSAOKEuyMQkEmEj8pH0BfQoW7Ftj4QQb2oU/EJk\nioJfiExR8AuRKQp+ITKlo7v97kBlOl2Pr16MdrB5O6k3K40ar1tYr/Kkn/7enuT49ddsoHNee2U3\nta0aSO+WA8DJkxPUVkJaQXjb+rQKAADTllaCAODpX/2a2iaCGoReS1/i9WBX3I2/5lKJ+2hFPi8q\n4ce6cjXC/mXEh0soFag7vxCZouAXIlMU/EJkioJfiExR8AuRKQp+ITKlo1Jfo97A1ASRZbr436FS\nWr16UzM9zpNmuotcBrzxxi3J8eLUUTrn/e/ays/VxZNtXh7i0tbg8rTU9/53vpPO2f0iTyLafv21\n1HZs6hS1jY2dTI+f4XLp0RNcOjxynP9eqgUeTt19/CJm+TulIDrL5FyFS8iC051fiExR8AuRKQp+\nITJFwS9Epij4hcgUBb8QmTKr1GdmDwD4MIBj7v721tgQgIcAbAGwH8C97s41kBYFc/SW0jJVLWiR\n1KjOodfRFc67R66hNqtxKWpoWbot15oh3iF5eAWXoU6cOE5thw+9Rm0DpL3WBMnqBIDeQe7j7Xf/\na2orLQuy6RrpjNCpaX5NjZ3lWaSP/+wpanv0H/+J2k6c5nJksZgOwyJL9wOwYU1aSt1b5msxk3bu\n/H8N4I4ZY/cDeMzdtwN4rPWzEOIKYtbgd/fHAcz8s3UXgAdbjx8EcPcC+yWEWGTm+pl/rbsfbj0+\ngmbHXiHEFcS8N/y82c+YfmA3s51mNmpmo/Uq/6wqhOgscw3+o2a2HgBa/x9jT3T3Xe4+4u4jxTIv\ngSSE6CxzDf6HAdzXenwfgB8sjDtCiE7RjtT3TQC3AlhtZgcBfAbA5wB828w+AeA1APe2dbZGDT6Z\nzrLq6llGp1Xyq9+Je+96H7UdPbSf2p4eTctNY/V+Oqd2Pi3LAcDw6iFqq0yNU9u+fS+mDfUKnfMP\n/+cfqe2lY69T2+0f/j1q20gKhq4Z5i3KCgWe8bf9mmFqu+ba36e2n/3zz6nt8OETyfGtW3gm44Z1\nVyXH//mR/0nnzGTW4Hf3jxHTB9o+ixDiskPf8BMiUxT8QmSKgl+ITFHwC5EpCn4hMqWjBTxXD63A\nH//bjyRtD33/ETrv/Pnp5Pj2W/6Izmk0uKS0uq+b2v7NPbdT2223/k5yfOOG1XTO6dNpaRMADr3G\nC1Yu7+FZjr3rV1Lb66sGkuMHDxykc275yD3UdvXVm6ntHx79KbVdtWljcnzvft5z7+Q5/g3Qx/+J\nZ8xt3sbXf/3qdKbg+aBA6osv/JLaeso8ZNZtSstvAHDVxg9R2/5XjyTHx8+mr3sAOHEinSXYqPOs\nyZnozi9Epij4hcgUBb8QmaLgFyJTFPxCZIqCX4hM6ajUt2rlcvzhH9yZtFWDgorfePjH6TnO/3aV\ng0Znv38nz0li/gFAwdLFMc+eOpwcB4CHvvE1alvezzPtbv+926it2N1Hbe97Tzob8CfTP6Vzdu/e\nTW1TUxPUtnYdl9hWrk5nafae5nLe+qu4rLhumGcednfxopWnx9KS2Kv7XqZzTp2i5Slw441vp7ah\nZbyvYaHMf9dHD6YzFmtBdHZZWtIzXlfnN31q+5lCiDcVCn4hMkXBL0SmKPiFyBQFvxCZ0tHd/ka9\nhvOn0zup73vPO+i85185kBx/4hmegPHOHW+jtuhczD8AGBxIt7V67RVSrw5ArXKe2jZvv47axk7y\n+ngHDvLkmG1v2ZYcf/9tPLHk6WefpLbxyXPUdusHbqW2yUlLjr/lpnRyFAAUCzzhqp93FMPgIN/h\n7i6lW1719PCd+bfd8BZq6+3hykKhmH7NAFCp8LqAQyvSyViDvfzeXCykbV1B4tFMdOcXIlMU/EJk\nioJfiExR8AuRKQp+ITJFwS9EprTTrusBAB8GcMzd394a+yyAPwFwvPW0T7v7j2Y/FtBDzrhx3Qo6\n7723/Kvk+OgLv7rkObOdq7vBpS2vpfuGXbNlC52zadMfUltXkTcurQctyvqHeA2/FUPp9lqFIv87\nf9X2a6it4TwRp1TicplZOvnISjzBxYLL0YOajOUytzH5rVZNt/ECgBJX7NCo8Lp61aB+Xm831yqv\nuzYtz05N8eNt3rwpOd7X334z3Hbu/H8N4I7E+BfdfUfr36yBL4S4vJg1+N39cQDpvEghxBXLfD7z\nf9LMnjOzB8yMvw8VQlyWzDX4vwxgG4AdAA4D+Dx7opntNLNRMxs9dUpvIIS4XJhT8Lv7UXevu3sD\nwFcA3Bw8d5e7j7j7yBDZjBJCdJ45Bb+Zrb/ox3sAPL8w7gghOkU7Ut83AdwKYLWZHQTwGQC3mtkO\nAA5gP4A/bedktVoVJ46lWxON7nmJzvu7R9M15s6e4/Xl/u6HD1Nb99RvUdvIW7dT2/CadcnxVUPD\ndE6hi8thHpRbKxTT2WgA4IGtbkSnsuDvfCNdm7A5jUtbU9PcVvD0pVVo8Ow2d54x1wCv8VipcqnP\nG+nXbc71vEad26zGfSyWuZzHvQcq1fQ6loJr58zEmeS4N9pv1zVr8Lv7xxLDX237DEKIyxJ9w0+I\nTFHwC5EpCn4hMkXBL0SmKPiFyBTzSG9aYAb7en0HyWA6MnGWzqsMpOW3xgDPzCpM8EKcXRNcVlw3\nkG4zBQBXXb01Ob563frkOACs2ZjOvgKAtRt5u6tVa4LMvWEuLQ6SL1INDvLj9ZaiTDAu55WCYpFF\nS0tiAyV+vRU8kEWNn6tR5FJlrZCW7UrGz9Ud2EoNLrOiwF9bHdzHhpPWW8aPNz6RLvB6220fxDPP\nPBvkJf5/dOcXIlMU/EJkioJfiExR8AuRKQp+ITJFwS9EpnS0V58Vu9C1Ki19LV/B1Ylz9bQUNeVc\nPukZXE5t/St4H7+uIpdXzpNsr8kpXuTytZd5P8E9e3g22thZ3uPv9Gnex6+CtP+lXp5xViByGACY\n8TUGyyAEUCbFPXsHuIxWCAqC9vTwwp9dvcExy2nJsVTkfQHLBZ6511Pm87q6uC0qrMmKjAbLiyL5\nnR2/hII5uvMLkSkKfiEyRcEvRKYo+IXIFAW/EJnS0d1+FEuw/nRSytFDJ+k0Jy2jLNgOHQ8SliaC\neSuCZJuugXRShwcJHds2b6S2dZuuprZly9P1AgGgOslVgnOknt3ZClckKg1um5jgdRLHgp3lM+Pp\nRK3xoO0WCvxeNHGeqx+1SZ4U1iA1/KKahtUq75VWLnElINrtj2w9PWklpjuYUyqlr8XpSrC+M9Cd\nX4hMUfALkSkKfiEyRcEvRKYo+IXIFAW/EJnSTruuzQD+BsBaNNtz7XL3L5nZEICHAGxBs2XXve4+\nFh1rulLFqwcOJW1T01wus0ACmgve4M2TmH8AcPXvvDM53lfmyS979x3gtv37qW3rFi4Dblq7gdrW\nbEjXE9w2zGsJ9vYNUNtAP7d1Be2kCsX076xMEm0AoBi0IatWgwSjgCJJxGnMsXZl5COiFmBBvy62\njmwNAcCRPtcvHv0JP9HM47fxnBqAP3f3GwC8G8CfmdkNAO4H8Ji7bwfwWOtnIcQVwqzB7+6H3f3p\n1uNxAHsAbARwF4AHW097EMDdi+WkEGLhuaT302a2BcA7ADwBYK27H26ZjqD5sUAIcYXQdvCb2QCA\n7wL4lLu/4fuU3iz+n/wQZWY7zWzUzEYbtfa/eiiEWFzaCn4zK6MZ+F939++1ho+a2fqWfT2AZJcM\nd9/l7iPuPhJVahFCdJZZg9+a2TNfBbDH3b9wkelhAPe1Ht8H4AcL754QYrFoJ6vvFgAfB7DbzJ5p\njX0awOcAfNvMPgHgNQD3znager2BMxPp7KyacQmlUEjbioEUUq9zbaXRSLdHAoAzVW77+S92J8fv\nvftDdM71N7yV2o4ff5XaJsePU9uRI/upbaqWznBbW+dZccv7eCbjVPdpauvu5nUBe7rTEltvmb/7\nK5LfMwB09/AMtyi7s1BO/z5LgWRXCjL3CqS1FsCvU4BLcwCASjqLMKrhh2I6dO0SJMxZg9/dfwZQ\nzz/Q9pmEEJcV+oafEJmi4BciUxT8QmSKgl+ITFHwC5EpHS3gWSgW0NO/LGmbrk3TeUy9KJW5+w5e\nhLFY4vO6S7wt1LGxdDHL7/zw7+mcuz78QWrbtu0GaqtNcKlv/Di3nRtPS3qH9vPswtpKvlbr1vFC\noj29fdS2sj+9jt1BVl8hyN6MCmdGmYJeSh8zSLIL/QhlxUDqs+A+WyFS3/TUFJ1zbjJtq9faz37U\nnV+ITFHwC5EpCn4hMkXBL0SmKPiFyBQFvxCZ0lGpzx2oEdmO9SsDuLwSSkORDBhkPlWrQVZUMZ1Z\ndvgEz3z72kPfo7ZbSUFQAHjXDdupbe3aa6mt3E36CUbZaDVuO/hrXtD09YOvUxtbxeENXDpcsWIl\ntXV1cTmvm2QQAlzqi5Lsomuxt7eXTwzupfU6v67q9fT6T07yHoqT59JSX40cK4Xu/EJkioJfiExR\n8AuRKQp+ITJFwS9EpnR0tx8A6vV04kG0A18opLdmoySL6HiNRmQLjknmFYwv4/gkL1f+yE//L7W9\nunc/tW1bx1skDK9ZQcb5Tvq6oXSyFQBUgl3qwT6eBMWSp85U+HqcPMzVg6juYm+QYNTTn9657+vj\nc/qqPNFpRYkn7zSCtaoxmQtApZJOaqsFSTrs0r+UJmS68wuRKQp+ITJFwS9Epij4hcgUBb8QmaLg\nFyJTZpX6zGwzgL9BswW3A9jl7l8ys88C+BMAFwrKfdrdfzTb8ZhsB3AphyVFuPNKbGZzrcMWLAmZ\n54HAUizw9lSNILtkzz5ec2/3U89QG0DqvhX4+m7fupnarrvuOmrbuHETtW3YuCFt6OXrUe4KbEHd\nRa8G18F02lYocBmtq4v/Po8fG+O24yeorRC0o1u+PC3P8lgBiuS+HUncM2lH568B+HN3f9rMBgE8\nZWaPtGxfdPf/2vbZhBCXDe306jsM4HDr8biZ7QGwcbEdE0IsLpf0md/MtgB4B4AnWkOfNLPnzOwB\nM+NfIRNCXHa0HfxmNgDguwA+5e5nAXwZwDYAO9B8Z/B5Mm+nmY2a2Wijzr/aKYToLG0Fv5mV0Qz8\nr7v79wDA3Y+6e92bu25fAXBzaq6773L3EXcfKRT5ho4QorPMGvzW3Br/KoA97v6Fi8bXX/S0ewA8\nv/DuCSEWi3Z2+28B8HEAu83sgsb0aQAfM7MdaMp/+wH86axHckejls6YcuPSS6ORlmvYOBDX9wvb\nMQVSCcvq80KQkRhlHnL1BxXj0tzAygFqWz6Y3nrp7ed17ian+do//cKL1PbygcPUNrhsMDneH2Tg\nRSpVOajh1xvU3Fu7Kr0e69YM0zlXbb6K2hqBvHz49aPUdv5cuo0aAGzYmN4/j7L6WFuuqfO87t9M\n2tnt/xnS5Q5n1fSFEJcv+oafEJmi4BciUxT8QmSKgl+ITFHwC5EpnS3gaUCRZCo1giw8qgFFcl6Y\nuRfM86CPEzE1EEiO/GioO5dyBpZzaW5lF28Zde5sOrNsfOwMndNV4IU4owzIyZMnqe31I2nZq7/M\nX1d/VFQzaJPF2l0BwFh/+pgHg+PtHXyB2qJWXtUq9yMqGnvk4JHk+Llz5+ic02fS2YVnz5ylc2ai\nO78QmaLgFyJTFPxCZIqCX4hMUfALkSkKfiEypaNSnwEoEuWoVAwy7Yg8WAsyrEqBdOhRph1Xa3jP\nwEAerAW96Xp7uPxTPc8lm4NHuGxXJgu8Zj3v71cNZKizp7kftSne087Ir6ZS5pfcwHIuvzVYYVIA\na4Z5Eanly5YTC/+dTTW4BMskNgAoBMesBnLk+bG0LHrwdd67kJ1pqtp+wRzd+YXIFAW/EJmi4Bci\nUxT8QmSKgl+ITFHwC5EpHZX63BuoVtOSTaERSH3MEGTn1epcrom6mTXqQd83IokVC5HkGPTxC3zs\n6eFlzodX8N56/f3pDL2jx4/ROcdOcZvXuP+loAJpN8ne27yVF8e8ehPp7wcAgYQ1EKxVsZz2cbrC\npbfJQJ7tCgqQ9gS9Bk+dPkVtB44eSo6fPMvnXHf99cnx4r6X6JyZ6M4vRKYo+IXIFAW/EJmi4Bci\nUxT8QmTKrLv9ZtYD4HEA3a3nf8fdP2NmWwF8C8AqAE8B+Li7z5pV4EjvslarfOeb7fdbkAzkwa59\ntN/PkneapH0vlrgf3WS3GQB6g13q3h4+L2pTduhwOhlkfGKCzvEgIaV/gNf3W7WMJ9QMLU/bVgzx\nOfXgGigHv+tq0NaqQhSaQiFo/xXU96sEdfpePfQqtZ08dZzaxsfTyVM7bvwtOqdEagkWi0EPuBm0\nc+efBnCbu/82mu247zCzdwP4KwBfdPdrAYwB+ETbZxVCLDmzBr83uXDbKLf+OYDbAHynNf4ggLsX\nxUMhxKLQ1md+Myu2OvQeA/AIgFcAnHb/f7WnDwJItxoVQlyWtBX87l539x0ANgG4GUD660UJzGyn\nmY2a2Wgj+EabEKKzXNJuv7ufBvATAO8BsMLMLmwYbgKQ/I6iu+9y9xF3HykUO9sjRAjBmTX4zWzY\nzFa0HvcCuB3AHjT/CPxB62n3AfjBYjkphFh42rkVrwfwoJkV0fxj8W13/6GZvQDgW2b2nwH8AsBX\nZzuQw+FO6r5ZIG0xaS6S8wLFLvqL50ELLVha5imVeAsqK3BpqB74P3aGS3PVIMllcpokTpX4+q7s\nG6S24aHV1LZ2aIjaBvrSEmFf/wCdU6tMU1u1Mklt56bOU9s0WeOpKb6GtSqXgn+9/yC1HTuWbrsF\nAOvWDlPb296a/hR91WaeBLX7V3uT41FC20xmDX53fw7AOxLj+9D8/C+EuALRN/yEyBQFvxCZouAX\nIlMU/EJkioJfiEwx96ii3QKfzOw4gNdaP64GcKJjJ+fIjzciP97IlebH1e7OdcWL6Gjwv+HEZqPu\nPrIkJ5cf8kN+6G2/ELmi4BciU5Yy+Hct4bkvRn68EfnxRt60fizZZ34hxNKit/1CZMqSBL+Z3WFm\nL5rZy2Z2/1L40PJjv5ntNrNnzGy0g+d9wMyOmdnzF40NmdkjZvZS639e6XJx/fismR1qrckzZnZn\nB/zYbGY/MbMXzOyXZvbvW+MdXZPAj46uiZn1mNnPzezZlh//qTW+1cyeaMXNQ2bGK8C2g7t39B+A\nIpplwK4B0AXgWQA3dNqPli/7AaxegvP+LoCbADx/0dh/AXB/6/H9AP5qifz4LID/0OH1WA/gptbj\nQQC/AnBDp9ck8KOja4JmQvpA63EZwBMA3g3g2wA+2hr/7wD+3XzOsxR3/psBvOzu+7xZ6vtbAO5a\nAj+WDHd/HMDMLox3oVkIFehQQVTiR8dx98Pu/nTr8TiaxWI2osNrEvjRUbzJohfNXYrg3wjgwEU/\nL2XxTwfwYzN7ysx2LpEPF1jr7odbj48AWLuEvnzSzJ5rfSxY9I8fF2NmW9CsH/EElnBNZvgBdHhN\nOlE0N/cNv/e6+00APgTgz8zsd5faIaD5lx9xJ/HF5MsAtqHZo+EwgM936sRmNgDguwA+5e5v6GTR\nyTVJ+NHxNfF5FM1tl6UI/kMALm4wT4t/Ljbufqj1/zEA38fSViY6ambrAaD1/7GlcMLdj7YuvAaA\nr6BDa2JmZTQD7uvu/r3WcMfXJOXHUq1J69yXXDS3XZYi+J8EsL21c9kF4KMAHu60E2bWb2aDFx4D\n+CCA5+NZi8rDaBZCBZawIOqFYGtxDzqwJmZmaNaA3OPuX7jI1NE1YX50ek06VjS3UzuYM3Yz70Rz\nJ/UVAH+xRD5cg6bS8CyAX3bSDwDfRPPtYxXNz26fQLPn4WMAXgLwKIChJfLjawB2A3gOzeBb3wE/\n3ovmW/rnADzT+ndnp9ck8KOjawLgRjSL4j6H5h+av7zomv05gJcB/C2A7vmcR9/wEyJTct/wEyJb\nFPxCZIqCX4hMUfALkSkKfiEyRcEvRKYo+IXIFAW/EJnyL+lZuHwLcy8CAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKnsdKUUovds",
        "colab_type": "code",
        "outputId": "c2610938-f29e-49d8-d91a-33ecc8f69279",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "vz.plot_cifar10_files(train_ds2)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAH9tJREFUeJztnXmMXNeV3r9Ta2/VK7cWF5FiJNGU\nRNEaWvLAiiPJkaFxjMhKAsfGwFAAYzQIxkgMTP4QHGDsAEFgB2Mb/iNwQI/k0QwUy56xDWsSZSKP\nRjOyR44kaiG1UBYpsklx6529VFdV13LyRxUDirrf7RabrKb8vh9AsPqeuu+deu+dV1X3q3OOuTuE\nEMkjtdoOCCFWBwW/EAlFwS9EQlHwC5FQFPxCJBQFvxAJRcEvREJR8AuRUBT8QiSUzEomm9k9AL4D\nIA3gT9z967HnFzpzPlToCNpivzNMpcL3KINFZvEtxvZVzTSoraMjfLjcq3ROqcJ9dK9TW3cXPzVz\nc3yep8L+d3Z20Tml+TK1pdP8/aGn0Elt9XrYj0qF+97VGdsen7dYrVFboac3OF6r8TkTk6PUFrt6\n6vwywMBQP7V1dhSC42dnJ+icXKY7OD41eRbz8wuxwPj/XHTwm1kawH8DcDeAEwBeMLPH3f0NNmeo\n0IE/+te3BW015/7mu8IXRTr2waXBg9jT3HZqaIHaduxYExyvLY7ROQeO8tdVL89S20d2h/cFAE//\n4izfZtdicPymm26ic17+h19TW38hHDwA8PF/cgO1Tc+UguMjI9N0zq4bdlHb7MwMtR0bm6S2O267\nOzg+Nc3n/Mn3/5jaUuA3oZkJfkP5l797H7XdvPPO4PjP/vr7dM6WdbcEx7/xXx6icy5kJR/7bwVw\n2N2PuPsigMcA3LuC7Qkh2shKgn8jgHfO+/tEa0wI8QHgsi/4mdkDZrbPzPbNlyJfioQQbWUlwX8S\nwObz/t7UGnsX7r7X3fe4+56ezuwKdieEuJSsJPhfAHCtmW0zsxyAzwF4/NK4JYS43Fz0ar+718zs\nSwD+D5pS38Pu/npsjhmQThNbirtiZMG80M2loQb4in55nn/9yBpfVT5+9Ehw/OotXMbZPsz9+Ntn\nuZSzYwdfZZ86G15JB4AcKc5Smef3+TNnuOqwbh1/bU8++Stq6+7PB8dvvTG8sg0Ao68fp7ZGmasw\nN+7gqkOxGD7GHbkhOieT5grN/HSF2vp6e6ht7eAmavu7X/1FcLy3ewOf87dPBsfn5/j1eyEr0vnd\n/QkAT6xkG0KI1UG/8BMioSj4hUgoCn4hEoqCX4iEouAXIqGsaLX//WJmyGTCWl/d+X3ISdZWscgl\nqr4Cl10qaZ6Zdf3GcMIEADy7/6Xw9qo80aZ/gPuR7wxnOALA9Fg4QQcASuU5alsgh/H5516kc1Ar\nUtP6zVwSO3qcS3Pl0mB4fC1Pgprbf5DaOvty1DY+zZNtrrnzruD4wMZhOicdSfyqlbltzUYuzfXk\n+bk+fuxYcLw4yWNi29rwdZUj8RVC7/xCJBQFvxAJRcEvREJR8AuRUBT8QiSUtq72p8zQmQ+n9ZYq\nsbJbYTdrNZ6gk450H06luG3k9UPUtnb91uD43z37f+mc374pcn+N+H/0+Glqqy9yJSDVGa7t9s6h\nU3TOzbs2U9tbr4dXogFg28brqW10NKxITL09TuesuYonv/T1h+vcAcDcNFdbnnvqF8HxgesO0zmN\nOj++kbKLyHXya/iXz/8VtY0dD5c2y6Z4WbB//unfCY4//r9fpXMuRO/8QiQUBb8QCUXBL0RCUfAL\nkVAU/EIkFAW/EAml7Yk9uVw4QWOxwmu0pbMkWSHL710NRBIcuCKDzixPwKh2hH3fto13mnn9zbeo\nbdeuj1Dbq++8Rm3pSGLSwmy49ValwmWjdC9v5dVY5Ps6foi3taoshqW+zTt54tT6gQFqS0cu1WON\nEWo78+KbwfFKB5dZq1zpQy7L/chFqlO/dfgEtZUXwzUZu/t4Utgbp8ISbCnm/AXonV+IhKLgFyKh\nKPiFSCgKfiESioJfiISi4BcioaxI6jOzEQBzAOoAau6+J/r8dAod3URWqnH9LU2y+izD3T87z1ta\nlcpcDqmkeZusQmFtcPzGnXzOM6NcDsv3cGnr2q1cEpue4LXzDh19OzjeM8Cz4qbP8hZPOVtDbfPV\ncDYaAGzfdG1w/KbbPkbn1OZ5bcJDB16mtlNneMZivRG+rooz/BpoVLlM3N3N5bxcJ28fl2twyTRT\nCcfE2Civrfj3L4TP83yRtxN7z36X/UzOne7Om84JIa5I9LFfiISy0uB3AE+a2Ytm9sClcEgI0R5W\n+rH/dnc/aWbrAPzczN5092fOf0LrpvAAAKzrD1eZEUK0nxW987v7ydb/YwB+CuDWwHP2uvsed9/T\n1xPu2S6EaD8XHfxm1m1mhXOPAXwSAM9GEUJcUazkY/96AD81s3Pb+R/u/texCV5voDQ/H7T19/Kv\nBLV6WK4pkzZeAJAjhUIBoKODfwKZbPAKjaXZsBS1WOYtqAZ6uWz0N089SW3XXb+d2mbJMQSATCMs\nG920mbenyvZyGXBkZITaPnTDh6jtrtv/TXC8OnQNnfPoY9+mtrf2c6kv0+DXATLh62AgfzWdko/I\nvV09PGSmpnlx0qvWb6G24qkjwfFChrcou3bTdcHxp3Nc9ryQiw5+dz8C4OaLnS+EWF0k9QmRUBT8\nQiQUBb8QCUXBL0RCUfALkVDaWsDT3VFbDEtplSzPRqqTvnulUqzoJy9K2dXFi3QuzPHMMu8Ky3aj\nZ/icap1Lh32RgpULRV5gcnKWZ+GNT00Fx7Npo3O2dPNstHvu+CS1pQtcjnxrdDI4/tTzXLJ7/h9+\nRW0dKS6Z9kekyhSR+oZ6huice+74LLUdPb2f2sbnR6ht+5arqO2NV8N9A2tlHp6DQ2H/M5FM1wvR\nO78QCUXBL0RCUfALkVAU/EIkFAW/EAmlze26UsjmwyvtrNYaAMzOzQbHq87dL/C8HjQiyTuVOV77\nrzcXvlfuvG4bnbO4ia/aHxkJt1wCgENHeO2/gRxv45QeCq/cb97AfbyqsJna1mS3UtsZ4/N+8XLY\n/5E3+Wsu1/mqfR/4eemuhxUOACiT97fxs0fpnLFZrn6sWbuB2jLO1afX3gi3DQOAq0hdy+23czUl\nxV6z82v7PdtY9jOFEL9RKPiFSCgKfiESioJfiISi4BcioSj4hUgo7ZX6UoaOjnBdsuJ8WM4DgGo1\nLAOenORztmzgiSyZDn7Ps4gMuGVd2NY9yBN0kOJtoabOnOF+zPL2TkWe44JrttwQHF/XF5GoUlwX\nnZzgLbkWunjdxXw23NpszboddM6r+1+gtp6t66itsJafz4nj4UScyuJ6OufAGzzBaONVPEEnbfya\na1TK1PZPfyss6X3o7t10zmIlLCt2f/9ZOudC9M4vREJR8AuRUBT8QiQUBb8QCUXBL0RCUfALkVCW\nlPrM7GEAnwYw5u43tsYGAfwQwFYAIwA+6+5cE2qRTqdQKIQzpmqVIp1XInXY6jXetmp0km9vYo7L\nLp1pfkgqpXCG3vwJnoHXyPL76/hJ7v/0LM8G7I60NisVw7UQZ/O87l+uxtuX1SJJYvXySWob6B8M\njndlucSW/cjHqG14A89knD75OrVlLCw5HjnMW2v1ZfuobWMHr/33xsgJatu9g7cpGxreGhxvTPNr\ncb4YtjVqXG68kOW88/8pgHsuGHsQwFPufi2Ap1p/CyE+QCwZ/O7+DIALk4fvBfBI6/EjAD5zif0S\nQlxmLvY7/3p3P916fAbNjr1CiA8QK17wc3cHQH+LamYPmNk+M9t3dp5/1xZCtJeLDf5RMxsGgNb/\ntEG9u+919z3uvqe/hzfLEEK0l4sN/scB3N96fD+An10ad4QQ7WI5Ut8PANwBYI2ZnQDwVQBfB/Aj\nM/sigGMAeH+j86jV65iYCctb87O8QGOpEtabUpHWRGcmuLRV6OeFIrPdXCqZORn+2nLk+HG+PeOy\n3MG3JqhtscH9sBL/+jQ+FW6Tle/gn7qKZb69Ek9KRGcX1wFH334mOL5xC89Uu/Z6bquNHaS22UhG\naLEc/kY6P8tl1t4cT5vcf5jLeV6rUdvJsbMRW3jeoTG+r8033BQcb1g4azbEksHv7p8npk8sey9C\niCsO/cJPiISi4BcioSj4hUgoCn4hEoqCX4iE0tYCnvVaHdNT4eQ/r3PZqFQOFyvsiHg/PBDufwYA\nAxFbRyfPcKtOhP24hcguAPDrg7xI5+lJLkemM1yyWSiHM/cAoErS8DojUl82Yhs/yyWxwQI/juWx\ncC+58hzPgNx5/S5qq8zypNH9FZ4BmSaX+HwkXfG6SLHQyXHux9bNvEhqoY9LvsWZ8LHqXc8zCL0a\nlg7dudx4IXrnFyKhKPiFSCgKfiESioJfiISi4BcioSj4hUgobZX6UqkUejrD8lA2w7PY+nrCRT/n\n5rjkFatjuCaS1TcVKThSJVlbN669ms754ROvUVsllrlX4el06RS/Zy8S2atS5RJQRze/DE4fH6G2\nuW5eVJPtrVbiGZC7jvGegfORHoqPFueoracjfO0snuXbu23HNmob/Ei4FyIAdOQi9Srq/Hx6NXwd\n96b58aiPhQvUpklfyxB65xcioSj4hUgoCn4hEoqCX4iEouAXIqG0dbW/0WhgvhheTc9EbkO5fDjZ\nZibWSyrLE3SKDf6yxxb4qvjAmt7g+LGTp+ic147yOmyNKn/RVXDVIU/alwEAOyJ1WlwdmCJ1FQFg\ngiRiAcD8fDjRCQCmS+EV7OsLa+icjUN8pfrQAk+CmpgMJ8YAQKMQbr21wHOBkIuoGDfczJWA8ixX\nnxYrvEalk7PW2c1VqfJiuM5gKs3rD77nuct+phDiNwoFvxAJRcEvREJR8AuRUBT8QiQUBb8QCWU5\n7boeBvBpAGPufmNr7GsAfg/AeOtpX3H3J5baVjqbw+BVm4O22SKXm0oeTorI9PAacvlOLpMUNqyl\nts19fN5A/0BwfLbIZZz52XACBgCUI+2dLM2TfhqRe3aR1PebmePHd3Kay2gN3oAZU9NcBpwlMuzV\nvcN0TmeWS5i5SKZWMSI5dqbC8xZr/HUtRJKg8nl+fdQieT3p7rDkCAANlvSzyOXezkzYf/Jyw89d\nxnP+FMA9gfFvu/vu1r8lA18IcWWxZPC7+zMA+K8ohBAfSFbynf9LZnbAzB42s/DnYSHEFcvFBv93\nAWwHsBvAaQDfZE80swfMbJ+Z7ZshP+0VQrSfiwp+dx9197q7NwB8D8Ctkefudfc97r6nrzuyIiKE\naCsXFfxmdv6S7X0AeK0qIcQVyXKkvh8AuAPAGjM7AeCrAO4ws90AHMAIgN9f1s6yGawhLYi6FsK1\n1gCgRGqcDUdkl7Tx+1r/AP8EcvVmLkWxLZ6e4VJfucS/6tSMH/5s5NSUIu260umwl6Pj48FxAFiM\n1AssV7mtEnttJEGvnuXnJQuuU70wO0FtqUirtxrx3yK17tZFWmtZnc9zcFupxK+RTD2cYlgqcQkz\nnQpn7zUay6/ht2Twu/vnA8MPLXsPQogrEv3CT4iEouAXIqEo+IVIKAp+IRKKgl+IhNLWAp4A4BaW\nc3q6uLyyvveq4HiuI+J+nctGc8VZausqhIt0AkDGwvJKZYxnzMXur5kUzyyrN3hmWS2SDehkf7Nz\nvGJlZz5HbdUqn+exdmNEfvtfp3i7rn82HM74BIDnizzz0CKt3haJj915Pmf9hnXU1nA+byjSBq5c\n4RKcNcLHeM3wIJ1TKoXnpLPLD2m98wuRUBT8QiQUBb8QCUXBL0RCUfALkVAU/EIklLZKfWYpZPLh\nopulMs8eq5Pik+tzvChiJsd79Xmaz6tFMsTcw7bGwhyds3UD703365Nj1FYHl/Ni1GL9C9m+Iq85\nk+aXSLqDy161cjjj76Wzk3TOv3v1OWo7XYkVgolIfYthSWzbVfwasEgm4+j4WWrr6ebXXGmB+58i\nRVIzmUhWH8la9UjW4Xv3K4RIJAp+IRKKgl+IhKLgFyKhKPiFSChtXe1PpVLoyodr9VUiLZLmZ8Or\n6ax9FgDUU3wFOxtTApyvljZq4ZXjDUM8Gei2G66mtpFTZ6htpsITauA8IchS4ft5I7KiX1/kq9vp\nTDiZCQA8ssoOchyzkTZkL02OUlsmUpMxHelRNdQbvt7+xV230TmDPTzJrFLnx75MFA4AaESuqxKp\nhZgm5xIAbaJW12q/EGIpFPxCJBQFvxAJRcEvREJR8AuRUBT8QiSU5bTr2gzgzwCsR1Nh2Ovu3zGz\nQQA/BLAVzZZdn3X36di2UmbozIXrxcWq4PX0h5Mw6ou8bVW5zKWtQoHXWqtHJJl6I7zNTIFLQ3tu\n3k5tI2emqO3vX36T2iqRGn6p99Gu6RwxGWpxISI5RpS+bJpIhJE56ch7UUTpg1HhC/jMHTcExz90\nEz8vZePbi5FJZbnRuZzaR67vYqSrNet6ZqRGZojlvPPXAPyhu+8E8FEAf2BmOwE8COApd78WwFOt\nv4UQHxCWDH53P+3uL7UezwE4CGAjgHsBPNJ62iMAPnO5nBRCXHre13d+M9sK4MMAngOw3t1Pt0xn\n0PxaIIT4gLDs4DezHgA/BvBld39X4Xt3d5BfHJrZA2a2z8z2Tc7EvtkLIdrJsoLfzLJoBv6j7v6T\n1vComQ237MMAgmVp3H2vu+9x9z1DfT2XwmchxCVgyeC35vLhQwAOuvu3zjM9DuD+1uP7Afzs0rsn\nhLhcLCer72MAvgDgVTN7pTX2FQBfB/AjM/sigGMAPrvUhqq1Gk6Ph2u4VStctkt1hrPwGqUi35nx\nl1Yulfi8SCZVnflI/AOAwiCvFXf3x2+ltg4iiQLAXz27n9oaLKuLJ+chG2lBdd3Va6lttsjP2Xwl\nLIuWIvJsI9JirbLI5c1/vOt6arthx9bgeC3SKq3kfF9duQ5qa0R87MqFa1cCABNaU5FzVqmG/Y8k\nfL6HJYPf3X8Jrs5+Yvm7EkJcSegXfkIkFAW/EAlFwS9EQlHwC5FQFPxCJJT2tuuCIU8KOGbyXNrK\n5cPySnmOt046duQotTl4xl9f3yC1LTbCOkpnjmdzdXbzHzZt6uPyz8d28ayzVw7y1zZVDP+KsiPF\nM/eG+rgc+YlbdlIb6ly2M1Ko9e1TPJPx2Vd4JuNgvp/afve+O6lteCg8r2+Ab2/mLE9OrYDLxOk8\nvw5qEQmuRjIxG41IAc9MOF48lv54AXrnFyKhKPiFSCgKfiESioJfiISi4BcioSj4hUgobZX60qkU\nCt3hYpeNSOHJGssrSnP3BzesobbJ6RlqOzE+wf1YDEuElYUFOieV53JexH3kOrls9Knbb6a2sYlg\nWQXMneWvefdv7eZ+UAtQj2SxNRrhwp+bB7j0eefurdQ2vH4jtQ328APZIMVOz05xOa9W5RJmvoNn\n9dUiRWPBCpoC8GzY/2wn3xeq4X3Z+yg+qnd+IRKKgl+IhKLgFyKhKPiFSCgKfiESSltX+x2Oej3c\ntsgjrY5m58Ntiypk9R0AOiOVgndu3ERt9SpvT9Woh1eOpyZ5gtHM7By1kTJsrZ3xlfQbt/NEnPr2\nDcHxtw4fpnMKOf4ewFpJAUCjwttJvXN6PDgeUzFu2XIttY2dmqW2Xz79MrUB4ett7TBXDzZuGKK2\nwaFIjcdIp7Q1m3hbixpJxhk7c5LOWSiGj0ejHlEcLkDv/EIkFAW/EAlFwS9EQlHwC5FQFPxCJBQF\nvxAJZUmpz8w2A/gzNFtwO4C97v4dM/sagN8DcE7T+Yq7PxHbljuwyLpJRVokZdNhTSxWFy0dqX9W\nLvHEjRKpgQcA6VQ4wai3hyfvWKReYIokdADA9DSXCGcj3Y57+8O18wb7C5Ht8X3VjUtzXuUJTQO9\n4QSudIafl1ykRdm2bVy6zWa5j+OT4ZqBoxPhtnEAMDE6Sm35SK3JVJYn7wyfOENthUJvcLwSkVJ7\nushrfh/9upaj89cA/KG7v2RmBQAvmtnPW7Zvu/sfL3tvQogrhuX06jsN4HTr8ZyZHQTAfyEhhPhA\n8L6+85vZVgAfBvBca+hLZnbAzB42s4FL7JsQ4jKy7OA3sx4APwbwZXefBfBdANsB7Ebzk8E3ybwH\nzGyfme2bno201BZCtJVlBb+ZZdEM/Efd/ScA4O6j7l539waA7wEINpt3973uvsfd97BFICFE+1ky\n+M3MADwE4KC7f+u88eHznnYfgNcuvXtCiMvFclb7PwbgCwBeNbNXWmNfAfB5M9uNpvw3AuD3l9qQ\nu6NWCWfNsZZFAFAl2XTpTMx9fl+rlXnLJTS4NFckEmE6xeWVeiQ7r1qJzYtkLEbkphkibXV1hSVA\nAMhk+bEqzvDaf/nIvEwhXH8uRWRbAOjq5pIpwGU0S3E/1q9fFxxfu45n2eVISzkAKJX4V9fZIpfm\napFsu3dOHAuOT0zxY18h2afzRS6/XshyVvt/CQQraEY1fSHElY1+4SdEQlHwC5FQFPxCJBQFvxAJ\nRcEvREJpawFPAGj+bOC9LEaqH7KihDGlryPHM72qlUibqVSkrRJpGzY+ySUZ56oRLMuN1YhstMhd\nRO9Q+IdUmTSX+qZnedHSXAeX5mq1cHFMAFgoh4/x+g39dE6+g2fuzUbajcUuYnaEU5Hz3DPAC3h2\ndXE5sq+fH8fuLl4ItVILS8gDkZZik8QWy3C8EL3zC5FQFPxCJBQFvxAJRcEvREJR8AuRUBT8QiSU\n9vbqc0elFpaAigs8067QE5apiGoIAKhHsgTrkRqHqYh+mO0Ky2iNs7wAZndHntoqVf6aU91cmqvn\nuY+Z7rCPvWkuo00t8OKSc5GCptnIsUplw1l9uR4u9R07zotq9nZzCcucS44NcpGkjF8E42Nj1JbP\n8ozKnn5uGx09xed1hc9NR0QKzpHir0xKD6F3fiESioJfiISi4BcioSj4hUgoCn4hEoqCX4iE0lap\nr9FooLIQLjDYkedSTq0algdjRS7LpFAoANQjGYT9fbynXZ30ThuK9MFrEGkTACLqJqo0Hw3RJoUd\nA+G+b0ff4f3nuru5/8V5XrCyi8iKAABynstlfuy9znsoFov8fPYUuCyazoWz99KR89IwnvFXrnIf\nM8VIBmSd+1+uEqkyFQnPNPFx+Uqf3vmFSCoKfiESioJfiISi4BcioSj4hUgoS672m1kHgGcA5FvP\n/0t3/6qZbQPwGIAhAC8C+IJ7JMMCQCplyOXDiS6xhIRKJbzC2km2BQDVRb66ms5G6vRF2iplSVuo\nFFt5BdAgCRgA0Igszc7O8mSh3h6+yp4mq9GlIq+BZ+Cn7epNvK1VZ2c4eQcAGkRRmTg9TudkU/zY\n93Xy49iVi13G4W1WGnxlPp/i56UWUVqKkS7Ufb1hFQYAqiQJbWGRKws9neEkolSkddl7nruM51QA\n3OXuN6PZjvseM/sogG8A+La7/yMA0wC+uOy9CiFWnSWD35ucy+vMtv45gLsA/GVr/BEAn7ksHgoh\nLgvL+oxgZulWh94xAD8H8DaAs+5+7pcSJwBsvDwuCiEuB8sKfnevu/tuAJsA3Apgx3J3YGYPmNk+\nM9s3Pbf89sFCiMvL+1rtd/ezAJ4G8NsA+s3s3ErLJgAnyZy97r7H3fcMFGL914UQ7WTJ4DeztWbW\n33rcCeBuAAfRvAn8q9bT7gfws8vlpBDi0rOcxJ5hAI+YWRrNm8WP3P1/mtkbAB4zs/8M4GUADy21\nIXegUQ8nVGQi9eB6usOfGOo1niSSy/N6al2R+njzM1xiq9XCslFHB98XGlw26u7iUlk+x7c5t8il\nufnJcB287RvX0TlVck4AIN/HP63VwY9/bTFsy1S5VJbL8Ndc6OTnLBNpzWbEx0qVy3LW4Mc3m+bn\n09Lcf4/UlFyYC/tSJzIlwHO7Yvu5kCWD390PAPhwYPwImt//hRAfQPQLPyESioJfiISi4BcioSj4\nhUgoCn4hEoq5R3pXXeqdmY0DONb6cw2AibbtnCM/3o38eDcfND+udve1y9lgW4P/XTs22+fue1Zl\n5/JDfsgPfewXIqko+IVIKKsZ/HtXcd/nIz/ejfx4N7+xfqzad34hxOqij/1CJJRVCX4zu8fMfm1m\nh83swdXwoeXHiJm9amavmNm+Nu73YTMbM7PXzhsbNLOfm9mh1v8Dq+TH18zsZOuYvGJmn2qDH5vN\n7Gkze8PMXjezf98ab+sxifjR1mNiZh1m9ryZ7W/58Z9a49vM7LlW3PzQzCLppMvA3dv6D0AazTJg\n1wDIAdgPYGe7/Wj5MgJgzSrs9+MAbgHw2nlj/xXAg63HDwL4xir58TUA/6HNx2MYwC2txwUAbwHY\n2e5jEvGjrccEzY57Pa3HWQDPAfgogB8B+Fxr/L8D+Lcr2c9qvPPfCuCwux/xZqnvxwDcuwp+rBru\n/gyAqQuG70WzECrQpoKoxI+24+6n3f2l1uM5NIvFbESbj0nEj7biTS570dzVCP6NAN457+/VLP7p\nAJ40sxfN7IFV8uEc6939dOvxGQC8YP7l50tmdqD1teCyf/04HzPbimb9iOewisfkAj+ANh+TdhTN\nTfqC3+3ufguA3wHwB2b28dV2CGje+dG8Ma0G3wWwHc0eDacBfLNdOzazHgA/BvBld58939bOYxLw\no+3HxFdQNHe5rEbwnwSw+by/afHPy427n2z9Pwbgp1jdykSjZjYMAK3/x1bDCXcfbV14DQDfQ5uO\niZll0Qy4R939J63hth+TkB+rdUxa+37fRXOXy2oE/wsArm2tXOYAfA7A4+12wsy6zaxw7jGATwJ4\nLT7rsvI4moVQgVUsiHou2FrchzYcE2v2ansIwEF3/9Z5prYeE+ZHu49J24rmtmsF84LVzE+huZL6\nNoD/uEo+XIOm0rAfwOvt9APAD9D8+FhF87vbF9HsefgUgEMA/gbA4Cr58ecAXgVwAM3gG26DH7ej\n+ZH+AIBXWv8+1e5jEvGjrccEwC40i+IeQPNG80fnXbPPAzgM4C8A5FeyH/3CT4iEkvQFPyESi4Jf\niISi4BcioSj4hUgoCn4hEoqCX4iEouAXIqEo+IVIKP8PkKWdTmpnHMQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhUgoCn4hEoqCX4iEouAXIqEo+IVIKP8XXEGO7ze+UQ8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlPxCJIqSX4hEUfILkShKfiES5f8Bz0cHMP8pf3sAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qN5z9vjRsvL",
        "colab_type": "text"
      },
      "source": [
        "### Training - Build model , compile and train "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m26XDkJi1YyS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, GlobalAveragePooling2D,  Activation, GlobalMaxPooling2D\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization , Dense, Lambda\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam,SGD\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "WEIGHT_DECAY=1.25e-4\n",
        "reg=tf.keras.regularizers.l2(WEIGHT_DECAY)\n",
        "def init_pytorch(shape, dtype=tf.float32, partition_info=None):\n",
        "  fan = np.prod(shape[:-1])\n",
        "  bound = 1 / math.sqrt(fan)\n",
        "  return tf.random.uniform(shape, minval=-bound, maxval=bound, dtype=dtype)\n",
        "\n",
        "def conv(inp,f=32,k=3):\n",
        "  conv_layer=Conv2D(f,k,use_bias=False,padding='same',kernel_initializer=init_pytorch, kernel_regularizer=reg)(inp)\n",
        "  conv_layer=BatchNormalization(momentum=0.9, epsilon=1e-5)(conv_layer)\n",
        "  conv_layer=Activation('relu')(conv_layer)\n",
        "  return conv_layer\n",
        "def resBlk(inp,f=32,k=3,residual=True) :\n",
        "  res1=conv(inp,f,k)\n",
        "  res1=MaxPooling2D(pool_size=(2,2))(res1)\n",
        "  if residual:\n",
        "    res2=conv(res1,f,k)\n",
        "    res3=conv(res2,f,k)\n",
        "    return res1+res3\n",
        "  else:\n",
        "    return res1  \n",
        "\n",
        "def apply_weight(x):\n",
        "  return x*0.125  \n",
        "\n",
        "def random_pad_crop(image,padding=2):\n",
        "  shp=tf.shape(image)\n",
        "  \n",
        "  image=tf.pad(image,[(0, 0), (padding, padding), (padding, padding), (0, 0)], mode='reflect')\n",
        "  \n",
        "  image=tf.image.random_crop(image,size=shp)\n",
        "  return image  \n",
        "\n",
        "def flip_left_right(image):\n",
        "  return tf.image.random_flip_left_right(image)   \n",
        "\n",
        "def aug_fn1(img):\n",
        "  shp=img.get_shape()\n",
        "  #print(shp)\n",
        "  #out_shp=[shp[0],shp[1],shp[2]]\n",
        "  return ds.flip_left_right(ds.random_crop(ds.cutout(ds.pad_img(img,padding=2),100,size=4),out_shape=shp))\n",
        "\n",
        "def aug_fn2(img):\n",
        "  shp=img.get_shape()\n",
        "  #print(shp)\n",
        "  #out_shp=[shp[0],shp[1],shp[2]]\n",
        "  return ds.flip_left_right(ds.random_crop(ds.cutout(ds.pad_img(img,padding=1),100,size=2),out_shape=shp))\n",
        "\n",
        "def aug1(image):\n",
        "  print('is_training',is_training)\n",
        "  if is_training:\n",
        "    \n",
        "    #return tf.map_fn(lambda img: aug_fn1(img),image,parallel_iterations=ds.CPU_CORES,back_prop=is_training) \n",
        "    return tf.map_fn(lambda img: aug_fn1(img) ,image,back_prop=is_training)    \n",
        "  else:\n",
        "    print('inside validation cycle\\n===============\\n')\n",
        "    return image  \n",
        "\n",
        "def aug2(image):\n",
        "  \n",
        "  print('is_training',is_training)\n",
        "  if is_training:\n",
        "    \n",
        "    return tf.map_fn(lambda img: aug_fn2(img) ,image,back_prop=is_training)\n",
        "  else:\n",
        "    print('inside validation cycle\\n===============\\n')\n",
        "    return image        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVqdMyzkx4tY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(distort_param=0):\n",
        "  f=64\n",
        "  inp=Input(shape=(32,32,3))\n",
        "  layer1=conv(inp,f,3)\n",
        "  res1=resBlk(layer1,f*2,3)\n",
        "  if (distort_param in [1,3,4]):\n",
        "    res1=Lambda(aug1)(res1)\n",
        "  res2=resBlk(res1,f*4,3,False)\n",
        "  if (distort_param in [2,3,5]):\n",
        "    res2=Lambda(aug2)(res2)\n",
        "  res3=resBlk(res2,f*8,3)\n",
        "  \n",
        "  layer2=GlobalMaxPooling2D()(res3)\n",
        "  layer3=Dense(10, kernel_initializer=init_pytorch, use_bias=False,kernel_regularizer=reg)(layer2)\n",
        "  layer4=Lambda(lambda x: x*0.125)(layer3)\n",
        "  out=Activation('softmax')(layer4)\n",
        "  model=Model(inputs=[inp],outputs=[out])\n",
        "  model.summary()\n",
        "  return model "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nng9bFm1rQq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "\n",
        "def lr_schedule():\n",
        "    \n",
        "    def schedule(epoch):\n",
        "\n",
        "      lr=lr1=np.interp([epoch],[0, EPOCHS//3,EPOCHS], [0.025, 0.4, 0])[0]\n",
        "      print('epoch ', epoch+1, ': setting learning rate to ',lr1)\n",
        "      return lr\n",
        "    \n",
        "    return LearningRateScheduler(schedule)\n",
        "\n",
        "lr_sched = lr_schedule()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaDK2E2h81vV",
        "colab_type": "code",
        "outputId": "9e85540b-5a60-492f-bfc4-a6337bc17b68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "for model_params in [5,4,3,2,1,0]:\n",
        "  is_training=True\n",
        "  #global_step = tf.train.get_or_create_global_step()\n",
        "  model=model=build_model(model_params)\n",
        "  opt=SGD(lr=0.025,momentum=0.9,nesterov=True)\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,metrics=['accuracy']\n",
        "              )\n",
        "  batch_size=512\n",
        "  \n",
        "  if model_params in [0,4,5]:\n",
        "    train_ds=train_ds1\n",
        "    print('Model will be trained with image augmentation in put layer\\n====================')\n",
        "    if model_params==4:\n",
        "      print('Model will also include distortion after Res Blk 1\\n=======================')\n",
        "    elif model_params==5:\n",
        "      print('Model will also include distortion after Res Blk 2\\n=======================')\n",
        "      \n",
        "  else:\n",
        "    if model_params==1:\n",
        "      print('Model will be trained with distortion after Res Blk 1\\n=======================')\n",
        "    elif model_params==2:\n",
        "      print('Model will be trained with distortion after Res Blk 2\\n=======================')\n",
        "    elif model_params==3:\n",
        "      print('Model will be trained with distortion after Res Blk 1 and Res Blk 2\\n=======================')    \n",
        "    train_ds=train_ds2  \n",
        "  model.fit(train_ds,epochs=EPOCHS, steps_per_epoch=np.ceil(50000/batch_size), \n",
        "          callbacks=[lr_sched],\n",
        "          verbose=1)\n",
        "  is_training=False\n",
        "  score=model.evaluate(test_ds, steps =np.ceil(10000/batch_size), verbose=1)\n",
        "\n",
        "  del(model)\n",
        "  del(train_ds)\n",
        "  \n",
        "  print('val accuracy score at the end of training model type ',model_params, score)\n",
        "  print(\"=========================================\\n\")\n",
        "\n",
        "#validation_data=test_ds, validation_steps=np.ceil(10000/batch_size),"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "is_training True\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 32, 32, 64)   1728        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 32, 32, 64)   256         conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 32, 32, 64)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 128)  73728       activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 128)  512         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 128)  0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 16, 16, 128)  0           activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 16, 16, 128)  147456      max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 16, 16, 128)  512         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 16, 16, 128)  0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 16, 16, 128)  147456      activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 16, 16, 128)  512         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 16, 16, 128)  0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add (TensorFlowOpLa [(None, 16, 16, 128) 0           max_pooling2d[0][0]              \n",
            "                                                                 activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 16, 16, 256)  294912      tf_op_layer_add[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 16, 16, 256)  1024        conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 16, 16, 256)  0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 8, 8, 256)    0           activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, 8, 8, 256)    0           max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 8, 8, 512)    1179648     lambda[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 8, 8, 512)    2048        conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 8, 8, 512)    0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 4, 4, 512)    0           activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 4, 4, 512)    2359296     max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 4, 4, 512)    2048        conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 4, 4, 512)    0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 4, 4, 512)    2359296     activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 4, 4, 512)    2048        conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 4, 4, 512)    0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_1 (TensorFlowOp [(None, 4, 4, 512)]  0           max_pooling2d_2[0][0]            \n",
            "                                                                 activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling2d (GlobalMax (None, 512)          0           tf_op_layer_add_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 10)           5120        global_max_pooling2d[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 10)           0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 10)           0           lambda_1[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 6,577,600\n",
            "Trainable params: 6,573,120\n",
            "Non-trainable params: 4,480\n",
            "__________________________________________________________________________________________________\n",
            "Model will be trained with image augmentation in put layer\n",
            "====================\n",
            "Model will also include distortion after Res Blk 2\n",
            "=======================\n",
            "Train for 98.0 steps\n",
            "epoch  1 : setting learning rate to  0.025\n",
            "Epoch 1/50\n",
            "is_training True\n",
            "is_training True\n",
            "98/98 [==============================] - 172s 2s/step - loss: 1.7205 - accuracy: 0.4069\n",
            "epoch  2 : setting learning rate to  0.0484375\n",
            "Epoch 2/50\n",
            "98/98 [==============================] - 161s 2s/step - loss: 1.2125 - accuracy: 0.5964\n",
            "epoch  3 : setting learning rate to  0.071875\n",
            "Epoch 3/50\n",
            "98/98 [==============================] - 161s 2s/step - loss: 0.9743 - accuracy: 0.6911\n",
            "epoch  4 : setting learning rate to  0.0953125\n",
            "Epoch 4/50\n",
            "98/98 [==============================] - 161s 2s/step - loss: 0.8493 - accuracy: 0.7393\n",
            "epoch  5 : setting learning rate to  0.11875\n",
            "Epoch 5/50\n",
            "98/98 [==============================] - 162s 2s/step - loss: 0.7707 - accuracy: 0.7716\n",
            "epoch  6 : setting learning rate to  0.1421875\n",
            "Epoch 6/50\n",
            "98/98 [==============================] - 162s 2s/step - loss: 0.7192 - accuracy: 0.7934\n",
            "epoch  7 : setting learning rate to  0.165625\n",
            "Epoch 7/50\n",
            "98/98 [==============================] - 162s 2s/step - loss: 0.6836 - accuracy: 0.8089\n",
            "epoch  8 : setting learning rate to  0.1890625\n",
            "Epoch 8/50\n",
            "98/98 [==============================] - 161s 2s/step - loss: 0.6667 - accuracy: 0.8187\n",
            "epoch  9 : setting learning rate to  0.2125\n",
            "Epoch 9/50\n",
            "98/98 [==============================] - 162s 2s/step - loss: 0.6423 - accuracy: 0.8294\n",
            "epoch  10 : setting learning rate to  0.2359375\n",
            "Epoch 10/50\n",
            "98/98 [==============================] - 161s 2s/step - loss: 0.6381 - accuracy: 0.8363\n",
            "epoch  11 : setting learning rate to  0.259375\n",
            "Epoch 11/50\n",
            "98/98 [==============================] - 161s 2s/step - loss: 0.6226 - accuracy: 0.8471\n",
            "epoch  12 : setting learning rate to  0.2828125\n",
            "Epoch 12/50\n",
            "98/98 [==============================] - 160s 2s/step - loss: 0.6296 - accuracy: 0.8483\n",
            "epoch  13 : setting learning rate to  0.30625\n",
            "Epoch 13/50\n",
            "98/98 [==============================] - 160s 2s/step - loss: 0.6355 - accuracy: 0.8503\n",
            "epoch  14 : setting learning rate to  0.3296875\n",
            "Epoch 14/50\n",
            "98/98 [==============================] - 159s 2s/step - loss: 0.6379 - accuracy: 0.8541\n",
            "epoch  15 : setting learning rate to  0.353125\n",
            "Epoch 15/50\n",
            "98/98 [==============================] - 161s 2s/step - loss: 0.6317 - accuracy: 0.8620\n",
            "epoch  16 : setting learning rate to  0.3765625\n",
            "Epoch 16/50\n",
            "98/98 [==============================] - 163s 2s/step - loss: 0.6432 - accuracy: 0.8611\n",
            "epoch  17 : setting learning rate to  0.4\n",
            "Epoch 17/50\n",
            "98/98 [==============================] - 162s 2s/step - loss: 0.6463 - accuracy: 0.8651\n",
            "epoch  18 : setting learning rate to  0.38823529411764707\n",
            "Epoch 18/50\n",
            "98/98 [==============================] - 163s 2s/step - loss: 0.6261 - accuracy: 0.8737\n",
            "epoch  19 : setting learning rate to  0.3764705882352941\n",
            "Epoch 19/50\n",
            "98/98 [==============================] - 164s 2s/step - loss: 0.6139 - accuracy: 0.8812\n",
            "epoch  20 : setting learning rate to  0.3647058823529412\n",
            "Epoch 20/50\n",
            "98/98 [==============================] - 164s 2s/step - loss: 0.5980 - accuracy: 0.8870\n",
            "epoch  21 : setting learning rate to  0.35294117647058826\n",
            "Epoch 21/50\n",
            "98/98 [==============================] - 162s 2s/step - loss: 0.5851 - accuracy: 0.8914\n",
            "epoch  22 : setting learning rate to  0.3411764705882353\n",
            "Epoch 22/50\n",
            "98/98 [==============================] - 160s 2s/step - loss: 0.5703 - accuracy: 0.8964\n",
            "epoch  23 : setting learning rate to  0.3294117647058824\n",
            "Epoch 23/50\n",
            "98/98 [==============================] - 163s 2s/step - loss: 0.5633 - accuracy: 0.8994\n",
            "epoch  24 : setting learning rate to  0.31764705882352945\n",
            "Epoch 24/50\n",
            "98/98 [==============================] - 162s 2s/step - loss: 0.5533 - accuracy: 0.9027\n",
            "epoch  25 : setting learning rate to  0.3058823529411765\n",
            "Epoch 25/50\n",
            "98/98 [==============================] - 162s 2s/step - loss: 0.5401 - accuracy: 0.9063\n",
            "epoch  26 : setting learning rate to  0.29411764705882354\n",
            "Epoch 26/50\n",
            "98/98 [==============================] - 163s 2s/step - loss: 0.5247 - accuracy: 0.9112\n",
            "epoch  27 : setting learning rate to  0.2823529411764706\n",
            "Epoch 27/50\n",
            "98/98 [==============================] - 163s 2s/step - loss: 0.5138 - accuracy: 0.9153\n",
            "epoch  28 : setting learning rate to  0.2705882352941177\n",
            "Epoch 28/50\n",
            "98/98 [==============================] - 163s 2s/step - loss: 0.5040 - accuracy: 0.9174\n",
            "epoch  29 : setting learning rate to  0.25882352941176473\n",
            "Epoch 29/50\n",
            "98/98 [==============================] - 162s 2s/step - loss: 0.4984 - accuracy: 0.9174\n",
            "epoch  30 : setting learning rate to  0.24705882352941178\n",
            "Epoch 30/50\n",
            "98/98 [==============================] - 163s 2s/step - loss: 0.4819 - accuracy: 0.9237\n",
            "epoch  31 : setting learning rate to  0.23529411764705885\n",
            "Epoch 31/50\n",
            "98/98 [==============================] - 163s 2s/step - loss: 0.4766 - accuracy: 0.9245\n",
            "epoch  32 : setting learning rate to  0.22352941176470592\n",
            "Epoch 32/50\n",
            "98/98 [==============================] - 162s 2s/step - loss: 0.4657 - accuracy: 0.9273\n",
            "epoch  33 : setting learning rate to  0.21176470588235297\n",
            "Epoch 33/50\n",
            "98/98 [==============================] - 161s 2s/step - loss: 0.4502 - accuracy: 0.9330\n",
            "epoch  34 : setting learning rate to  0.2\n",
            "Epoch 34/50\n",
            "98/98 [==============================] - 163s 2s/step - loss: 0.4389 - accuracy: 0.9350\n",
            "epoch  35 : setting learning rate to  0.18823529411764708\n",
            "Epoch 35/50\n",
            "98/98 [==============================] - 162s 2s/step - loss: 0.4231 - accuracy: 0.9392\n",
            "epoch  36 : setting learning rate to  0.17647058823529416\n",
            "Epoch 36/50\n",
            "98/98 [==============================] - 163s 2s/step - loss: 0.4153 - accuracy: 0.9414\n",
            "epoch  37 : setting learning rate to  0.1647058823529412\n",
            "Epoch 37/50\n",
            "98/98 [==============================] - 163s 2s/step - loss: 0.4005 - accuracy: 0.9456\n",
            "epoch  38 : setting learning rate to  0.15294117647058825\n",
            "Epoch 38/50\n",
            "98/98 [==============================] - 163s 2s/step - loss: 0.3838 - accuracy: 0.9501\n",
            "epoch  39 : setting learning rate to  0.14117647058823535\n",
            "Epoch 39/50\n",
            "98/98 [==============================] - 163s 2s/step - loss: 0.3729 - accuracy: 0.9524\n",
            "epoch  40 : setting learning rate to  0.1294117647058824\n",
            "Epoch 40/50\n",
            "98/98 [==============================] - 162s 2s/step - loss: 0.3569 - accuracy: 0.9570\n",
            "epoch  41 : setting learning rate to  0.11764705882352944\n",
            "Epoch 41/50\n",
            "98/98 [==============================] - 163s 2s/step - loss: 0.3457 - accuracy: 0.9587\n",
            "epoch  42 : setting learning rate to  0.10588235294117648\n",
            "Epoch 42/50\n",
            "98/98 [==============================] - 163s 2s/step - loss: 0.3316 - accuracy: 0.9626\n",
            "epoch  43 : setting learning rate to  0.09411764705882353\n",
            "Epoch 43/50\n",
            "98/98 [==============================] - 162s 2s/step - loss: 0.3193 - accuracy: 0.9651\n",
            "epoch  44 : setting learning rate to  0.08235294117647063\n",
            "Epoch 44/50\n",
            "98/98 [==============================] - 162s 2s/step - loss: 0.3077 - accuracy: 0.9681\n",
            "epoch  45 : setting learning rate to  0.07058823529411767\n",
            "Epoch 45/50\n",
            "98/98 [==============================] - 162s 2s/step - loss: 0.2921 - accuracy: 0.9716\n",
            "epoch  46 : setting learning rate to  0.05882352941176472\n",
            "Epoch 46/50\n",
            "98/98 [==============================] - 162s 2s/step - loss: 0.2774 - accuracy: 0.9771\n",
            "epoch  47 : setting learning rate to  0.04705882352941182\n",
            "Epoch 47/50\n",
            "98/98 [==============================] - 162s 2s/step - loss: 0.2646 - accuracy: 0.9804\n",
            "epoch  48 : setting learning rate to  0.035294117647058865\n",
            "Epoch 48/50\n",
            "98/98 [==============================] - 161s 2s/step - loss: 0.2557 - accuracy: 0.9820\n",
            "epoch  49 : setting learning rate to  0.02352941176470591\n",
            "Epoch 49/50\n",
            "98/98 [==============================] - 162s 2s/step - loss: 0.2423 - accuracy: 0.9862\n",
            "epoch  50 : setting learning rate to  0.011764705882352955\n",
            "Epoch 50/50\n",
            "98/98 [==============================] - 163s 2s/step - loss: 0.2383 - accuracy: 0.9873\n",
            "is_training False\n",
            "inside validation cycle\n",
            "===============\n",
            "\n",
            "20/20 [==============================] - 3s 140ms/step - loss: 0.3890 - accuracy: 0.9434\n",
            "val accuracy score at the end of training model type  5 [0.3890042811632156, 0.9433594]\n",
            "=========================================\n",
            "\n",
            "is_training True\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 32, 32, 64)   1728        input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 32, 32, 64)   256         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 32, 32, 64)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 32, 32, 128)  73728       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 32, 32, 128)  512         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 32, 32, 128)  0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 16, 16, 128)  0           activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 16, 16, 128)  147456      max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 16, 16, 128)  512         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 16, 16, 128)  0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 16, 16, 128)  147456      activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 16, 16, 128)  512         conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 16, 16, 128)  0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_2 (TensorFlowOp [(None, 16, 16, 128) 0           max_pooling2d_3[0][0]            \n",
            "                                                                 activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_2 (Lambda)               (None, 16, 16, 128)  0           tf_op_layer_add_2[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 16, 16, 256)  294912      lambda_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 16, 16, 256)  1024        conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 16, 16, 256)  0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 8, 8, 256)    0           activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 8, 8, 512)    1179648     max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 8, 8, 512)    2048        conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 8, 8, 512)    0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2D)  (None, 4, 4, 512)    0           activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 4, 4, 512)    2359296     max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 4, 4, 512)    2048        conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 4, 4, 512)    0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 4, 4, 512)    2359296     activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 4, 4, 512)    2048        conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 4, 4, 512)    0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_3 (TensorFlowOp [(None, 4, 4, 512)]  0           max_pooling2d_5[0][0]            \n",
            "                                                                 activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling2d_1 (GlobalM (None, 512)          0           tf_op_layer_add_3[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 10)           5120        global_max_pooling2d_1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "lambda_3 (Lambda)               (None, 10)           0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 10)           0           lambda_3[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 6,577,600\n",
            "Trainable params: 6,573,120\n",
            "Non-trainable params: 4,480\n",
            "__________________________________________________________________________________________________\n",
            "Model will be trained with image augmentation in put layer\n",
            "====================\n",
            "Model will also include distortion after Res Blk 1\n",
            "=======================\n",
            "Train for 98.0 steps\n",
            "epoch  1 : setting learning rate to  0.025\n",
            "Epoch 1/50\n",
            "is_training True\n",
            "is_training True\n",
            "98/98 [==============================] - 170s 2s/step - loss: 1.7383 - accuracy: 0.4005\n",
            "epoch  2 : setting learning rate to  0.0484375\n",
            "Epoch 2/50\n",
            "98/98 [==============================] - 167s 2s/step - loss: 1.2196 - accuracy: 0.5948\n",
            "epoch  3 : setting learning rate to  0.071875\n",
            "Epoch 3/50\n",
            "98/98 [==============================] - 167s 2s/step - loss: 0.9887 - accuracy: 0.6846\n",
            "epoch  4 : setting learning rate to  0.0953125\n",
            "Epoch 4/50\n",
            "98/98 [==============================] - 166s 2s/step - loss: 0.8551 - accuracy: 0.7377\n",
            "epoch  5 : setting learning rate to  0.11875\n",
            "Epoch 5/50\n",
            "98/98 [==============================] - 163s 2s/step - loss: 0.7772 - accuracy: 0.7672\n",
            "epoch  6 : setting learning rate to  0.1421875\n",
            "Epoch 6/50\n",
            "98/98 [==============================] - 165s 2s/step - loss: 0.7346 - accuracy: 0.7875\n",
            "epoch  7 : setting learning rate to  0.165625\n",
            "Epoch 7/50\n",
            "98/98 [==============================] - 166s 2s/step - loss: 0.6983 - accuracy: 0.8019\n",
            "epoch  8 : setting learning rate to  0.1890625\n",
            "Epoch 8/50\n",
            "98/98 [==============================] - 167s 2s/step - loss: 0.6690 - accuracy: 0.8168\n",
            "epoch  9 : setting learning rate to  0.2125\n",
            "Epoch 9/50\n",
            "98/98 [==============================] - 167s 2s/step - loss: 0.6609 - accuracy: 0.8245\n",
            "epoch  10 : setting learning rate to  0.2359375\n",
            "Epoch 10/50\n",
            "98/98 [==============================] - 166s 2s/step - loss: 0.6437 - accuracy: 0.8330\n",
            "epoch  11 : setting learning rate to  0.259375\n",
            "Epoch 11/50\n",
            "98/98 [==============================] - 167s 2s/step - loss: 0.6454 - accuracy: 0.8365\n",
            "epoch  12 : setting learning rate to  0.2828125\n",
            "Epoch 12/50\n",
            "98/98 [==============================] - 166s 2s/step - loss: 0.6406 - accuracy: 0.8435\n",
            "epoch  13 : setting learning rate to  0.30625\n",
            "Epoch 13/50\n",
            "98/98 [==============================] - 165s 2s/step - loss: 0.6383 - accuracy: 0.8497\n",
            "epoch  14 : setting learning rate to  0.3296875\n",
            "Epoch 14/50\n",
            "98/98 [==============================] - 166s 2s/step - loss: 0.6407 - accuracy: 0.8527\n",
            "epoch  15 : setting learning rate to  0.353125\n",
            "Epoch 15/50\n",
            "98/98 [==============================] - 166s 2s/step - loss: 0.6462 - accuracy: 0.8552\n",
            "epoch  16 : setting learning rate to  0.3765625\n",
            "Epoch 16/50\n",
            "98/98 [==============================] - 165s 2s/step - loss: 0.6435 - accuracy: 0.8610\n",
            "epoch  17 : setting learning rate to  0.4\n",
            "Epoch 17/50\n",
            "98/98 [==============================] - 166s 2s/step - loss: 0.6533 - accuracy: 0.8596\n",
            "epoch  18 : setting learning rate to  0.38823529411764707\n",
            "Epoch 18/50\n",
            "98/98 [==============================] - 165s 2s/step - loss: 0.6341 - accuracy: 0.8698\n",
            "epoch  19 : setting learning rate to  0.3764705882352941\n",
            "Epoch 19/50\n",
            "98/98 [==============================] - 166s 2s/step - loss: 0.6170 - accuracy: 0.8811\n",
            "epoch  20 : setting learning rate to  0.3647058823529412\n",
            "Epoch 20/50\n",
            "98/98 [==============================] - 166s 2s/step - loss: 0.6052 - accuracy: 0.8824\n",
            "epoch  21 : setting learning rate to  0.35294117647058826\n",
            "Epoch 21/50\n",
            "98/98 [==============================] - 165s 2s/step - loss: 0.5959 - accuracy: 0.8870\n",
            "epoch  22 : setting learning rate to  0.3411764705882353\n",
            "Epoch 22/50\n",
            "98/98 [==============================] - 166s 2s/step - loss: 0.5826 - accuracy: 0.8915\n",
            "epoch  23 : setting learning rate to  0.3294117647058824\n",
            "Epoch 23/50\n",
            "98/98 [==============================] - 166s 2s/step - loss: 0.5702 - accuracy: 0.8961\n",
            "epoch  24 : setting learning rate to  0.31764705882352945\n",
            "Epoch 24/50\n",
            "98/98 [==============================] - 167s 2s/step - loss: 0.5510 - accuracy: 0.9025\n",
            "epoch  25 : setting learning rate to  0.3058823529411765\n",
            "Epoch 25/50\n",
            "98/98 [==============================] - 165s 2s/step - loss: 0.5462 - accuracy: 0.9033\n",
            "epoch  26 : setting learning rate to  0.29411764705882354\n",
            "Epoch 26/50\n",
            "98/98 [==============================] - 163s 2s/step - loss: 0.5403 - accuracy: 0.9048\n",
            "epoch  27 : setting learning rate to  0.2823529411764706\n",
            "Epoch 27/50\n",
            "98/98 [==============================] - 163s 2s/step - loss: 0.5233 - accuracy: 0.9110\n",
            "epoch  28 : setting learning rate to  0.2705882352941177\n",
            "Epoch 28/50\n",
            "98/98 [==============================] - 165s 2s/step - loss: 0.5144 - accuracy: 0.9143\n",
            "epoch  29 : setting learning rate to  0.25882352941176473\n",
            "Epoch 29/50\n",
            "98/98 [==============================] - 166s 2s/step - loss: 0.4982 - accuracy: 0.9179\n",
            "epoch  30 : setting learning rate to  0.24705882352941178\n",
            "Epoch 30/50\n",
            "98/98 [==============================] - 166s 2s/step - loss: 0.4916 - accuracy: 0.9204\n",
            "epoch  31 : setting learning rate to  0.23529411764705885\n",
            "Epoch 31/50\n",
            "98/98 [==============================] - 166s 2s/step - loss: 0.4776 - accuracy: 0.9246\n",
            "epoch  32 : setting learning rate to  0.22352941176470592\n",
            "Epoch 32/50\n",
            "98/98 [==============================] - 164s 2s/step - loss: 0.4702 - accuracy: 0.9259\n",
            "epoch  33 : setting learning rate to  0.21176470588235297\n",
            "Epoch 33/50\n",
            "98/98 [==============================] - 164s 2s/step - loss: 0.4608 - accuracy: 0.9297\n",
            "epoch  34 : setting learning rate to  0.2\n",
            "Epoch 34/50\n",
            "98/98 [==============================] - 164s 2s/step - loss: 0.4400 - accuracy: 0.9345\n",
            "epoch  35 : setting learning rate to  0.18823529411764708\n",
            "Epoch 35/50\n",
            "98/98 [==============================] - 165s 2s/step - loss: 0.4328 - accuracy: 0.9364\n",
            "epoch  36 : setting learning rate to  0.17647058823529416\n",
            "Epoch 36/50\n",
            "98/98 [==============================] - 166s 2s/step - loss: 0.4158 - accuracy: 0.9412\n",
            "epoch  37 : setting learning rate to  0.1647058823529412\n",
            "Epoch 37/50\n",
            "98/98 [==============================] - 166s 2s/step - loss: 0.4047 - accuracy: 0.9435\n",
            "epoch  38 : setting learning rate to  0.15294117647058825\n",
            "Epoch 38/50\n",
            "98/98 [==============================] - 164s 2s/step - loss: 0.3897 - accuracy: 0.9480\n",
            "epoch  39 : setting learning rate to  0.14117647058823535\n",
            "Epoch 39/50\n",
            "98/98 [==============================] - 164s 2s/step - loss: 0.3799 - accuracy: 0.9494\n",
            "epoch  40 : setting learning rate to  0.1294117647058824\n",
            "Epoch 40/50\n",
            "98/98 [==============================] - 166s 2s/step - loss: 0.3657 - accuracy: 0.9525\n",
            "epoch  41 : setting learning rate to  0.11764705882352944\n",
            "Epoch 41/50\n",
            "98/98 [==============================] - 165s 2s/step - loss: 0.3508 - accuracy: 0.9567\n",
            "epoch  42 : setting learning rate to  0.10588235294117648\n",
            "Epoch 42/50\n",
            "98/98 [==============================] - 163s 2s/step - loss: 0.3373 - accuracy: 0.9618\n",
            "epoch  43 : setting learning rate to  0.09411764705882353\n",
            "Epoch 43/50\n",
            "98/98 [==============================] - 164s 2s/step - loss: 0.3244 - accuracy: 0.9637\n",
            "epoch  44 : setting learning rate to  0.08235294117647063\n",
            "Epoch 44/50\n",
            "98/98 [==============================] - 164s 2s/step - loss: 0.3075 - accuracy: 0.9688\n",
            "epoch  45 : setting learning rate to  0.07058823529411767\n",
            "Epoch 45/50\n",
            "98/98 [==============================] - 164s 2s/step - loss: 0.2918 - accuracy: 0.9732\n",
            "epoch  46 : setting learning rate to  0.05882352941176472\n",
            "Epoch 46/50\n",
            "98/98 [==============================] - 165s 2s/step - loss: 0.2809 - accuracy: 0.9752\n",
            "epoch  47 : setting learning rate to  0.04705882352941182\n",
            "Epoch 47/50\n",
            "98/98 [==============================] - 164s 2s/step - loss: 0.2669 - accuracy: 0.9790\n",
            "epoch  48 : setting learning rate to  0.035294117647058865\n",
            "Epoch 48/50\n",
            "98/98 [==============================] - 163s 2s/step - loss: 0.2571 - accuracy: 0.9815\n",
            "epoch  49 : setting learning rate to  0.02352941176470591\n",
            "Epoch 49/50\n",
            "98/98 [==============================] - 164s 2s/step - loss: 0.2457 - accuracy: 0.9857\n",
            "epoch  50 : setting learning rate to  0.011764705882352955\n",
            "Epoch 50/50\n",
            "98/98 [==============================] - 165s 2s/step - loss: 0.2409 - accuracy: 0.9863\n",
            "is_training False\n",
            "inside validation cycle\n",
            "===============\n",
            "\n",
            "20/20 [==============================] - 3s 134ms/step - loss: 0.3984 - accuracy: 0.9425\n",
            "val accuracy score at the end of training model type  4 [0.39838994294404984, 0.94248044]\n",
            "=========================================\n",
            "\n",
            "is_training True\n",
            "is_training True\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 32, 32, 64)   1728        input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 32, 32, 64)   256         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 32, 32, 64)   0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 32, 32, 128)  73728       activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 32, 32, 128)  512         conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 32, 32, 128)  0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2D)  (None, 16, 16, 128)  0           activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 16, 16, 128)  147456      max_pooling2d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 16, 16, 128)  512         conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 16, 16, 128)  0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 16, 16, 128)  147456      activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 16, 16, 128)  512         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 16, 16, 128)  0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_4 (TensorFlowOp [(None, 16, 16, 128) 0           max_pooling2d_6[0][0]            \n",
            "                                                                 activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_4 (Lambda)               (None, 16, 16, 128)  0           tf_op_layer_add_4[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 16, 16, 256)  294912      lambda_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 16, 16, 256)  1024        conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 16, 16, 256)  0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2D)  (None, 8, 8, 256)    0           activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_5 (Lambda)               (None, 8, 8, 256)    0           max_pooling2d_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 8, 8, 512)    1179648     lambda_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 8, 8, 512)    2048        conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 8, 8, 512)    0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2D)  (None, 4, 4, 512)    0           activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 4, 4, 512)    2359296     max_pooling2d_8[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 4, 4, 512)    2048        conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 4, 4, 512)    0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 4, 4, 512)    2359296     activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 4, 4, 512)    2048        conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 4, 4, 512)    0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_5 (TensorFlowOp [(None, 4, 4, 512)]  0           max_pooling2d_8[0][0]            \n",
            "                                                                 activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling2d_2 (GlobalM (None, 512)          0           tf_op_layer_add_5[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 10)           5120        global_max_pooling2d_2[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "lambda_6 (Lambda)               (None, 10)           0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 10)           0           lambda_6[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 6,577,600\n",
            "Trainable params: 6,573,120\n",
            "Non-trainable params: 4,480\n",
            "__________________________________________________________________________________________________\n",
            "Model will be trained with distortion after Res Blk 1 and Res Blk 2\n",
            "=======================\n",
            "Train for 98.0 steps\n",
            "epoch  1 : setting learning rate to  0.025\n",
            "Epoch 1/50\n",
            "is_training True\n",
            "is_training True\n",
            "is_training True\n",
            "is_training True\n",
            "98/98 [==============================] - 270s 3s/step - loss: 1.6494 - accuracy: 0.4334\n",
            "epoch  2 : setting learning rate to  0.0484375\n",
            "Epoch 2/50\n",
            "98/98 [==============================] - 265s 3s/step - loss: 1.0929 - accuracy: 0.6434\n",
            "epoch  3 : setting learning rate to  0.071875\n",
            "Epoch 3/50\n",
            "98/98 [==============================] - 263s 3s/step - loss: 0.8624 - accuracy: 0.7302\n",
            "epoch  4 : setting learning rate to  0.0953125\n",
            "Epoch 4/50\n",
            "98/98 [==============================] - 266s 3s/step - loss: 0.7358 - accuracy: 0.7798\n",
            "epoch  5 : setting learning rate to  0.11875\n",
            "Epoch 5/50\n",
            "98/98 [==============================] - 266s 3s/step - loss: 0.6517 - accuracy: 0.8124\n",
            "epoch  6 : setting learning rate to  0.1421875\n",
            "Epoch 6/50\n",
            "98/98 [==============================] - 262s 3s/step - loss: 0.6030 - accuracy: 0.8329\n",
            "epoch  7 : setting learning rate to  0.165625\n",
            "Epoch 7/50\n",
            "98/98 [==============================] - 264s 3s/step - loss: 0.5734 - accuracy: 0.8472\n",
            "epoch  8 : setting learning rate to  0.1890625\n",
            "Epoch 8/50\n",
            "98/98 [==============================] - 265s 3s/step - loss: 0.5423 - accuracy: 0.8615\n",
            "epoch  9 : setting learning rate to  0.2125\n",
            "Epoch 9/50\n",
            "98/98 [==============================] - 263s 3s/step - loss: 0.5269 - accuracy: 0.8695\n",
            "epoch  10 : setting learning rate to  0.2359375\n",
            "Epoch 10/50\n",
            "98/98 [==============================] - 262s 3s/step - loss: 0.5158 - accuracy: 0.8782\n",
            "epoch  11 : setting learning rate to  0.259375\n",
            "Epoch 11/50\n",
            "98/98 [==============================] - 264s 3s/step - loss: 0.5081 - accuracy: 0.8833\n",
            "epoch  12 : setting learning rate to  0.2828125\n",
            "Epoch 12/50\n",
            "98/98 [==============================] - 261s 3s/step - loss: 0.4994 - accuracy: 0.8922\n",
            "epoch  13 : setting learning rate to  0.30625\n",
            "Epoch 13/50\n",
            "98/98 [==============================] - 258s 3s/step - loss: 0.5046 - accuracy: 0.8957\n",
            "epoch  14 : setting learning rate to  0.3296875\n",
            "Epoch 14/50\n",
            "98/98 [==============================] - 260s 3s/step - loss: 0.5053 - accuracy: 0.8973\n",
            "epoch  15 : setting learning rate to  0.353125\n",
            "Epoch 15/50\n",
            "98/98 [==============================] - 259s 3s/step - loss: 0.5042 - accuracy: 0.9037\n",
            "epoch  16 : setting learning rate to  0.3765625\n",
            "Epoch 16/50\n",
            "98/98 [==============================] - 259s 3s/step - loss: 0.5182 - accuracy: 0.9020\n",
            "epoch  17 : setting learning rate to  0.4\n",
            "Epoch 17/50\n",
            "98/98 [==============================] - 259s 3s/step - loss: 0.5124 - accuracy: 0.9092\n",
            "epoch  18 : setting learning rate to  0.38823529411764707\n",
            "Epoch 18/50\n",
            "98/98 [==============================] - 259s 3s/step - loss: 0.5104 - accuracy: 0.9126\n",
            "epoch  19 : setting learning rate to  0.3764705882352941\n",
            "Epoch 19/50\n",
            "98/98 [==============================] - 258s 3s/step - loss: 0.4928 - accuracy: 0.9211\n",
            "epoch  20 : setting learning rate to  0.3647058823529412\n",
            "Epoch 20/50\n",
            "98/98 [==============================] - 259s 3s/step - loss: 0.4704 - accuracy: 0.9303\n",
            "epoch  21 : setting learning rate to  0.35294117647058826\n",
            "Epoch 21/50\n",
            "98/98 [==============================] - 258s 3s/step - loss: 0.4617 - accuracy: 0.9339\n",
            "epoch  22 : setting learning rate to  0.3411764705882353\n",
            "Epoch 22/50\n",
            "98/98 [==============================] - 261s 3s/step - loss: 0.4464 - accuracy: 0.9379\n",
            "epoch  23 : setting learning rate to  0.3294117647058824\n",
            "Epoch 23/50\n",
            "98/98 [==============================] - 260s 3s/step - loss: 0.4366 - accuracy: 0.9417\n",
            "epoch  24 : setting learning rate to  0.31764705882352945\n",
            "Epoch 24/50\n",
            "98/98 [==============================] - 260s 3s/step - loss: 0.4296 - accuracy: 0.9436\n",
            "epoch  25 : setting learning rate to  0.3058823529411765\n",
            "Epoch 25/50\n",
            "98/98 [==============================] - 259s 3s/step - loss: 0.4117 - accuracy: 0.9491\n",
            "epoch  26 : setting learning rate to  0.29411764705882354\n",
            "Epoch 26/50\n",
            "98/98 [==============================] - 259s 3s/step - loss: 0.4006 - accuracy: 0.9508\n",
            "epoch  27 : setting learning rate to  0.2823529411764706\n",
            "Epoch 27/50\n",
            "98/98 [==============================] - 260s 3s/step - loss: 0.3932 - accuracy: 0.9531\n",
            "epoch  28 : setting learning rate to  0.2705882352941177\n",
            "Epoch 28/50\n",
            "98/98 [==============================] - 261s 3s/step - loss: 0.3769 - accuracy: 0.9593\n",
            "epoch  29 : setting learning rate to  0.25882352941176473\n",
            "Epoch 29/50\n",
            "98/98 [==============================] - 258s 3s/step - loss: 0.3657 - accuracy: 0.9605\n",
            "epoch  30 : setting learning rate to  0.24705882352941178\n",
            "Epoch 30/50\n",
            "98/98 [==============================] - 259s 3s/step - loss: 0.3543 - accuracy: 0.9636\n",
            "epoch  31 : setting learning rate to  0.23529411764705885\n",
            "Epoch 31/50\n",
            "98/98 [==============================] - 259s 3s/step - loss: 0.3370 - accuracy: 0.9684\n",
            "epoch  32 : setting learning rate to  0.22352941176470592\n",
            "Epoch 32/50\n",
            "98/98 [==============================] - 259s 3s/step - loss: 0.3243 - accuracy: 0.9706\n",
            "epoch  33 : setting learning rate to  0.21176470588235297\n",
            "Epoch 33/50\n",
            "98/98 [==============================] - 258s 3s/step - loss: 0.3147 - accuracy: 0.9725\n",
            "epoch  34 : setting learning rate to  0.2\n",
            "Epoch 34/50\n",
            "98/98 [==============================] - 258s 3s/step - loss: 0.3115 - accuracy: 0.9722\n",
            "epoch  35 : setting learning rate to  0.18823529411764708\n",
            "Epoch 35/50\n",
            "98/98 [==============================] - 259s 3s/step - loss: 0.2948 - accuracy: 0.9759\n",
            "epoch  36 : setting learning rate to  0.17647058823529416\n",
            "Epoch 36/50\n",
            "98/98 [==============================] - 259s 3s/step - loss: 0.2826 - accuracy: 0.9783\n",
            "epoch  37 : setting learning rate to  0.1647058823529412\n",
            "Epoch 37/50\n",
            "98/98 [==============================] - 258s 3s/step - loss: 0.2668 - accuracy: 0.9817\n",
            "epoch  38 : setting learning rate to  0.15294117647058825\n",
            "Epoch 38/50\n",
            "98/98 [==============================] - 260s 3s/step - loss: 0.2556 - accuracy: 0.9831\n",
            "epoch  39 : setting learning rate to  0.14117647058823535\n",
            "Epoch 39/50\n",
            "98/98 [==============================] - 259s 3s/step - loss: 0.2463 - accuracy: 0.9850\n",
            "epoch  40 : setting learning rate to  0.1294117647058824\n",
            "Epoch 40/50\n",
            "98/98 [==============================] - 259s 3s/step - loss: 0.2300 - accuracy: 0.9884\n",
            "epoch  41 : setting learning rate to  0.11764705882352944\n",
            "Epoch 41/50\n",
            "98/98 [==============================] - 261s 3s/step - loss: 0.2191 - accuracy: 0.9903\n",
            "epoch  42 : setting learning rate to  0.10588235294117648\n",
            "Epoch 42/50\n",
            "98/98 [==============================] - 261s 3s/step - loss: 0.2064 - accuracy: 0.9921\n",
            "epoch  43 : setting learning rate to  0.09411764705882353\n",
            "Epoch 43/50\n",
            "98/98 [==============================] - 260s 3s/step - loss: 0.1963 - accuracy: 0.9938\n",
            "epoch  44 : setting learning rate to  0.08235294117647063\n",
            "Epoch 44/50\n",
            "98/98 [==============================] - 259s 3s/step - loss: 0.1879 - accuracy: 0.9947\n",
            "epoch  45 : setting learning rate to  0.07058823529411767\n",
            "Epoch 45/50\n",
            "98/98 [==============================] - 258s 3s/step - loss: 0.1782 - accuracy: 0.9964\n",
            "epoch  46 : setting learning rate to  0.05882352941176472\n",
            "Epoch 46/50\n",
            "98/98 [==============================] - 262s 3s/step - loss: 0.1718 - accuracy: 0.9971\n",
            "epoch  47 : setting learning rate to  0.04705882352941182\n",
            "Epoch 47/50\n",
            "98/98 [==============================] - 264s 3s/step - loss: 0.1657 - accuracy: 0.9975\n",
            "epoch  48 : setting learning rate to  0.035294117647058865\n",
            "Epoch 48/50\n",
            "98/98 [==============================] - 264s 3s/step - loss: 0.1615 - accuracy: 0.9983\n",
            "epoch  49 : setting learning rate to  0.02352941176470591\n",
            "Epoch 49/50\n",
            "98/98 [==============================] - 265s 3s/step - loss: 0.1576 - accuracy: 0.9986\n",
            "epoch  50 : setting learning rate to  0.011764705882352955\n",
            "Epoch 50/50\n",
            "98/98 [==============================] - 263s 3s/step - loss: 0.1565 - accuracy: 0.9984\n",
            "is_training False\n",
            "inside validation cycle\n",
            "===============\n",
            "\n",
            "is_training False\n",
            "inside validation cycle\n",
            "===============\n",
            "\n",
            "20/20 [==============================] - 3s 129ms/step - loss: 0.4806 - accuracy: 0.9208\n",
            "val accuracy score at the end of training model type  3 [0.4806245148181915, 0.9208008]\n",
            "=========================================\n",
            "\n",
            "is_training True\n",
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 32, 32, 64)   1728        input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 32, 32, 64)   256         conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 32, 32, 64)   0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 32, 32, 128)  73728       activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 32, 32, 128)  512         conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 32, 32, 128)  0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2D)  (None, 16, 16, 128)  0           activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 16, 16, 128)  147456      max_pooling2d_9[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 16, 16, 128)  512         conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 16, 16, 128)  0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 16, 16, 128)  147456      activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 16, 16, 128)  512         conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 16, 16, 128)  0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_6 (TensorFlowOp [(None, 16, 16, 128) 0           max_pooling2d_9[0][0]            \n",
            "                                                                 activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 16, 16, 256)  294912      tf_op_layer_add_6[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 16, 16, 256)  1024        conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 16, 16, 256)  0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling2D) (None, 8, 8, 256)    0           activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_7 (Lambda)               (None, 8, 8, 256)    0           max_pooling2d_10[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 8, 8, 512)    1179648     lambda_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 8, 8, 512)    2048        conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 8, 8, 512)    0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling2D) (None, 4, 4, 512)    0           activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 4, 4, 512)    2359296     max_pooling2d_11[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 4, 4, 512)    2048        conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 4, 4, 512)    0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 4, 4, 512)    2359296     activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 4, 4, 512)    2048        conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 4, 4, 512)    0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_7 (TensorFlowOp [(None, 4, 4, 512)]  0           max_pooling2d_11[0][0]           \n",
            "                                                                 activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling2d_3 (GlobalM (None, 512)          0           tf_op_layer_add_7[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 10)           5120        global_max_pooling2d_3[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "lambda_8 (Lambda)               (None, 10)           0           dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 10)           0           lambda_8[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 6,577,600\n",
            "Trainable params: 6,573,120\n",
            "Non-trainable params: 4,480\n",
            "__________________________________________________________________________________________________\n",
            "Model will be trained with distortion after Res Blk 2\n",
            "=======================\n",
            "Train for 98.0 steps\n",
            "epoch  1 : setting learning rate to  0.025\n",
            "Epoch 1/50\n",
            "is_training True\n",
            "is_training True\n",
            "98/98 [==============================] - 155s 2s/step - loss: 1.5900 - accuracy: 0.4593\n",
            "epoch  2 : setting learning rate to  0.0484375\n",
            "Epoch 2/50\n",
            "98/98 [==============================] - 152s 2s/step - loss: 1.0146 - accuracy: 0.6722\n",
            "epoch  3 : setting learning rate to  0.071875\n",
            "Epoch 3/50\n",
            "98/98 [==============================] - 152s 2s/step - loss: 0.7682 - accuracy: 0.7689\n",
            "epoch  4 : setting learning rate to  0.0953125\n",
            "Epoch 4/50\n",
            "98/98 [==============================] - 152s 2s/step - loss: 0.6407 - accuracy: 0.8155\n",
            "epoch  5 : setting learning rate to  0.11875\n",
            "Epoch 5/50\n",
            "98/98 [==============================] - 151s 2s/step - loss: 0.5683 - accuracy: 0.8431\n",
            "epoch  6 : setting learning rate to  0.1421875\n",
            "Epoch 6/50\n",
            "98/98 [==============================] - 152s 2s/step - loss: 0.5186 - accuracy: 0.8637\n",
            "epoch  7 : setting learning rate to  0.165625\n",
            "Epoch 7/50\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.4830 - accuracy: 0.8799\n",
            "epoch  8 : setting learning rate to  0.1890625\n",
            "Epoch 8/50\n",
            "98/98 [==============================] - 152s 2s/step - loss: 0.4535 - accuracy: 0.8938\n",
            "epoch  9 : setting learning rate to  0.2125\n",
            "Epoch 9/50\n",
            "98/98 [==============================] - 152s 2s/step - loss: 0.4319 - accuracy: 0.9049\n",
            "epoch  10 : setting learning rate to  0.2359375\n",
            "Epoch 10/50\n",
            "98/98 [==============================] - 151s 2s/step - loss: 0.4228 - accuracy: 0.9117\n",
            "epoch  11 : setting learning rate to  0.259375\n",
            "Epoch 11/50\n",
            "98/98 [==============================] - 152s 2s/step - loss: 0.4150 - accuracy: 0.9200\n",
            "epoch  12 : setting learning rate to  0.2828125\n",
            "Epoch 12/50\n",
            "98/98 [==============================] - 152s 2s/step - loss: 0.4100 - accuracy: 0.9270\n",
            "epoch  13 : setting learning rate to  0.30625\n",
            "Epoch 13/50\n",
            "98/98 [==============================] - 153s 2s/step - loss: 0.4138 - accuracy: 0.9304\n",
            "epoch  14 : setting learning rate to  0.3296875\n",
            "Epoch 14/50\n",
            "98/98 [==============================] - 151s 2s/step - loss: 0.4247 - accuracy: 0.9316\n",
            "epoch  15 : setting learning rate to  0.353125\n",
            "Epoch 15/50\n",
            "98/98 [==============================] - 152s 2s/step - loss: 0.4238 - accuracy: 0.9367\n",
            "epoch  16 : setting learning rate to  0.3765625\n",
            "Epoch 16/50\n",
            "98/98 [==============================] - 152s 2s/step - loss: 0.4391 - accuracy: 0.9359\n",
            "epoch  17 : setting learning rate to  0.4\n",
            "Epoch 17/50\n",
            "98/98 [==============================] - 151s 2s/step - loss: 0.4532 - accuracy: 0.9377\n",
            "epoch  18 : setting learning rate to  0.38823529411764707\n",
            "Epoch 18/50\n",
            "98/98 [==============================] - 151s 2s/step - loss: 0.4272 - accuracy: 0.9487\n",
            "epoch  19 : setting learning rate to  0.3764705882352941\n",
            "Epoch 19/50\n",
            "98/98 [==============================] - 151s 2s/step - loss: 0.4178 - accuracy: 0.9548\n",
            "epoch  20 : setting learning rate to  0.3647058823529412\n",
            "Epoch 20/50\n",
            "98/98 [==============================] - 152s 2s/step - loss: 0.3983 - accuracy: 0.9614\n",
            "epoch  21 : setting learning rate to  0.35294117647058826\n",
            "Epoch 21/50\n",
            "98/98 [==============================] - 152s 2s/step - loss: 0.3878 - accuracy: 0.9639\n",
            "epoch  22 : setting learning rate to  0.3411764705882353\n",
            "Epoch 22/50\n",
            "98/98 [==============================] - 152s 2s/step - loss: 0.3781 - accuracy: 0.9668\n",
            "epoch  23 : setting learning rate to  0.3294117647058824\n",
            "Epoch 23/50\n",
            "98/98 [==============================] - 153s 2s/step - loss: 0.3587 - accuracy: 0.9713\n",
            "epoch  24 : setting learning rate to  0.31764705882352945\n",
            "Epoch 24/50\n",
            "98/98 [==============================] - 152s 2s/step - loss: 0.3493 - accuracy: 0.9726\n",
            "epoch  25 : setting learning rate to  0.3058823529411765\n",
            "Epoch 25/50\n",
            "98/98 [==============================] - 153s 2s/step - loss: 0.3350 - accuracy: 0.9757\n",
            "epoch  26 : setting learning rate to  0.29411764705882354\n",
            "Epoch 26/50\n",
            "98/98 [==============================] - 152s 2s/step - loss: 0.3239 - accuracy: 0.9763\n",
            "epoch  27 : setting learning rate to  0.2823529411764706\n",
            "Epoch 27/50\n",
            "98/98 [==============================] - 152s 2s/step - loss: 0.3222 - accuracy: 0.9765\n",
            "epoch  28 : setting learning rate to  0.2705882352941177\n",
            "Epoch 28/50\n",
            "98/98 [==============================] - 151s 2s/step - loss: 0.3047 - accuracy: 0.9804\n",
            "epoch  29 : setting learning rate to  0.25882352941176473\n",
            "Epoch 29/50\n",
            "98/98 [==============================] - 151s 2s/step - loss: 0.2988 - accuracy: 0.9805\n",
            "epoch  30 : setting learning rate to  0.24705882352941178\n",
            "Epoch 30/50\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.2825 - accuracy: 0.9844\n",
            "epoch  31 : setting learning rate to  0.23529411764705885\n",
            "Epoch 31/50\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.2680 - accuracy: 0.9860\n",
            "epoch  32 : setting learning rate to  0.22352941176470592\n",
            "Epoch 32/50\n",
            "98/98 [==============================] - 151s 2s/step - loss: 0.2635 - accuracy: 0.9855\n",
            "epoch  33 : setting learning rate to  0.21176470588235297\n",
            "Epoch 33/50\n",
            "98/98 [==============================] - 151s 2s/step - loss: 0.2471 - accuracy: 0.9884\n",
            "epoch  34 : setting learning rate to  0.2\n",
            "Epoch 34/50\n",
            "98/98 [==============================] - 151s 2s/step - loss: 0.2351 - accuracy: 0.9899\n",
            "epoch  35 : setting learning rate to  0.18823529411764708\n",
            "Epoch 35/50\n",
            "98/98 [==============================] - 152s 2s/step - loss: 0.2278 - accuracy: 0.9898\n",
            "epoch  36 : setting learning rate to  0.17647058823529416\n",
            "Epoch 36/50\n",
            "98/98 [==============================] - 151s 2s/step - loss: 0.2120 - accuracy: 0.9925\n",
            "epoch  37 : setting learning rate to  0.1647058823529412\n",
            "Epoch 37/50\n",
            "98/98 [==============================] - 152s 2s/step - loss: 0.2000 - accuracy: 0.9940\n",
            "epoch  38 : setting learning rate to  0.15294117647058825\n",
            "Epoch 38/50\n",
            "98/98 [==============================] - 151s 2s/step - loss: 0.1875 - accuracy: 0.9953\n",
            "epoch  39 : setting learning rate to  0.14117647058823535\n",
            "Epoch 39/50\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.1759 - accuracy: 0.9969\n",
            "epoch  40 : setting learning rate to  0.1294117647058824\n",
            "Epoch 40/50\n",
            "98/98 [==============================] - 151s 2s/step - loss: 0.1643 - accuracy: 0.9978\n",
            "epoch  41 : setting learning rate to  0.11764705882352944\n",
            "Epoch 41/50\n",
            "98/98 [==============================] - 151s 2s/step - loss: 0.1544 - accuracy: 0.9987\n",
            "epoch  42 : setting learning rate to  0.10588235294117648\n",
            "Epoch 42/50\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.1456 - accuracy: 0.9991\n",
            "epoch  43 : setting learning rate to  0.09411764705882353\n",
            "Epoch 43/50\n",
            "98/98 [==============================] - 150s 2s/step - loss: 0.1384 - accuracy: 0.9995\n",
            "epoch  44 : setting learning rate to  0.08235294117647063\n",
            "Epoch 44/50\n",
            "98/98 [==============================] - 151s 2s/step - loss: 0.1327 - accuracy: 0.9995\n",
            "epoch  45 : setting learning rate to  0.07058823529411767\n",
            "Epoch 45/50\n",
            "98/98 [==============================] - 153s 2s/step - loss: 0.1278 - accuracy: 0.9995\n",
            "epoch  46 : setting learning rate to  0.05882352941176472\n",
            "Epoch 46/50\n",
            "98/98 [==============================] - 151s 2s/step - loss: 0.1236 - accuracy: 0.9997\n",
            "epoch  47 : setting learning rate to  0.04705882352941182\n",
            "Epoch 47/50\n",
            "98/98 [==============================] - 153s 2s/step - loss: 0.1207 - accuracy: 0.9994\n",
            "epoch  48 : setting learning rate to  0.035294117647058865\n",
            "Epoch 48/50\n",
            "98/98 [==============================] - 152s 2s/step - loss: 0.1178 - accuracy: 0.9998\n",
            "epoch  49 : setting learning rate to  0.02352941176470591\n",
            "Epoch 49/50\n",
            "98/98 [==============================] - 152s 2s/step - loss: 0.1159 - accuracy: 0.9997\n",
            "epoch  50 : setting learning rate to  0.011764705882352955\n",
            "Epoch 50/50\n",
            "98/98 [==============================] - 152s 2s/step - loss: 0.1149 - accuracy: 0.9998\n",
            "is_training False\n",
            "inside validation cycle\n",
            "===============\n",
            "\n",
            "20/20 [==============================] - 3s 127ms/step - loss: 0.4399 - accuracy: 0.9161\n",
            "val accuracy score at the end of training model type  2 [0.43987464904785156, 0.91611326]\n",
            "=========================================\n",
            "\n",
            "is_training True\n",
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 32, 32, 64)   1728        input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 32, 32, 64)   256         conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 32, 32, 64)   0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 32, 32, 128)  73728       activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 32, 32, 128)  512         conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 32, 32, 128)  0           batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling2D) (None, 16, 16, 128)  0           activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 16, 16, 128)  147456      max_pooling2d_12[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 16, 16, 128)  512         conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 16, 16, 128)  0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 16, 16, 128)  147456      activation_38[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 16, 16, 128)  512         conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 16, 16, 128)  0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_8 (TensorFlowOp [(None, 16, 16, 128) 0           max_pooling2d_12[0][0]           \n",
            "                                                                 activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_9 (Lambda)               (None, 16, 16, 128)  0           tf_op_layer_add_8[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 16, 16, 256)  294912      lambda_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 16, 16, 256)  1024        conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 16, 16, 256)  0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling2D) (None, 8, 8, 256)    0           activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 8, 8, 512)    1179648     max_pooling2d_13[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 8, 8, 512)    2048        conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 8, 8, 512)    0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling2D) (None, 4, 4, 512)    0           activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 4, 4, 512)    2359296     max_pooling2d_14[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 4, 4, 512)    2048        conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 4, 4, 512)    0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 4, 4, 512)    2359296     activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 4, 4, 512)    2048        conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 4, 4, 512)    0           batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_9 (TensorFlowOp [(None, 4, 4, 512)]  0           max_pooling2d_14[0][0]           \n",
            "                                                                 activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling2d_4 (GlobalM (None, 512)          0           tf_op_layer_add_9[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 10)           5120        global_max_pooling2d_4[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "lambda_10 (Lambda)              (None, 10)           0           dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 10)           0           lambda_10[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 6,577,600\n",
            "Trainable params: 6,573,120\n",
            "Non-trainable params: 4,480\n",
            "__________________________________________________________________________________________________\n",
            "Model will be trained with distortion after Res Blk 1\n",
            "=======================\n",
            "Train for 98.0 steps\n",
            "epoch  1 : setting learning rate to  0.025\n",
            "Epoch 1/50\n",
            "is_training True\n",
            "is_training True\n",
            "98/98 [==============================] - 160s 2s/step - loss: 1.5988 - accuracy: 0.4565\n",
            "epoch  2 : setting learning rate to  0.0484375\n",
            "Epoch 2/50\n",
            "98/98 [==============================] - 157s 2s/step - loss: 1.0153 - accuracy: 0.6754\n",
            "epoch  3 : setting learning rate to  0.071875\n",
            "Epoch 3/50\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.7738 - accuracy: 0.7665\n",
            "epoch  4 : setting learning rate to  0.0953125\n",
            "Epoch 4/50\n",
            "98/98 [==============================] - 160s 2s/step - loss: 0.6568 - accuracy: 0.8098\n",
            "epoch  5 : setting learning rate to  0.11875\n",
            "Epoch 5/50\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.5846 - accuracy: 0.8362\n",
            "epoch  6 : setting learning rate to  0.1421875\n",
            "Epoch 6/50\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.5453 - accuracy: 0.8541\n",
            "epoch  7 : setting learning rate to  0.165625\n",
            "Epoch 7/50\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.5091 - accuracy: 0.8689\n",
            "epoch  8 : setting learning rate to  0.1890625\n",
            "Epoch 8/50\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.4886 - accuracy: 0.8805\n",
            "epoch  9 : setting learning rate to  0.2125\n",
            "Epoch 9/50\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.4768 - accuracy: 0.8887\n",
            "epoch  10 : setting learning rate to  0.2359375\n",
            "Epoch 10/50\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.4630 - accuracy: 0.8968\n",
            "epoch  11 : setting learning rate to  0.259375\n",
            "Epoch 11/50\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.4552 - accuracy: 0.9022\n",
            "epoch  12 : setting learning rate to  0.2828125\n",
            "Epoch 12/50\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.4511 - accuracy: 0.9108\n",
            "epoch  13 : setting learning rate to  0.30625\n",
            "Epoch 13/50\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.4544 - accuracy: 0.9111\n",
            "epoch  14 : setting learning rate to  0.3296875\n",
            "Epoch 14/50\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.4618 - accuracy: 0.9135\n",
            "epoch  15 : setting learning rate to  0.353125\n",
            "Epoch 15/50\n",
            "98/98 [==============================] - 155s 2s/step - loss: 0.4591 - accuracy: 0.9197\n",
            "epoch  16 : setting learning rate to  0.3765625\n",
            "Epoch 16/50\n",
            "98/98 [==============================] - 154s 2s/step - loss: 0.4624 - accuracy: 0.9222\n",
            "epoch  17 : setting learning rate to  0.4\n",
            "Epoch 17/50\n",
            "98/98 [==============================] - 155s 2s/step - loss: 0.4743 - accuracy: 0.9233\n",
            "epoch  18 : setting learning rate to  0.38823529411764707\n",
            "Epoch 18/50\n",
            "98/98 [==============================] - 154s 2s/step - loss: 0.4662 - accuracy: 0.9288\n",
            "epoch  19 : setting learning rate to  0.3764705882352941\n",
            "Epoch 19/50\n",
            "98/98 [==============================] - 155s 2s/step - loss: 0.4462 - accuracy: 0.9377\n",
            "epoch  20 : setting learning rate to  0.3647058823529412\n",
            "Epoch 20/50\n",
            "98/98 [==============================] - 154s 2s/step - loss: 0.4236 - accuracy: 0.9447\n",
            "epoch  21 : setting learning rate to  0.35294117647058826\n",
            "Epoch 21/50\n",
            "98/98 [==============================] - 155s 2s/step - loss: 0.4179 - accuracy: 0.9472\n",
            "epoch  22 : setting learning rate to  0.3411764705882353\n",
            "Epoch 22/50\n",
            "98/98 [==============================] - 155s 2s/step - loss: 0.4051 - accuracy: 0.9509\n",
            "epoch  23 : setting learning rate to  0.3294117647058824\n",
            "Epoch 23/50\n",
            "98/98 [==============================] - 154s 2s/step - loss: 0.3938 - accuracy: 0.9558\n",
            "epoch  24 : setting learning rate to  0.31764705882352945\n",
            "Epoch 24/50\n",
            "98/98 [==============================] - 154s 2s/step - loss: 0.3750 - accuracy: 0.9608\n",
            "epoch  25 : setting learning rate to  0.3058823529411765\n",
            "Epoch 25/50\n",
            "98/98 [==============================] - 155s 2s/step - loss: 0.3686 - accuracy: 0.9611\n",
            "epoch  26 : setting learning rate to  0.29411764705882354\n",
            "Epoch 26/50\n",
            "98/98 [==============================] - 154s 2s/step - loss: 0.3573 - accuracy: 0.9638\n",
            "epoch  27 : setting learning rate to  0.2823529411764706\n",
            "Epoch 27/50\n",
            "98/98 [==============================] - 154s 2s/step - loss: 0.3480 - accuracy: 0.9670\n",
            "epoch  28 : setting learning rate to  0.2705882352941177\n",
            "Epoch 28/50\n",
            "98/98 [==============================] - 152s 2s/step - loss: 0.3346 - accuracy: 0.9696\n",
            "epoch  29 : setting learning rate to  0.25882352941176473\n",
            "Epoch 29/50\n",
            "98/98 [==============================] - 155s 2s/step - loss: 0.3204 - accuracy: 0.9731\n",
            "epoch  30 : setting learning rate to  0.24705882352941178\n",
            "Epoch 30/50\n",
            "98/98 [==============================] - 155s 2s/step - loss: 0.3148 - accuracy: 0.9737\n",
            "epoch  31 : setting learning rate to  0.23529411764705885\n",
            "Epoch 31/50\n",
            "98/98 [==============================] - 155s 2s/step - loss: 0.2984 - accuracy: 0.9776\n",
            "epoch  32 : setting learning rate to  0.22352941176470592\n",
            "Epoch 32/50\n",
            "98/98 [==============================] - 159s 2s/step - loss: 0.2892 - accuracy: 0.9784\n",
            "epoch  33 : setting learning rate to  0.21176470588235297\n",
            "Epoch 33/50\n",
            "98/98 [==============================] - 160s 2s/step - loss: 0.2784 - accuracy: 0.9804\n",
            "epoch  34 : setting learning rate to  0.2\n",
            "Epoch 34/50\n",
            "98/98 [==============================] - 156s 2s/step - loss: 0.2654 - accuracy: 0.9822\n",
            "epoch  35 : setting learning rate to  0.18823529411764708\n",
            "Epoch 35/50\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.2510 - accuracy: 0.9855\n",
            "epoch  36 : setting learning rate to  0.17647058823529416\n",
            "Epoch 36/50\n",
            "98/98 [==============================] - 156s 2s/step - loss: 0.2373 - accuracy: 0.9886\n",
            "epoch  37 : setting learning rate to  0.1647058823529412\n",
            "Epoch 37/50\n",
            "98/98 [==============================] - 156s 2s/step - loss: 0.2255 - accuracy: 0.9892\n",
            "epoch  38 : setting learning rate to  0.15294117647058825\n",
            "Epoch 38/50\n",
            "98/98 [==============================] - 156s 2s/step - loss: 0.2162 - accuracy: 0.9904\n",
            "epoch  39 : setting learning rate to  0.14117647058823535\n",
            "Epoch 39/50\n",
            "95/98 [============================>.] - ETA: 4s - loss: 0.2051 - accuracy: 0.9922"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUhCe02o-sFY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6af09aba-3f09-496f-c86c-e39ac80d9b1a"
      },
      "source": [
        "\n",
        "for model_params in [0,1]:\n",
        "  is_training=True\n",
        "  #global_step = tf.train.get_or_create_global_step()\n",
        "  model=model=build_model(model_params)\n",
        "  opt=SGD(lr=0.025,momentum=0.9,nesterov=True)\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,metrics=['accuracy']\n",
        "              )\n",
        "  batch_size=512\n",
        "  \n",
        "  if model_params in [0,4,5]:\n",
        "    train_ds=train_ds1\n",
        "    print('Model will be trained with image augmentation in put layer\\n====================')\n",
        "    if model_params==4:\n",
        "      print('Model will also include distortion after Res Blk 1\\n=======================')\n",
        "    elif model_params==5:\n",
        "      print('Model will also include distortion after Res Blk 2\\n=======================')\n",
        "      \n",
        "  else:\n",
        "    if model_params==1:\n",
        "      print('Model will be trained with distortion after Res Blk 1\\n=======================')\n",
        "    elif model_params==2:\n",
        "      print('Model will be trained with distortion after Res Blk 2\\n=======================')\n",
        "    elif model_params==3:\n",
        "      print('Model will be trained with distortion after Res Blk 1 and Res Blk 2\\n=======================')    \n",
        "    train_ds=train_ds2  \n",
        "  model.fit(train_ds,epochs=EPOCHS, steps_per_epoch=np.ceil(50000/batch_size), \n",
        "          callbacks=[lr_sched],\n",
        "          verbose=1)\n",
        "  is_training=False\n",
        "  score=model.evaluate(test_ds, steps =np.ceil(10000/batch_size), verbose=1)\n",
        "\n",
        "  del(model)\n",
        "  del(train_ds)\n",
        "  \n",
        "  print('val accuracy score at the end of training model type ',model_params, score)\n",
        "  print(\"=========================================\\n\")\n",
        "\n",
        "#validation_data=test_ds, validation_steps=np.ceil(10000/batch_size),"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 32, 32, 64)   1728        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 32, 32, 64)   256         conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 32, 32, 64)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 128)  73728       activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 128)  512         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 128)  0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 16, 16, 128)  0           activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 16, 16, 128)  147456      max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 16, 16, 128)  512         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 16, 16, 128)  0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 16, 16, 128)  147456      activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 16, 16, 128)  512         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 16, 16, 128)  0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add (TensorFlowOpLa [(None, 16, 16, 128) 0           max_pooling2d[0][0]              \n",
            "                                                                 activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 16, 16, 256)  294912      tf_op_layer_add[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 16, 16, 256)  1024        conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 16, 16, 256)  0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 8, 8, 256)    0           activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 8, 8, 512)    1179648     max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 8, 8, 512)    2048        conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 8, 8, 512)    0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 4, 4, 512)    0           activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 4, 4, 512)    2359296     max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 4, 4, 512)    2048        conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 4, 4, 512)    0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 4, 4, 512)    2359296     activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 4, 4, 512)    2048        conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 4, 4, 512)    0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_1 (TensorFlowOp [(None, 4, 4, 512)]  0           max_pooling2d_2[0][0]            \n",
            "                                                                 activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling2d (GlobalMax (None, 512)          0           tf_op_layer_add_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 10)           5120        global_max_pooling2d[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, 10)           0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 10)           0           lambda[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 6,577,600\n",
            "Trainable params: 6,573,120\n",
            "Non-trainable params: 4,480\n",
            "__________________________________________________________________________________________________\n",
            "Model will be trained with image augmentation in put layer\n",
            "====================\n",
            "Train for 98.0 steps\n",
            "epoch  1 : setting learning rate to  0.025\n",
            "Epoch 1/50\n",
            "98/98 [==============================] - 51s 518ms/step - loss: 1.6647 - accuracy: 0.4286\n",
            "epoch  2 : setting learning rate to  0.0484375\n",
            "Epoch 2/50\n",
            "98/98 [==============================] - 40s 412ms/step - loss: 1.1140 - accuracy: 0.6369\n",
            "epoch  3 : setting learning rate to  0.071875\n",
            "Epoch 3/50\n",
            "98/98 [==============================] - 40s 411ms/step - loss: 0.8603 - accuracy: 0.7327\n",
            "epoch  4 : setting learning rate to  0.0953125\n",
            "Epoch 4/50\n",
            "98/98 [==============================] - 40s 411ms/step - loss: 0.7483 - accuracy: 0.7783\n",
            "epoch  5 : setting learning rate to  0.11875\n",
            "Epoch 5/50\n",
            "98/98 [==============================] - 40s 411ms/step - loss: 0.6827 - accuracy: 0.8030\n",
            "epoch  6 : setting learning rate to  0.1421875\n",
            "Epoch 6/50\n",
            "98/98 [==============================] - 40s 411ms/step - loss: 0.6396 - accuracy: 0.8211\n",
            "epoch  7 : setting learning rate to  0.165625\n",
            "Epoch 7/50\n",
            "98/98 [==============================] - 40s 411ms/step - loss: 0.6126 - accuracy: 0.8336\n",
            "epoch  8 : setting learning rate to  0.1890625\n",
            "Epoch 8/50\n",
            "98/98 [==============================] - 40s 411ms/step - loss: 0.5883 - accuracy: 0.8486\n",
            "epoch  9 : setting learning rate to  0.2125\n",
            "Epoch 9/50\n",
            "98/98 [==============================] - 40s 411ms/step - loss: 0.5747 - accuracy: 0.8547\n",
            "epoch  10 : setting learning rate to  0.2359375\n",
            "Epoch 10/50\n",
            "98/98 [==============================] - 40s 412ms/step - loss: 0.5739 - accuracy: 0.8594\n",
            "epoch  11 : setting learning rate to  0.259375\n",
            "Epoch 11/50\n",
            "98/98 [==============================] - 40s 411ms/step - loss: 0.5690 - accuracy: 0.8657\n",
            "epoch  12 : setting learning rate to  0.2828125\n",
            "Epoch 12/50\n",
            "98/98 [==============================] - 40s 411ms/step - loss: 0.5689 - accuracy: 0.8698\n",
            "epoch  13 : setting learning rate to  0.30625\n",
            "Epoch 13/50\n",
            "98/98 [==============================] - 40s 411ms/step - loss: 0.5723 - accuracy: 0.8745\n",
            "epoch  14 : setting learning rate to  0.3296875\n",
            "Epoch 14/50\n",
            "98/98 [==============================] - 40s 410ms/step - loss: 0.5756 - accuracy: 0.8771\n",
            "epoch  15 : setting learning rate to  0.353125\n",
            "Epoch 15/50\n",
            "98/98 [==============================] - 40s 410ms/step - loss: 0.5751 - accuracy: 0.8822\n",
            "epoch  16 : setting learning rate to  0.3765625\n",
            "Epoch 16/50\n",
            "98/98 [==============================] - 40s 410ms/step - loss: 0.5799 - accuracy: 0.8840\n",
            "epoch  17 : setting learning rate to  0.4\n",
            "Epoch 17/50\n",
            "98/98 [==============================] - 40s 409ms/step - loss: 0.5942 - accuracy: 0.8842\n",
            "epoch  18 : setting learning rate to  0.38823529411764707\n",
            "Epoch 18/50\n",
            "98/98 [==============================] - 40s 410ms/step - loss: 0.5660 - accuracy: 0.8978\n",
            "epoch  19 : setting learning rate to  0.3764705882352941\n",
            "Epoch 19/50\n",
            "98/98 [==============================] - 40s 411ms/step - loss: 0.5556 - accuracy: 0.9022\n",
            "epoch  20 : setting learning rate to  0.3647058823529412\n",
            "Epoch 20/50\n",
            "98/98 [==============================] - 40s 410ms/step - loss: 0.5418 - accuracy: 0.9077\n",
            "epoch  21 : setting learning rate to  0.35294117647058826\n",
            "Epoch 21/50\n",
            "98/98 [==============================] - 40s 410ms/step - loss: 0.5255 - accuracy: 0.9126\n",
            "epoch  22 : setting learning rate to  0.3411764705882353\n",
            "Epoch 22/50\n",
            "98/98 [==============================] - 40s 410ms/step - loss: 0.5168 - accuracy: 0.9157\n",
            "epoch  23 : setting learning rate to  0.3294117647058824\n",
            "Epoch 23/50\n",
            "98/98 [==============================] - 40s 410ms/step - loss: 0.5050 - accuracy: 0.9195\n",
            "epoch  24 : setting learning rate to  0.31764705882352945\n",
            "Epoch 24/50\n",
            "98/98 [==============================] - 40s 410ms/step - loss: 0.4933 - accuracy: 0.9240\n",
            "epoch  25 : setting learning rate to  0.3058823529411765\n",
            "Epoch 25/50\n",
            "98/98 [==============================] - 40s 410ms/step - loss: 0.4814 - accuracy: 0.9269\n",
            "epoch  26 : setting learning rate to  0.29411764705882354\n",
            "Epoch 26/50\n",
            "98/98 [==============================] - 40s 411ms/step - loss: 0.4672 - accuracy: 0.9325\n",
            "epoch  27 : setting learning rate to  0.2823529411764706\n",
            "Epoch 27/50\n",
            "98/98 [==============================] - 40s 410ms/step - loss: 0.4656 - accuracy: 0.9326\n",
            "epoch  28 : setting learning rate to  0.2705882352941177\n",
            "Epoch 28/50\n",
            "98/98 [==============================] - 40s 410ms/step - loss: 0.4467 - accuracy: 0.9390\n",
            "epoch  29 : setting learning rate to  0.25882352941176473\n",
            "Epoch 29/50\n",
            "98/98 [==============================] - 40s 410ms/step - loss: 0.4358 - accuracy: 0.9414\n",
            "epoch  30 : setting learning rate to  0.24705882352941178\n",
            "Epoch 30/50\n",
            "98/98 [==============================] - 40s 410ms/step - loss: 0.4234 - accuracy: 0.9428\n",
            "epoch  31 : setting learning rate to  0.23529411764705885\n",
            "Epoch 31/50\n",
            "98/98 [==============================] - 40s 410ms/step - loss: 0.4198 - accuracy: 0.9453\n",
            "epoch  32 : setting learning rate to  0.22352941176470592\n",
            "Epoch 32/50\n",
            "98/98 [==============================] - 40s 410ms/step - loss: 0.4072 - accuracy: 0.9479\n",
            "epoch  33 : setting learning rate to  0.21176470588235297\n",
            "Epoch 33/50\n",
            "98/98 [==============================] - 40s 411ms/step - loss: 0.3966 - accuracy: 0.9509\n",
            "epoch  34 : setting learning rate to  0.2\n",
            "Epoch 34/50\n",
            "98/98 [==============================] - 40s 410ms/step - loss: 0.3791 - accuracy: 0.9562\n",
            "epoch  35 : setting learning rate to  0.18823529411764708\n",
            "Epoch 35/50\n",
            "98/98 [==============================] - 40s 410ms/step - loss: 0.3783 - accuracy: 0.9549\n",
            "epoch  36 : setting learning rate to  0.17647058823529416\n",
            "Epoch 36/50\n",
            "98/98 [==============================] - 40s 410ms/step - loss: 0.3599 - accuracy: 0.9609\n",
            "epoch  37 : setting learning rate to  0.1647058823529412\n",
            "Epoch 37/50\n",
            "98/98 [==============================] - 40s 410ms/step - loss: 0.3503 - accuracy: 0.9615\n",
            "epoch  38 : setting learning rate to  0.15294117647058825\n",
            "Epoch 38/50\n",
            "98/98 [==============================] - 40s 410ms/step - loss: 0.3303 - accuracy: 0.9679\n",
            "epoch  39 : setting learning rate to  0.14117647058823535\n",
            "Epoch 39/50\n",
            "98/98 [==============================] - 40s 410ms/step - loss: 0.3167 - accuracy: 0.9705\n",
            "epoch  40 : setting learning rate to  0.1294117647058824\n",
            "Epoch 40/50\n",
            "98/98 [==============================] - 40s 410ms/step - loss: 0.3092 - accuracy: 0.9713\n",
            "epoch  41 : setting learning rate to  0.11764705882352944\n",
            "Epoch 41/50\n",
            "98/98 [==============================] - 40s 411ms/step - loss: 0.2969 - accuracy: 0.9741\n",
            "epoch  42 : setting learning rate to  0.10588235294117648\n",
            "Epoch 42/50\n",
            "98/98 [==============================] - 40s 410ms/step - loss: 0.2837 - accuracy: 0.9773\n",
            "epoch  43 : setting learning rate to  0.09411764705882353\n",
            "Epoch 43/50\n",
            "98/98 [==============================] - 40s 410ms/step - loss: 0.2715 - accuracy: 0.9801\n",
            "epoch  44 : setting learning rate to  0.08235294117647063\n",
            "Epoch 44/50\n",
            "98/98 [==============================] - 40s 410ms/step - loss: 0.2560 - accuracy: 0.9845\n",
            "epoch  45 : setting learning rate to  0.07058823529411767\n",
            "Epoch 45/50\n",
            "98/98 [==============================] - 40s 410ms/step - loss: 0.2450 - accuracy: 0.9863\n",
            "epoch  46 : setting learning rate to  0.05882352941176472\n",
            "Epoch 46/50\n",
            "98/98 [==============================] - 40s 410ms/step - loss: 0.2309 - accuracy: 0.9900\n",
            "epoch  47 : setting learning rate to  0.04705882352941182\n",
            "Epoch 47/50\n",
            "98/98 [==============================] - 40s 410ms/step - loss: 0.2239 - accuracy: 0.9907\n",
            "epoch  48 : setting learning rate to  0.035294117647058865\n",
            "Epoch 48/50\n",
            "98/98 [==============================] - 40s 410ms/step - loss: 0.2162 - accuracy: 0.9924\n",
            "epoch  49 : setting learning rate to  0.02352941176470591\n",
            "Epoch 49/50\n",
            "98/98 [==============================] - 40s 411ms/step - loss: 0.2106 - accuracy: 0.9935\n",
            "epoch  50 : setting learning rate to  0.011764705882352955\n",
            "Epoch 50/50\n",
            "98/98 [==============================] - 40s 410ms/step - loss: 0.2061 - accuracy: 0.9948\n",
            "20/20 [==============================] - 3s 141ms/step - loss: 0.3724 - accuracy: 0.9457\n",
            "val accuracy score at the end of training model type  0 [0.3724463224411011, 0.94570315]\n",
            "=========================================\n",
            "\n",
            "is_training True\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 32, 32, 64)   1728        input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 32, 32, 64)   256         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 32, 32, 64)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 32, 32, 128)  73728       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 32, 32, 128)  512         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 32, 32, 128)  0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 16, 16, 128)  0           activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 16, 16, 128)  147456      max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 16, 16, 128)  512         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 16, 16, 128)  0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 16, 16, 128)  147456      activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 16, 16, 128)  512         conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 16, 16, 128)  0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_2 (TensorFlowOp [(None, 16, 16, 128) 0           max_pooling2d_3[0][0]            \n",
            "                                                                 activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 16, 16, 128)  0           tf_op_layer_add_2[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 16, 16, 256)  294912      lambda_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 16, 16, 256)  1024        conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 16, 16, 256)  0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 8, 8, 256)    0           activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 8, 8, 512)    1179648     max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 8, 8, 512)    2048        conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 8, 8, 512)    0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2D)  (None, 4, 4, 512)    0           activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 4, 4, 512)    2359296     max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 4, 4, 512)    2048        conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 4, 4, 512)    0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 4, 4, 512)    2359296     activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 4, 4, 512)    2048        conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 4, 4, 512)    0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_3 (TensorFlowOp [(None, 4, 4, 512)]  0           max_pooling2d_5[0][0]            \n",
            "                                                                 activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling2d_1 (GlobalM (None, 512)          0           tf_op_layer_add_3[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 10)           5120        global_max_pooling2d_1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "lambda_2 (Lambda)               (None, 10)           0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 10)           0           lambda_2[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 6,577,600\n",
            "Trainable params: 6,573,120\n",
            "Non-trainable params: 4,480\n",
            "__________________________________________________________________________________________________\n",
            "Model will be trained with distortion after Res Blk 1\n",
            "=======================\n",
            "Train for 98.0 steps\n",
            "epoch  1 : setting learning rate to  0.025\n",
            "Epoch 1/50\n",
            "is_training True\n",
            "is_training True\n",
            "98/98 [==============================] - 158s 2s/step - loss: 1.5928 - accuracy: 0.4586\n",
            "epoch  2 : setting learning rate to  0.0484375\n",
            "Epoch 2/50\n",
            "98/98 [==============================] - 154s 2s/step - loss: 1.0073 - accuracy: 0.6774\n",
            "epoch  3 : setting learning rate to  0.071875\n",
            "Epoch 3/50\n",
            "98/98 [==============================] - 154s 2s/step - loss: 0.7823 - accuracy: 0.7603\n",
            "epoch  4 : setting learning rate to  0.0953125\n",
            "Epoch 4/50\n",
            "98/98 [==============================] - 155s 2s/step - loss: 0.6622 - accuracy: 0.8089\n",
            "epoch  5 : setting learning rate to  0.11875\n",
            "Epoch 5/50\n",
            "98/98 [==============================] - 155s 2s/step - loss: 0.5908 - accuracy: 0.8351\n",
            "epoch  6 : setting learning rate to  0.1421875\n",
            "Epoch 6/50\n",
            "98/98 [==============================] - 155s 2s/step - loss: 0.5463 - accuracy: 0.8537\n",
            "epoch  7 : setting learning rate to  0.165625\n",
            "Epoch 7/50\n",
            "98/98 [==============================] - 154s 2s/step - loss: 0.5127 - accuracy: 0.8680\n",
            "epoch  8 : setting learning rate to  0.1890625\n",
            "Epoch 8/50\n",
            "98/98 [==============================] - 154s 2s/step - loss: 0.4955 - accuracy: 0.8797\n",
            "epoch  9 : setting learning rate to  0.2125\n",
            "Epoch 9/50\n",
            "98/98 [==============================] - 151s 2s/step - loss: 0.4780 - accuracy: 0.8875\n",
            "epoch  10 : setting learning rate to  0.2359375\n",
            "Epoch 10/50\n",
            "98/98 [==============================] - 152s 2s/step - loss: 0.4645 - accuracy: 0.8954\n",
            "epoch  11 : setting learning rate to  0.259375\n",
            "Epoch 11/50\n",
            "98/98 [==============================] - 152s 2s/step - loss: 0.4574 - accuracy: 0.9026\n",
            "epoch  12 : setting learning rate to  0.2828125\n",
            "Epoch 12/50\n",
            "98/98 [==============================] - 151s 2s/step - loss: 0.4562 - accuracy: 0.9080\n",
            "epoch  13 : setting learning rate to  0.30625\n",
            "Epoch 13/50\n",
            "98/98 [==============================] - 152s 2s/step - loss: 0.4564 - accuracy: 0.9114\n",
            "epoch  14 : setting learning rate to  0.3296875\n",
            "Epoch 14/50\n",
            "98/98 [==============================] - 152s 2s/step - loss: 0.4550 - accuracy: 0.9162\n",
            "epoch  15 : setting learning rate to  0.353125\n",
            "Epoch 15/50\n",
            "98/98 [==============================] - 153s 2s/step - loss: 0.4692 - accuracy: 0.9152\n",
            "epoch  16 : setting learning rate to  0.3765625\n",
            "Epoch 16/50\n",
            "98/98 [==============================] - 153s 2s/step - loss: 0.4785 - accuracy: 0.9176\n",
            "epoch  17 : setting learning rate to  0.4\n",
            "Epoch 17/50\n",
            "98/98 [==============================] - 153s 2s/step - loss: 0.4832 - accuracy: 0.9196\n",
            "epoch  18 : setting learning rate to  0.38823529411764707\n",
            "Epoch 18/50\n",
            "98/98 [==============================] - 153s 2s/step - loss: 0.4603 - accuracy: 0.9314\n",
            "epoch  19 : setting learning rate to  0.3764705882352941\n",
            "Epoch 19/50\n",
            "98/98 [==============================] - 154s 2s/step - loss: 0.4469 - accuracy: 0.9369\n",
            "epoch  20 : setting learning rate to  0.3647058823529412\n",
            "Epoch 20/50\n",
            "98/98 [==============================] - 153s 2s/step - loss: 0.4329 - accuracy: 0.9423\n",
            "epoch  21 : setting learning rate to  0.35294117647058826\n",
            "Epoch 21/50\n",
            "98/98 [==============================] - 153s 2s/step - loss: 0.4171 - accuracy: 0.9475\n",
            "epoch  22 : setting learning rate to  0.3411764705882353\n",
            "Epoch 22/50\n",
            "98/98 [==============================] - 153s 2s/step - loss: 0.4019 - accuracy: 0.9526\n",
            "epoch  23 : setting learning rate to  0.3294117647058824\n",
            "Epoch 23/50\n",
            "98/98 [==============================] - 153s 2s/step - loss: 0.3957 - accuracy: 0.9536\n",
            "epoch  24 : setting learning rate to  0.31764705882352945\n",
            "Epoch 24/50\n",
            "98/98 [==============================] - 153s 2s/step - loss: 0.3863 - accuracy: 0.9580\n",
            "epoch  25 : setting learning rate to  0.3058823529411765\n",
            "Epoch 25/50\n",
            "98/98 [==============================] - 154s 2s/step - loss: 0.3738 - accuracy: 0.9604\n",
            "epoch  26 : setting learning rate to  0.29411764705882354\n",
            "Epoch 26/50\n",
            "98/98 [==============================] - 153s 2s/step - loss: 0.3533 - accuracy: 0.9667\n",
            "epoch  27 : setting learning rate to  0.2823529411764706\n",
            "Epoch 27/50\n",
            "98/98 [==============================] - 153s 2s/step - loss: 0.3439 - accuracy: 0.9676\n",
            "epoch  28 : setting learning rate to  0.2705882352941177\n",
            "Epoch 28/50\n",
            "98/98 [==============================] - 153s 2s/step - loss: 0.3363 - accuracy: 0.9692\n",
            "epoch  29 : setting learning rate to  0.25882352941176473\n",
            "Epoch 29/50\n",
            "98/98 [==============================] - 153s 2s/step - loss: 0.3208 - accuracy: 0.9726\n",
            "epoch  30 : setting learning rate to  0.24705882352941178\n",
            "Epoch 30/50\n",
            "98/98 [==============================] - 153s 2s/step - loss: 0.3093 - accuracy: 0.9742\n",
            "epoch  31 : setting learning rate to  0.23529411764705885\n",
            "Epoch 31/50\n",
            "98/98 [==============================] - 153s 2s/step - loss: 0.2938 - accuracy: 0.9786\n",
            "epoch  32 : setting learning rate to  0.22352941176470592\n",
            "Epoch 32/50\n",
            "98/98 [==============================] - 153s 2s/step - loss: 0.2906 - accuracy: 0.9774\n",
            "epoch  33 : setting learning rate to  0.21176470588235297\n",
            "Epoch 33/50\n",
            "98/98 [==============================] - 153s 2s/step - loss: 0.2795 - accuracy: 0.9800\n",
            "epoch  34 : setting learning rate to  0.2\n",
            "Epoch 34/50\n",
            "98/98 [==============================] - 152s 2s/step - loss: 0.2679 - accuracy: 0.9821\n",
            "epoch  35 : setting learning rate to  0.18823529411764708\n",
            "Epoch 35/50\n",
            "98/98 [==============================] - 153s 2s/step - loss: 0.2549 - accuracy: 0.9844\n",
            "epoch  36 : setting learning rate to  0.17647058823529416\n",
            "Epoch 36/50\n",
            "98/98 [==============================] - 153s 2s/step - loss: 0.2419 - accuracy: 0.9865\n",
            "epoch  37 : setting learning rate to  0.1647058823529412\n",
            "Epoch 37/50\n",
            "98/98 [==============================] - 154s 2s/step - loss: 0.2292 - accuracy: 0.9890\n",
            "epoch  38 : setting learning rate to  0.15294117647058825\n",
            "Epoch 38/50\n",
            "98/98 [==============================] - 152s 2s/step - loss: 0.2148 - accuracy: 0.9912\n",
            "epoch  39 : setting learning rate to  0.14117647058823535\n",
            "Epoch 39/50\n",
            "98/98 [==============================] - 151s 2s/step - loss: 0.2037 - accuracy: 0.9929\n",
            "epoch  40 : setting learning rate to  0.1294117647058824\n",
            "Epoch 40/50\n",
            "98/98 [==============================] - 153s 2s/step - loss: 0.1894 - accuracy: 0.9953\n",
            "epoch  41 : setting learning rate to  0.11764705882352944\n",
            "Epoch 41/50\n",
            "98/98 [==============================] - 153s 2s/step - loss: 0.1828 - accuracy: 0.9945\n",
            "epoch  42 : setting learning rate to  0.10588235294117648\n",
            "Epoch 42/50\n",
            "98/98 [==============================] - 152s 2s/step - loss: 0.1704 - accuracy: 0.9971\n",
            "epoch  43 : setting learning rate to  0.09411764705882353\n",
            "Epoch 43/50\n",
            "98/98 [==============================] - 152s 2s/step - loss: 0.1618 - accuracy: 0.9979\n",
            "epoch  44 : setting learning rate to  0.08235294117647063\n",
            "Epoch 44/50\n",
            "98/98 [==============================] - 154s 2s/step - loss: 0.1539 - accuracy: 0.9986\n",
            "epoch  45 : setting learning rate to  0.07058823529411767\n",
            "Epoch 45/50\n",
            "98/98 [==============================] - 151s 2s/step - loss: 0.1476 - accuracy: 0.9988\n",
            "epoch  46 : setting learning rate to  0.05882352941176472\n",
            "Epoch 46/50\n",
            "98/98 [==============================] - 152s 2s/step - loss: 0.1429 - accuracy: 0.9989\n",
            "epoch  47 : setting learning rate to  0.04705882352941182\n",
            "Epoch 47/50\n",
            "98/98 [==============================] - 151s 2s/step - loss: 0.1385 - accuracy: 0.9992\n",
            "epoch  48 : setting learning rate to  0.035294117647058865\n",
            "Epoch 48/50\n",
            "98/98 [==============================] - 152s 2s/step - loss: 0.1353 - accuracy: 0.9993\n",
            "epoch  49 : setting learning rate to  0.02352941176470591\n",
            "Epoch 49/50\n",
            "98/98 [==============================] - 151s 2s/step - loss: 0.1328 - accuracy: 0.9996\n",
            "epoch  50 : setting learning rate to  0.011764705882352955\n",
            "Epoch 50/50\n",
            "98/98 [==============================] - 152s 2s/step - loss: 0.1316 - accuracy: 0.9996\n",
            "is_training False\n",
            "inside validation cycle\n",
            "===============\n",
            "\n",
            "20/20 [==============================] - 3s 134ms/step - loss: 0.4277 - accuracy: 0.9229\n",
            "val accuracy score at the end of training model type  1 [0.4277105778455734, 0.92285156]\n",
            "=========================================\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqkyspQ4R1C6",
        "colab_type": "text"
      },
      "source": [
        "### Summary of Test Results \n",
        "Hyperparameters : Epochs:50, max_lr:0.4, momentum:0.9, L2-wt_decay on Conv Layers :1.25e-4 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzOpzOMqq-gQ",
        "colab_type": "text"
      },
      "source": [
        "| Trial | Augmentation strategy | Train accuracy |Test Accuracy | Hyperparameters |Comments |\n",
        "| :--- | :---: | :---: | :---: | :---: | :--- |\n",
        "| 1 | cutout(flip_lr(pad4_random_crop(inp_image),8)  |99.48  | 94.57 | as above | train-test acc gap 4.91 |\n",
        "| 2 | flip_lr(random_crop(cutout(pad2(res_blk1),4)))  |99.96  | 92.29 | \" | Lower Val accuracy , Tendency to overfit . Perhaps try a more stringent Aug policy ? |\n",
        "| 3 | flip_lr(random_crop(cutout(pad1(res_blk1),2))) | 99.98 | 91.61 | \" | Lowest Val Accuracy, Tendency to overfit. Try a more stringent aug policy? |\n",
        "| 4 | augmentation of trial 2 + augmentation of trial 3  | 99.84  | 92.08 | \" | Lower Val Accuracy, Tendency to overfit. Try a more stringent aug policy? |\n",
        "| 5 | augmentations of trial 1 + augmentation of trial 2  | 98.63  | 94.25 |\"| train-test acc gap 3.96 |\n",
        "| 5 | augmentations of trial 1 + augmentation of trial 3  | 98.73 | 94.34 |\"|  Train-Test acc gap : 3.90 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7EJpOdo09FV",
        "colab_type": "text"
      },
      "source": [
        "**Validation accuracy went down when only Distortion was used in the mid/lower layers (after Res Blk1 and Res Blk2 ) But when used in combination with usual Image augmentation , there seems to be better Regularization and perhaps we could explore such an option. If we were to pursue only Distortion of middle or lower layers (with a good enough size of channels ) , we may need to try more stringent augmentation strategies to overcome the problem of overfitting**"
      ]
    }
  ]
}