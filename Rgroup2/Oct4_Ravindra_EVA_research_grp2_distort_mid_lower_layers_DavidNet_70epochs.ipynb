{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Oct4_Ravindra-EVA_research_grp2_distort_mid_lower_layers_DavidNet-70epochs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ravindrabharathi/Project1/blob/master/Rgroup2/Oct4_Ravindra_EVA_research_grp2_distort_mid_lower_layers_DavidNet_70epochs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynqEXKR8Quk9",
        "colab_type": "text"
      },
      "source": [
        "# Effect of distortions/transformations in mid/lower layers of a Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndxyk_6t6FMY",
        "colab_type": "text"
      },
      "source": [
        "This exercise is to find out of there is any beneficial effect due to  performing distortions/transformation on the activation outputs of mid/lower layers (similar to augmenations on input images ) of a convolutional Neural Network . We will look at gain in test accuracy , regularization effects as seen in smaller gaps of train vs test accuracy , etc. Here we are using a reference model loosely based on the one defined by David Page in his DawnBench submission and explained very well in his series of blog posts (insert link here) . The Model is essentially a 3 block , nine layer modified Resnet Model .\n",
        "\n",
        "We will compare the accuracies achieved using the following methods \n",
        "\n",
        "1. Base Model with image augmnetation on input images alone \n",
        "2. Base Model without input image augmenation but with distortions after first resnet block \n",
        "3. Base Model without input image augmenation but with distortions after second resnet block \n",
        "4. Base Model without input image augmenation but with distortions after first and second resnet blocks \n",
        "5. Base Model with input image augmenation and with distortions after first resnet block \n",
        "6. Base Model without input image augmenation and with distortions after second resnet block \n",
        "\n",
        "Augmentations for input images as well as activation channels will be a combination of pad/random_crop , flip_left_right and cutout "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgLSEQaclSMi",
        "colab_type": "text"
      },
      "source": [
        "### install/import tensorflow_addons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3R6SSn4lb2D",
        "colab_type": "code",
        "outputId": "f9ea0f4c-c230-44e6-844b-8e5723631f27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "!pip install tensorflow==2.0\n",
        "!pip install tensorflow-gpu==2.0\n",
        "!pip install tensorflow_addons\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import math\n",
        "import random"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/0f/7bd55361168bb32796b360ad15a25de6966c9c1beb58a8e30c01c8279862/tensorflow-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (86.3MB)\n",
            "\u001b[K     |████████████████████████████████| 86.3MB 1.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (3.7.1)\n",
            "Collecting tensorboard<2.1.0,>=2.0.0 (from tensorflow==2.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/a6/e8ffa4e2ddb216449d34cfcb825ebb38206bee5c4553d69e7bc8bc2c5d64/tensorboard-2.0.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 39.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.0.8)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (0.1.7)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (3.0.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.11.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.1.0)\n",
            "Collecting tensorflow-estimator<2.1.0,>=2.0.0 (from tensorflow==2.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/00/5e6cdf86190a70d7382d320b2b04e4ff0f8191a37d90a422a2f8ff0705bb/tensorflow_estimator-2.0.0-py2.py3-none-any.whl (449kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 33.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.16.5)\n",
            "Collecting gast==0.2.2 (from tensorflow==2.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (0.33.6)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (0.8.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (0.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==2.0) (41.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (0.16.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0) (2.8.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=f78eb4240ce5c6124caf020fa17f441c0cb3d0ca0c962f2cc00f1b7a58748e87\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "Installing collected packages: tensorboard, tensorflow-estimator, gast, tensorflow\n",
            "  Found existing installation: tensorboard 1.14.0\n",
            "    Uninstalling tensorboard-1.14.0:\n",
            "      Successfully uninstalled tensorboard-1.14.0\n",
            "  Found existing installation: tensorflow-estimator 1.14.0\n",
            "    Uninstalling tensorflow-estimator-1.14.0:\n",
            "      Successfully uninstalled tensorflow-estimator-1.14.0\n",
            "  Found existing installation: gast 0.3.2\n",
            "    Uninstalling gast-0.3.2:\n",
            "      Successfully uninstalled gast-0.3.2\n",
            "  Found existing installation: tensorflow 1.14.0\n",
            "    Uninstalling tensorflow-1.14.0:\n",
            "      Successfully uninstalled tensorflow-1.14.0\n",
            "Successfully installed gast-0.2.2 tensorboard-2.0.0 tensorflow-2.0.0 tensorflow-estimator-2.0.0\n",
            "Collecting tensorflow-gpu==2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/44/47f0722aea081697143fbcf5d2aa60d1aee4aaacb5869aee2b568974777b/tensorflow_gpu-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (380.8MB)\n",
            "\u001b[K     |████████████████████████████████| 380.8MB 84kB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (0.8.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (1.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (1.16.5)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (1.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (1.11.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (0.33.6)\n",
            "Requirement already satisfied: tensorboard<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (2.0.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (2.0.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (0.2.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (1.15.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (3.0.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (0.8.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (3.7.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (0.1.7)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (1.0.8)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (41.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (0.16.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==2.0) (2.8.0)\n",
            "Installing collected packages: tensorflow-gpu\n",
            "Successfully installed tensorflow-gpu-2.0.0\n",
            "Collecting tensorflow_addons\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/b6/574034405ad527eec40ac426081694f50da9766db6b81c2b899041c44ef2/tensorflow_addons-0.5.2-cp36-cp36m-manylinux2010_x86_64.whl (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 6.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-gpu==2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_addons) (2.0.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_addons) (1.12.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (0.8.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (3.7.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (1.16.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (3.0.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (2.0.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (1.0.8)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (0.33.6)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (0.1.7)\n",
            "Requirement already satisfied: tensorboard<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (2.0.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (1.15.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (0.8.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (0.2.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow_addons) (1.11.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0->tensorflow_addons) (41.2.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==2.0.0->tensorflow_addons) (2.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0->tensorflow_addons) (0.16.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0->tensorflow_addons) (3.1.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.5.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFPFRpPDQ2e6",
        "colab_type": "text"
      },
      "source": [
        "### Install tf_utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0yHfmeMv-rI",
        "colab_type": "code",
        "outputId": "7177f002-20fa-4340-c7e8-3757be5540be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "!pip install --upgrade git+https://github.com/ravindrabharathi/tf_utils "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/ravindrabharathi/tf_utils\n",
            "  Cloning https://github.com/ravindrabharathi/tf_utils to /tmp/pip-req-build-ln2mzan5\n",
            "  Running command git clone -q https://github.com/ravindrabharathi/tf_utils /tmp/pip-req-build-ln2mzan5\n",
            "Building wheels for collected packages: tf-utils\n",
            "  Building wheel for tf-utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tf-utils: filename=tf_utils-0.2-cp36-none-any.whl size=8945 sha256=0123ceb366e336b9fda76f3b990ac7717cc0aee96a775950e9d21b26f1dc5917\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5_5_0vlr/wheels/95/af/bb/690b94c65a5aad47a5c39e75f158a2b043448e908c5c121791\n",
            "Successfully built tf-utils\n",
            "Installing collected packages: tf-utils\n",
            "Successfully installed tf-utils-0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VH-5ffTMQ8ms",
        "colab_type": "text"
      },
      "source": [
        "### import the data module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBcelJMpwYoX",
        "colab_type": "code",
        "outputId": "ba3dd1fb-2708-4bfe-c4d6-0cfabd4c6f9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tf_utils.data as ds"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished 'get_cpu_num' in 0.0000 secs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vb0kbJalRCPJ",
        "colab_type": "text"
      },
      "source": [
        "### set batch size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJTA3CO8xOtg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size=512\n",
        "ds.batch_size=batch_size\n",
        "EPOCHS=70"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTroxaquRE2Z",
        "colab_type": "text"
      },
      "source": [
        "### downlaod data and create tf records"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zF3qoDnbykb6",
        "colab_type": "code",
        "outputId": "e99802ee-eed2-4d0d-c59d-53cd4efadfa7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "ds.get_cifar10_and_create_tfrecords()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "05868253d9574fddba365bd4866c31d0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='cifar-10-python.tar.gz', max=170498071, style=ProgressStyle(d…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Finished 'download_file' in 6.7089 secs\n",
            "Finished 'download_cifar10_files' in 6.7096 secs\n",
            "Done\n",
            "Finished 'extract_cifar10_files' in 1.9317 secs\n",
            "Finished '_get_file_names' in 0.0000 secs\n",
            "Generating ./train.tfrecords\n",
            "Finished 'read_pickle_from_file' in 0.1546 secs\n",
            "Finished 'read_pickle_from_file' in 0.1450 secs\n",
            "Finished 'read_pickle_from_file' in 0.1398 secs\n",
            "Finished 'read_pickle_from_file' in 0.1420 secs\n",
            "Finished 'read_pickle_from_file' in 0.1383 secs\n",
            "Finished 'convert_to_tfrecord' in 3.1647 secs\n",
            "Done!\n",
            "Generating ./eval.tfrecords\n",
            "Finished 'read_pickle_from_file' in 0.1434 secs\n",
            "Finished 'convert_to_tfrecord' in 0.6154 secs\n",
            "Done!\n",
            "Finished 'create_tf_records' in 3.7823 secs\n",
            "Finished 'get_cifar10_and_create_tfrecords' in 12.4244 secs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7U-_CmC6RKDF",
        "colab_type": "text"
      },
      "source": [
        "### create train and test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lYOp7UKyoG5",
        "colab_type": "code",
        "outputId": "12ebb916-3a62-486f-e2b7-48d77527ca40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "def aug_fn(image):\n",
        "  return ds.cutout(ds.flip_left_right(ds.random_pad_crop(image)))\n",
        "\n",
        "train_ds1=ds.get_train_ds(batch_size=batch_size,shuffle=True,distort=True,distort_fn=aug_fn)\n",
        "train_ds2=ds.get_train_ds(batch_size=batch_size,shuffle=True,distort=False)\n",
        "\n",
        "test_ds=ds.get_eval_ds(batch_size=batch_size)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "distorting...\n",
            "Finished 'get_tf_dataset_2' in 2.8474 secs\n",
            "Finished 'get_tf_dataset_in_batches' in 2.8480 secs\n",
            "Finished 'get_train_ds' in 2.8481 secs\n",
            "Finished 'get_tf_dataset' in 0.1671 secs\n",
            "Finished 'get_tf_dataset_in_batches' in 0.1672 secs\n",
            "Finished 'get_train_ds' in 0.1676 secs\n",
            "Finished 'get_tf_dataset' in 0.0440 secs\n",
            "Finished 'get_tf_dataset_in_batches' in 0.0442 secs\n",
            "Finished 'get_eval_ds' in 0.0442 secs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7z9t4e5RRbH",
        "colab_type": "text"
      },
      "source": [
        "### import visualization module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHq2g5VjzmDz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tf_utils.visualize as vz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utItaz0RRXft",
        "colab_type": "text"
      },
      "source": [
        "### plot images from train dataset1 , train dataset by default uses image augmenttation of cutout,flip-left-right,random-pad-crop "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvonf0IP0uOX",
        "colab_type": "code",
        "outputId": "44d0fa46-f8d9-4aac-b6ed-3733425825c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "vz.plot_cifar10_files(train_ds1)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyv8HsYOd6TdGDucAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nvxCZouAXIlMU/EJkioJfiEz5/yjOGiO6x/diAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKnsdKUUovds",
        "colab_type": "code",
        "outputId": "34c16fb1-590f-4fd2-8480-1d8dfe3cb8eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "vz.plot_cifar10_files(train_ds2)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnF+IRJHzC5Eocn4hEuX/ASSBwP9ubn65AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyakAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAHo9JREFUeJztnW2M3Nd13p8z7/v+wiUpckmRIk2J\nZmVbUhjBqY3ASRBXdgPIBgrD/mDogxEGRQzUQPpBcIHaAQLECWob/hC4oCshcuHYVmK5Flq1jasa\nUNMAiimXpClRkkmZFLlc7pLLfd+d95MPM2qo1X3ujrjcWcn3+QEEZ++ZO/8z9/8/83KfOeeYu0MI\nkR6ZrXZACLE1KPiFSBQFvxCJouAXIlEU/EIkioJfiERR8AuRKAp+IRJFwS9EouQ2MtnMHgLwDQBZ\nAP/J3b8Su//wyKjvHt8btDn4Lw2zmWz4+Bmjcxr1BrXVG3Vqazab1Jax8GulcTeiNOoxP/h65PL8\ntK0sLwfHK5UqnVMsFamtVqtRGyLPm61/qciPVertpbbIcsAbkXNN/Hfn59kj14BHTnY2x89LNhu+\nhgEgQ67jWi1yfZDnPDc7g+WlpY6uyFsOfjPLAvgLAL8L4DKAn5rZ0+7+Epuze3wvvv2D/x60eZOf\nwP6BgeB47EK6ceMGt83OUtvqyiq1FQqF4Hgxx0+sRV7UYj6urnI/duzYQW0vvHAiOH7+/Gt0zqG7\n76G2yxOXqS1T4NfYwkx4je8+cDed894HHqC2lUggVOcXqe361NXgeL3C17eyGn4BBQAn1wAADG7b\nRm3DQ0PUViqGH3NqaprOWVlaCI7/xVf/lM5Zy0Y+9j8I4Jy7v+buVQDfA/DwBh5PCNFFNhL84wAu\n3fT35faYEOJdwKZv+JnZMTM7YWYnZmdnNvtwQogO2UjwTwC4efduT3vsTbj7cXc/6u5HR0b4dyIh\nRHfZSPD/FMAhM7vLzAoAPg3g6dvjlhBis7nl3X53r5vZ5wH8T7Skvsfd/cXYnGq1gksXya5zRMrZ\nPb47OJ4dHqFzFub4jv75V1+htpjUx+SaXIa/ho6McB+bEYVjYSG8mwsAO3fupLZ7731fcPzkyZN0\nzo1r4R1xAKivLlHb6twKtbEaMZcnuXpQP8XXsVgqUVtPRNiqLoevg6UFfn1UViK7/cYv1EyDr8fi\nDb5z3z8c/kQ8tiN83QPA7t3h7bVSZJ3WsiGd392fAfDMRh5DCLE16Bd+QiSKgl+IRFHwC5EoCn4h\nEkXBL0SibGi3/+1SrVYx8fqloK2nh0sUvSTrbD6SGDMxcYXayhEppxrJfmPZdKOjo3TOMsmyA4C+\nvj5qG41IhLU6z7QbIcklscerl7mPI308eWpmist2u+7cF/Zj5xid09/fQ221coXaqs4l03IlLL/V\n6/w8V6o86acaSQgqlXjST+8Qv0asVg4/XpbLij3F8LXIMgSD9+34nkKIXykU/EIkioJfiERR8AuR\nKAp+IRKlq7v9tWoVk5fDu/2xhISVxXCZpkwkoSaWGBOrSxcrn9UgiThzc/N0zlCkfNPg4CC1Vat8\nN7rOc49w5sWXg+OryzxBZ2mVr9XoQCRRpBLepQaARi3s/+Ejh+kc55W6cOkCVxaQ54pEvi9cAi6b\nz9M5/UP8vFz85S+pbWqaJ+8U5uaobftYWKHJO1c48sVwvcNq5JysRe/8QiSKgl+IRFHwC5EoCn4h\nEkXBL0SiKPiFSJSuSn3NZgOLi2HJo1Lm0svycliKykVaINWI1AQAzQbXyiwiH7IWWgvzXMZZIjIl\nAMz388SeckRyvPj6BWpbJc87m+fPq7rMNbZGpKZhJaI5Li2HE2piiSczszxRay7SZakK7kexGL6u\n8pHWWsUCTzAaJwlLALCyyCXTyYmwxA0ADZJYtTBzjc4plMI+Vsr8ulmL3vmFSBQFvxCJouAXIlEU\n/EIkioJfiERR8AuRKBuS+szsAoBFAA0AdXc/Grt/s9nAykpYDilnuGzHMv7ypKYeADRqXL5y1ksK\n8UxBWFimqjdir6Fc2lpu8uxCa3L/55a5fFghdd8G+sJZYAAwNLif2g7ddZAfq3CO2qqNsOR45fJb\nern+fxbneXZko8kz3EqFiGyXC68/OZUAgFyeX4tj23gtxCVyLAAo5Pg1wq7juUiNyjrL3iNydIjb\nofP/lrtfvw2PI4ToIvrYL0SibDT4HcDfmtkLZnbsdjgkhOgOG/3Y/2F3nzCzHQB+bGYvu/tzN9+h\n/aJwDAD6+vs3eDghxO1iQ+/87j7R/n8awA8BPBi4z3F3P+ruR3t6+G+mhRDd5ZaD38z6zGzgjdsA\nPgrgzO1yTAixuWzkY/9OAD+0lmaSA/BX7v4/YhOaTUe5HJYospEMvWw2LKE4uKxRr3NbTAzJRGRA\n93D2mEdkuUKOZyta5FisHRMA9PRvp7apxXChzlqVH+sD93+A2vaN76e21y6FM/cAYGomLOlNTnCp\nr1HjbcMKOd4Kqxgp4NmshyXCSqRAamWV+9FT4H7ECsPGvvL29oZl2FUSKwBQWQzLorGYWMstB7+7\nvwaAXzVCiHc0kvqESBQFvxCJouAXIlEU/EIkioJfiETpagFPd0ejEe5314wUiiyXw69R+UjPOjcu\nHcaIJUVlScZfpBYk+nq51Ld39x3UVl7ixSBL/byXXGkg3PetvMrlyJHhXdR2/rWr1La4wItFZsn7\nSqMW6T+XC18bLfi8ygrPpmsSOXU2kjFXrvBjDUQku1i2aCSJENls+AJajUiOTVKolT3fEHrnFyJR\nFPxCJIqCX4hEUfALkSgKfiESpeu7/Sz5IZbYUyG7r7Hd1XwkoaZY5IkgLHkHAPr7winJFmkXdde+\n3dQ20MtTnM9e4vXx+iOJRL0DO8NzeofonF9euEht09e46lCv8+SYkaFwK7KeEk9+iZS5Q63C55Uj\nu+KsJqM3+BpWI63SKgV+XcH4E4hdV3VyvGaDP+esdb6rz9A7vxCJouAXIlEU/EIkioJfiERR8AuR\nKAp+IRKlq1JfjJhsx5KBEJFCClkureTY4wHIRNqGGTlcJpLZc/ehA9T26ku83unZl05TW7nKZaoj\nD/zz4PjI2DidMz3DpS1kuBw5MspbgGVBWopFZMrFhXD9QQAor/Bkm1qdXwdM1s1EpLJcpO3W7Nws\ntWUi9f0GBgaorZAPy4f5HF/fYibsfzbXeUKb3vmFSBQFvxCJouAXIlEU/EIkioJfiERR8AuRKOtK\nfWb2OIDfAzDt7ve2x0YBfB/AfgAXAHzK3bkG0sabTdoKqRCRSdotwd46XuCSTC6SItbTE8v4K1Hb\n8krY91jGVrPOZcUTJ16gttl5nk3nkYpwkxPhDL3RkRE6p7/Efbw+c5naZqavUNuuO0aD4xnw89xo\nRGrgRTLmslluy+fDx2tGygU26pHz2eA2i5zrEvED4K28+iNZn14Py7Ox7Ni1dPLO/5cAHloz9iiA\nZ939EIBn238LId5FrBv87v4cgLWlTh8G8ET79hMAPnGb/RJCbDK3+p1/p7tPtm9fRatjrxDiXcSG\nN/y89btc+mXNzI6Z2QkzO1Gt8p9oCiG6y60G/5SZ7QKA9v/T7I7uftzdj7r70UKBl88SQnSXWw3+\npwE80r79CIAf3R53hBDdohOp77sAPgJgzMwuA/gSgK8AeNLMPgfgIoBPdXKwTAboLYaliFKJy2+r\n5XJwvFqNFFrkih0yDV54cnU5fCwAWFwMZ52NDoZbZAHA+V/wQpyXJ7hUVuoJF8AEgHykAOnC/Fxw\n/OUXT9I58wtcVrwRaWu1vMyz8GrlO4Pjhw8fpnOGh3mR0eVlXqRzaYlnCs7PzwfHGxHJrlbjj2eR\nbMDRiP+FSLYdSyL0SAYkK54ay459y3HXu4O7f4aYfqfjowgh3nHoF35CJIqCX4hEUfALkSgKfiES\nRcEvRKJ0tYBnX38ffuNDvx60VSsR+Y30MstGMveyWS55zBE5DABKRV40sb8/nGW1b89+/ngFvsRD\nQ7yoYyWyHpVI37rValgSm5m5Tuc0m3yt+vr4erhz29xsOMmTnUsg3kOxWuXrMTfHz+ccKbhZLEay\nCyMSW6HANeRinl+Pi/Mz1JbLhDMge4r8+mAydyzDdC165xciURT8QiSKgl+IRFHwC5EoCn4hEkXB\nL0SidFXqy2UzGB4Jy0PLS1yiGB4JSyGxwo25iOwyPBIumAgAQ0OD1FYixT2HB4fpnHOvnqe2bdv4\nvJmZcDYaAFydvkZtZSKJxXrFDQzwDMLeXi7nxWQ7Zjt16hSdk4v0PIwVeN23L5xBCAD33HMwOL68\nyjMZzfi1mDHuozu/5hoNbiuSQrSZDC8ISiW9zpP69M4vRKoo+IVIFAW/EImi4BciURT8QiRKV3f7\nHU24h2vkFYq8BVU2G97ZzGb5HBjfKR0Z5TvfuUittWptJTh+ZZLvHC8u8S5mdx3YR22FAt/Rn53j\nx+sj272ZDH+dr1R4SfXFxUVqiyXbxB6Tkc/zOo7Dw1wZWSEt4ACg0QzX1duxg9fbGxziCgcpnQcA\nePUVruwM9PHWW3v2hNtexOrx9fSG1yqf7zyk9c4vRKIo+IVIFAW/EImi4BciURT8QiSKgl+IROmk\nXdfjAH4PwLS739se+zKA3wfwhh71RXd/Zr3HymYz6BsIJ2hkjL8O1Rvhmmr1Oq+1Futa5M5r4FVr\nXMup1cLzLHKw3eM7qC2f4wlGETdw6fIkta2Uw3Lkykp4HACWIq2wyqRVGgA0mzwBpr8//Nxicl4m\nw6XbQqQWYqXME4xefvml4PjgEE8UOnBwL7XlMlyyy0RaedWqfB0rRKq0iDyby4YlabOI/L2GTt75\n/xLAQ4Hxr7v7fe1/6wa+EOKdxbrB7+7PAeDdGoUQ70o28p3/82Z22sweN7OR2+aREKIr3GrwfxPA\nQQD3AZgE8FV2RzM7ZmYnzOzEwgJv6SyE6C63FPzuPuXuDW+VE/kWgAcj9z3u7kfd/ejgIN/gEkJ0\nl1sKfjPbddOfnwRw5va4I4ToFp1Ifd8F8BEAY2Z2GcCXAHzEzO5Dq2LYBQB/0NnhmkAmLHk0I3JZ\nNk/ki4i0Uq9HbE2uo8XkqwypGZjh6hV6eIIYDFxyzHNFCfk8zzxcnQ5/tVqOZL7VIpJpTE61DPcj\nRyS9WG3FZkSCdedZggXjnyj7yafNmZlpOqfvnvdR2649PBNzfPwAtRUiEieT9Op1nplarYSv72y2\n86y+de/p7p8JDD/W8RGEEO9I9As/IRJFwS9Eoij4hUgUBb8QiaLgFyJRulrAE3C4h+WLmKyRJRlM\niCUwGZeoYj2NHNyPpocP2ORTsLTCjfkc97HUy7POKlWexVZZJdl7EQmTtn4CkIkUSY1lM2bI8Szi\nRz4TScUkmZ0AUF7mvxzdOx6W5hZmZ+icK5euUtudd3E5L1Pg/hci8iyTU7MRKTvD1iMy5y2P0fE9\nhRC/Uij4hUgUBb8QiaLgFyJRFPxCJIqCX4hE6bLUZ4CHD5nLRrKeSFFCi0khhVjvPy67NCJSVK0a\nzjrLGPe9VuMSVS2iEbIMQgAoFGJFMMNr0qzzjLlanWfMZSJrlWlw/60ezt4c6Oc98prNyFpVuP/N\nEp83MNgXHO/t52mTU9NXqM3BC3E2mlyCXV7l12qDrGOsFmeWZEea8et3LXrnFyJRFPxCJIqCX4hE\nUfALkSgKfiESpau7/WZZFAvhmmqx2nlsN7QR2cHORNQDON/BjmXp5EjNulyO7xwXeX4OPJIYUy3z\nOoM779hObdMTl8PHyvJjDW4bpLZ6ZD3Gd3A/7hgbC47/4tVzdM7KCt9Jt0grL4skBI2NhVtKLC2H\n/QOA8+dep7a5eZ4QVOy7tffSJkmsYi25AAAZFi9K7BFCrIOCX4hEUfALkSgKfiESRcEvRKIo+IVI\nlE7ade0F8G0AO9HSEY67+zfMbBTA9wHsR6tl16fcfTb2WO6OWjUsRVQjCTBZ0s7Inct5lSqXDmOJ\nPbE2X4xmjstQHmnJtbLKa881G/y5Vas8EadWC9sOHOS15x548NeprVLjkuPICO9FNtgfTqgpN/l6\nnDr1IrWhxs9Lb2+J+zEUlpbHx3fSOefPXaC2K1d40s+Bu++ktlotIkuT6ztWpLJRD9si6vFbj9vB\nfeoA/sjdjwD4IIA/NLMjAB4F8Ky7HwLwbPtvIcS7hHWD390n3f1n7duLAM4CGAfwMIAn2nd7AsAn\nNstJIcTt52195zez/QDuB/A8gJ3uPtk2XUXra4EQ4l1Cx8FvZv0AfgDgC+6+cLPNW79TDX7bMLNj\nZnbCzE4sLPDvuEKI7tJR8JtZHq3A/467P9UenjKzXW37LgDBhufuftzdj7r70UHSK10I0X3WDX5r\n1dB6DMBZd//aTaanATzSvv0IgB/dfveEEJtFJ1l9HwLwWQA/N7OT7bEvAvgKgCfN7HMALgL41HoP\nVCz24NDBw0FbTAphGX/NBpfz6nVus4iEUo1IW6ylWExeqVRI+ywAvcWwHAYAJ0+eobbZGZ5Zduie\ne4LjtUi7q507d1BbTDlaWuHK7vzScnD8n73//XTOtRsL1Pb6hYvUVo3Is6uV8DkbHo1kJO7eTW3z\nc4vUZuDybC7LQ41Jz7G6lplMOF00k+k8UXfde7r734ELjr/T8ZGEEO8o9As/IRJFwS9Eoij4hUgU\nBb8QiaLgFyJRulrAs1Qo4j13HgraqlUusXkzLOVkIkUdHbzwZEy/ihXVbBI/Il2rooVJl5a4bPR/\n//fz1PbQR/8FtQ2NhgtWfv/JJ+mc7WNc6hvbziWxSpUX3GTnk51LAKgRWQ4ARobCzwsApianqK3h\n4czDkW381+jvfR8/Z3//9/+H2mam+S9Yx0hBUwDIkPfgZiPSr2vj9Tv1zi9Eqij4hUgUBb8QiaLg\nFyJRFPxCJIqCX4hE6arUBweVKJjcAQBuYf0iazGpL6J5RBSUiAmWZz5y3/P5IrUtzM9T274791Lb\nxz72MWqbWwxnxg0P8X58Y9tGqe3e976X2pZXeMaikXOzvMTlsPnZOWo7cg/346mn/gu13f+B+4Lj\nu3ZzefPwPWE5GgBWlnjmYTFyro8cPkJt9Xo447IeyXRlF2qhEOlRuQa98wuRKAp+IRJFwS9Eoij4\nhUgUBb8QidLV3f5ms4lVskPciCTAMGiXIwDZbEQJiCTvsF3qGDHXY8eanr5KbQffcxe19UfaU5XL\n4dp5d47vonNmpnlijB8O1wQEgCzNLgEK+XCNuXMTl/icyPncf+cearv3SLguJACM7won8PT0hP0D\ngME+vmv/L2NKS0St2BupC8iEqUaTJzqx+pVs3UPonV+IRFHwC5EoCn4hEkXBL0SiKPiFSBQFvxCJ\nsq7UZ2Z7AXwbrRbcDuC4u3/DzL4M4PcBXGvf9Yvu/kzsser1Oq5dvxa0xVpesXZGsYJluRyX7DIx\njfAWiKmD9QaX5c6df4Xa9u7h0tbqKq/9VyPtwbZv4zXwXj37IrUdOcyTXGItqKwvXDtvJiJv5rL8\nvNTKPIlo3x4uo5UKYR/rlVU6pxm5PHZGavGNjWyjtkxEDmbXd55e94ARXfTtXNud6Px1AH/k7j8z\nswEAL5jZj9u2r7v7f+j4aEKIdwyd9OqbBDDZvr1oZmcBjG+2Y0KIzeVtff41s/0A7gfwRl3pz5vZ\naTN73Mz450ohxDuOjoPfzPoB/ADAF9x9AcA3ARwEcB9anwy+SuYdM7MTZnbixhz/+aMQort0FPxm\nlkcr8L/j7k8BgLtPuXvD3ZsAvgXgwdBcdz/u7kfd/ejo8PDt8lsIsUHWDX5rZbo8BuCsu3/tpvGb\nM0U+CeDM7XdPCLFZdLLb/yEAnwXwczM72R77IoDPmNl9aOltFwD8wXoP1PQmyqvhFk8WkSiYFNKM\nZD25h+uiAUAtUhstloWXIVJULIOwOl2htkuXLlLbLpKNBgATkcy4ClnfiIu4SuRXALh0kfuYz/PL\np6+vLzi+uMC/+sXOy9TkJLUV8lxOnZq8EhyvN/h5KRB5EACyGV4jz4zPa0Z6umWY1JeLhCc5n41Y\n77g1dLLb/3fkUFFNXwjxzka/8BMiURT8QiSKgl+IRFHwC5EoCn4hEqWrBTwbjQYWFsMtqljLIgCo\nVKvBcXOeKlUq8SKMuYiEwgojAkCjEfaxUOSPNx0pjpmLyJtMsgOAS5cu83nlsIS1sBgu7AkAA4O8\nlde16zPUFstm7OkJZ/X1D/Afel2+HJblAOCVV16mtvt/7f3Udv7CueC4O5fEiiVeBDOf4bJiPsdt\n0czPetiXmJRd6Alf39VqpMXXGvTOL0SiKPiFSBQFvxCJouAXIlEU/EIkioJfiETpqtRXr9dx/fp0\n0BbrkVclUl+1wjOzeks91BaT+mJZUUtLS8Hxnl4uK9ZqYd8BoCfi42vnz1NbM1LsdLUcPl4uz7PR\nspHCmafP8OKesaw+1r8wn+Nr1WjwJ9YEl99efPV1aisV2fVGp2BllRf3LESKavb3huVNAOiJ2Ng1\nNzt7g87pGwg/3tJy+BoNoXd+IRJFwS9Eoij4hUgUBb8QiaLgFyJRFPxCJEp3pb5aDdPTYemlUOBS\nDpMBYwUfVyNyzdISz3CLSUCskOjMbCQjMSJHLq9E/GAVGgHUIhmQTfIERkZ4TxWLyFfZSObhwPAQ\ntS0vhdf/+gyXr1aW+TmbW+C2mQu8AGm+EJY4Y9cOk3QBoFTg52XPnl3UNjg4QG3Ml0qZZ3Zmp8PZ\nlpWKsvqEEOug4BciURT8QiSKgl+IRFHwC5Eo6+72m1kJwHMAiu37/427f8nM7gLwPQDbALwA4LPu\nzrNY0NqlZrv9sWSbnp5wAkxvH0+WQIbvyrL6Z0A8sYcl1FQjCSmFnnDbqvXmxfzwSGbPyNhocHw4\n0iS1WOTrEUu4Kha5QjMwFFYXtt+xm85ZXl6htnJkF/va/AK1FchuP6ubBwArK9yP7aNj1DY6HF57\nAGiyTCfwpKtYjcd6Oezjf3vmJ3TOWjp5568A+G13/wBa7bgfMrMPAvgzAF939/cAmAXwuY6PKoTY\nctYNfm/xhvCZb/9zAL8N4G/a408A+MSmeCiE2BQ6+s5vZtl2h95pAD8GcB7AnP9TK9zLAMY3x0Uh\nxGbQUfC7e8Pd7wOwB8CDAA53egAzO2ZmJ8zsxMoK/5WWEKK7vK3dfnefA/ATAL8BYNj+qSH5HgAT\nZM5xdz/q7kd7e3nlGiFEd1k3+M1su5kNt2/3APhdAGfRehH4V+27PQLgR5vlpBDi9tNJYs8uAE+Y\nWRatF4sn3f2/mtlLAL5nZn8C4P8BeGzdg+WyGB3lcgiDJdT09HIZrdbkyS+W44kssYQPJg8N9fHn\nlM3wJc4VeXunmJzXiLSa6u0Pf7oq9fBjResWLi5SW7/3cz+IxGkRCbY/0jZse+RT48A8T+wpV8LJ\nMTF5M5/bSW1o8GunGZNuwaW+YjY8z8Gv4Xwh/L4dS0xby7rB7+6nAdwfGH8Nre//Qoh3IfqFnxCJ\nouAXIlEU/EIkioJfiERR8AuRKOYe6f10uw9mdg3AxfafYwCud+3gHPnxZuTHm3m3+bHP3bd38oBd\nDf43HdjshLsf3ZKDyw/5IT/0sV+IVFHwC5EoWxn8x7fw2DcjP96M/Hgzv7J+bNl3fiHE1qKP/UIk\nypYEv5k9ZGavmNk5M3t0K3xo+3HBzH5uZifN7EQXj/u4mU2b2ZmbxkbN7Mdm9ov2/7y/1ub68WUz\nm2ivyUkz+3gX/NhrZj8xs5fM7EUz+zft8a6uScSPrq6JmZXM7B/M7FTbjz9uj99lZs+34+b7ZsYr\nqHaCu3f1H4AsWmXADgAoADgF4Ei3/Wj7cgHA2BYc9zcBPADgzE1jfw7g0fbtRwH82Rb58WUA/7bL\n67ELwAPt2wMAXgVwpNtrEvGjq2sCwAD0t2/nATwP4IMAngTw6fb4fwTwrzdynK14538QwDl3f81b\npb6/B+DhLfBjy3D35wCs7Vj5MFqFUIEuFUQlfnQdd59095+1by+iVSxmHF1ek4gfXcVbbHrR3K0I\n/nEAl276eyuLfzqAvzWzF8zs2Bb58AY73X2yffsqgEhFiU3n82Z2uv21YNO/ftyMme1Hq37E89jC\nNVnjB9DlNelG0dzUN/w+7O4PAPgYgD80s9/caoeA1is/Wi9MW8E3ARxEq0fDJICvduvAZtYP4AcA\nvuDub+rE0c01CfjR9TXxDRTN7ZStCP4JAHtv+psW/9xs3H2i/f80gB9iaysTTZnZLgBo/x9ubbTJ\nuPtU+8JrAvgWurQmZpZHK+C+4+5PtYe7viYhP7ZqTdrHfttFcztlK4L/pwAOtXcuCwA+DeDpbjth\nZn1mNvDGbQAfBXAmPmtTeRqtQqjAFhZEfSPY2nwSXVgTa/UEewzAWXf/2k2mrq4J86Pba9K1ornd\n2sFcs5v5cbR2Us8D+Hdb5MMBtJSGUwBe7KYfAL6L1sfHGlrf3T6HVs/DZwH8AsD/AjC6RX78ZwA/\nB3AareDb1QU/PozWR/rTAE62/32822sS8aOrawLg/WgVxT2N1gvNv7/pmv0HAOcA/DWA4kaOo1/4\nCZEoqW/4CZEsCn4hEkXBL0SiKPiFSBQFvxCJouAXIlEU/EIkioJfiET5RxbKuDcAbtuJAAAAAElF\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUhT8QkTK/wMM6GsAOQJFWwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qN5z9vjRsvL",
        "colab_type": "text"
      },
      "source": [
        "### Training - Build model , compile and train "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m26XDkJi1YyS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, GlobalAveragePooling2D,  Activation, GlobalMaxPooling2D\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization , Dense, Lambda\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam,SGD\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "WEIGHT_DECAY=1.25e-4\n",
        "reg=tf.keras.regularizers.l2(WEIGHT_DECAY)\n",
        "def init_pytorch(shape, dtype=tf.float32, partition_info=None):\n",
        "  fan = np.prod(shape[:-1])\n",
        "  bound = 1 / math.sqrt(fan)\n",
        "  return tf.random.uniform(shape, minval=-bound, maxval=bound, dtype=dtype)\n",
        "\n",
        "def conv(inp,f=32,k=3):\n",
        "  conv_layer=Conv2D(f,k,use_bias=False,padding='same',kernel_initializer=init_pytorch, kernel_regularizer=reg)(inp)\n",
        "  conv_layer=BatchNormalization(momentum=0.9, epsilon=1e-5)(conv_layer)\n",
        "  conv_layer=Activation('relu')(conv_layer)\n",
        "  return conv_layer\n",
        "def resBlk(inp,f=32,k=3,residual=True) :\n",
        "  res1=conv(inp,f,k)\n",
        "  res1=MaxPooling2D(pool_size=(2,2))(res1)\n",
        "  if residual:\n",
        "    res2=conv(res1,f,k)\n",
        "    res3=conv(res2,f,k)\n",
        "    return res1+res3\n",
        "  else:\n",
        "    return res1  \n",
        "\n",
        "def apply_weight(x):\n",
        "  return x*0.125  \n",
        "\n",
        "def random_pad_crop(image,padding=2):\n",
        "  shp=tf.shape(image)\n",
        "  \n",
        "  image=tf.pad(image,[(0, 0), (padding, padding), (padding, padding), (0, 0)], mode='reflect')\n",
        "  \n",
        "  image=tf.image.random_crop(image,size=shp)\n",
        "  return image  \n",
        "\n",
        "def flip_left_right(image):\n",
        "  return tf.image.random_flip_left_right(image)   \n",
        "\n",
        "def aug_fn1(img):  \n",
        "  return ds.cutout(ds.flip_left_right(ds.random_pad_crop(img,padding=2)),90,size=4)\n",
        "\n",
        "def aug_fn2(img):\n",
        "  return ds.cutout(ds.flip_left_right(ds.random_pad_crop(img,padding=1)),90,size=2)\n",
        "  \n",
        "\n",
        "def aug1(image):\n",
        "  print('is_training',is_training)\n",
        "  if is_training:\n",
        "    \n",
        "    #return tf.map_fn(lambda img: aug_fn1(img),image,parallel_iterations=ds.CPU_CORES,back_prop=is_training) \n",
        "    return tf.map_fn(lambda img: aug_fn1(img) ,image,parallel_iterations=10*ds.CPU_CORES,back_prop=is_training)    \n",
        "  else:\n",
        "    print('inside validation cycle\\n===============\\n')\n",
        "    return image  \n",
        "\n",
        "def aug2(image):\n",
        "  \n",
        "  print('is_training',is_training)\n",
        "  if is_training:\n",
        "    \n",
        "    return tf.map_fn(lambda img: aug_fn2(img) ,image,parallel_iterations=10*ds.CPU_CORES,back_prop=is_training)\n",
        "  else:\n",
        "    print('inside validation cycle\\n===============\\n')\n",
        "    return image        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVqdMyzkx4tY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(distort_param=0):\n",
        "  f=64\n",
        "  inp=Input(shape=(32,32,3))\n",
        "  layer1=conv(inp,f,3)\n",
        "  res1=resBlk(layer1,f*2,3)\n",
        "  if (distort_param in [1,3,4]):\n",
        "    res1=Lambda(aug1)(res1)\n",
        "  res2=resBlk(res1,f*4,3,False)\n",
        "  if (distort_param in [2,3,5]):\n",
        "    res2=Lambda(aug2)(res2)\n",
        "  res3=resBlk(res2,f*8,3)\n",
        "  \n",
        "  layer2=GlobalMaxPooling2D()(res3)\n",
        "  layer3=Dense(10, kernel_initializer=init_pytorch, use_bias=False,kernel_regularizer=reg)(layer2)\n",
        "  layer4=Lambda(lambda x: x*0.125)(layer3)\n",
        "  out=Activation('softmax')(layer4)\n",
        "  model=Model(inputs=[inp],outputs=[out])\n",
        "  model.summary()\n",
        "  return model "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nng9bFm1rQq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "\n",
        "def lr_schedule():\n",
        "    \n",
        "    def schedule(epoch):\n",
        "\n",
        "      lr=lr1=np.interp([epoch],[0, EPOCHS//5,EPOCHS], [0.025, 0.4, 0])[0]\n",
        "      print('epoch ', epoch+1, ': setting learning rate to ',lr1)\n",
        "      return lr\n",
        "    \n",
        "    return LearningRateScheduler(schedule)\n",
        "\n",
        "lr_sched = lr_schedule()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaDK2E2h81vV",
        "colab_type": "code",
        "outputId": "6ef75d36-bff7-4f27-8b22-296c5414afe4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "for model_params in [0,5,4]:\n",
        "  is_training=True\n",
        "  #global_step = tf.train.get_or_create_global_step()\n",
        "  model=model=build_model(model_params)\n",
        "  opt=SGD(lr=0.025,momentum=0.9,nesterov=True)\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,metrics=['accuracy']\n",
        "              )\n",
        "  batch_size=512\n",
        "  \n",
        "  if model_params in [0,4,5]:\n",
        "    train_ds=train_ds1\n",
        "    print('Model will be trained with image augmentation input layer\\n====================')\n",
        "    if model_params==4:\n",
        "      print('Model will also include distortion after Res Blk 1\\n=======================')\n",
        "    elif model_params==5:\n",
        "      print('Model will also include distortion after Res Blk 2\\n=======================')\n",
        "      \n",
        "  else:\n",
        "    if model_params==1:\n",
        "      print('Model will be trained with distortion after Res Blk 1\\n=======================')\n",
        "    elif model_params==2:\n",
        "      print('Model will be trained with distortion after Res Blk 2\\n=======================')\n",
        "    elif model_params==3:\n",
        "      print('Model will be trained with distortion after Res Blk 1 and Res Blk 2\\n=======================')    \n",
        "    train_ds=train_ds2  \n",
        "  model.fit(train_ds,epochs=EPOCHS, steps_per_epoch=np.ceil(50000/batch_size), \n",
        "          callbacks=[lr_sched],\n",
        "          verbose=1)\n",
        "  is_training=False\n",
        "  score=model.evaluate(test_ds, steps =np.ceil(10000/batch_size), verbose=1)\n",
        "\n",
        "  del(model)\n",
        "  del(train_ds)\n",
        "  \n",
        "  print('val accuracy score at the end of training model type ',model_params, score)\n",
        "  print(\"=========================================\\n\")\n",
        "\n",
        "#validation_data=test_ds, validation_steps=np.ceil(10000/batch_size),"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 32, 32, 64)   1728        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 32, 32, 64)   256         conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 32, 32, 64)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 128)  73728       activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 128)  512         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 128)  0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 16, 16, 128)  0           activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 16, 16, 128)  147456      max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 16, 16, 128)  512         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 16, 16, 128)  0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 16, 16, 128)  147456      activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 16, 16, 128)  512         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 16, 16, 128)  0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add (TensorFlowOpLa [(None, 16, 16, 128) 0           max_pooling2d[0][0]              \n",
            "                                                                 activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 16, 16, 256)  294912      tf_op_layer_add[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 16, 16, 256)  1024        conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 16, 16, 256)  0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 8, 8, 256)    0           activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 8, 8, 512)    1179648     max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 8, 8, 512)    2048        conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 8, 8, 512)    0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 4, 4, 512)    0           activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 4, 4, 512)    2359296     max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 4, 4, 512)    2048        conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 4, 4, 512)    0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 4, 4, 512)    2359296     activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 4, 4, 512)    2048        conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 4, 4, 512)    0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_1 (TensorFlowOp [(None, 4, 4, 512)]  0           max_pooling2d_2[0][0]            \n",
            "                                                                 activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling2d (GlobalMax (None, 512)          0           tf_op_layer_add_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 10)           5120        global_max_pooling2d[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, 10)           0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 10)           0           lambda[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 6,577,600\n",
            "Trainable params: 6,573,120\n",
            "Non-trainable params: 4,480\n",
            "__________________________________________________________________________________________________\n",
            "Model will be trained with image augmentation input layer\n",
            "====================\n",
            "Train for 98.0 steps\n",
            "epoch  1 : setting learning rate to  0.025\n",
            "Epoch 1/70\n",
            "98/98 [==============================] - 49s 501ms/step - loss: 1.6634 - accuracy: 0.4328\n",
            "epoch  2 : setting learning rate to  0.05178571428571428\n",
            "Epoch 2/70\n",
            "98/98 [==============================] - 39s 397ms/step - loss: 1.1154 - accuracy: 0.6358\n",
            "epoch  3 : setting learning rate to  0.07857142857142857\n",
            "Epoch 3/70\n",
            "98/98 [==============================] - 39s 401ms/step - loss: 0.8787 - accuracy: 0.7267\n",
            "epoch  4 : setting learning rate to  0.10535714285714284\n",
            "Epoch 4/70\n",
            "98/98 [==============================] - 39s 401ms/step - loss: 0.7524 - accuracy: 0.7766\n",
            "epoch  5 : setting learning rate to  0.13214285714285715\n",
            "Epoch 5/70\n",
            "98/98 [==============================] - 39s 402ms/step - loss: 0.6863 - accuracy: 0.8031\n",
            "epoch  6 : setting learning rate to  0.15892857142857142\n",
            "Epoch 6/70\n",
            "98/98 [==============================] - 39s 402ms/step - loss: 0.6453 - accuracy: 0.8200\n",
            "epoch  7 : setting learning rate to  0.1857142857142857\n",
            "Epoch 7/70\n",
            "98/98 [==============================] - 39s 401ms/step - loss: 0.6149 - accuracy: 0.8374\n",
            "epoch  8 : setting learning rate to  0.2125\n",
            "Epoch 8/70\n",
            "98/98 [==============================] - 39s 402ms/step - loss: 0.5962 - accuracy: 0.8466\n",
            "epoch  9 : setting learning rate to  0.23928571428571427\n",
            "Epoch 9/70\n",
            "98/98 [==============================] - 39s 402ms/step - loss: 0.5862 - accuracy: 0.8560\n",
            "epoch  10 : setting learning rate to  0.26607142857142857\n",
            "Epoch 10/70\n",
            "98/98 [==============================] - 39s 402ms/step - loss: 0.5903 - accuracy: 0.8571\n",
            "epoch  11 : setting learning rate to  0.29285714285714287\n",
            "Epoch 11/70\n",
            "98/98 [==============================] - 39s 402ms/step - loss: 0.5819 - accuracy: 0.8669\n",
            "epoch  12 : setting learning rate to  0.3196428571428572\n",
            "Epoch 12/70\n",
            "98/98 [==============================] - 39s 402ms/step - loss: 0.5715 - accuracy: 0.8759\n",
            "epoch  13 : setting learning rate to  0.3464285714285714\n",
            "Epoch 13/70\n",
            "98/98 [==============================] - 39s 401ms/step - loss: 0.5809 - accuracy: 0.8765\n",
            "epoch  14 : setting learning rate to  0.3732142857142857\n",
            "Epoch 14/70\n",
            "98/98 [==============================] - 39s 402ms/step - loss: 0.5951 - accuracy: 0.8771\n",
            "epoch  15 : setting learning rate to  0.4\n",
            "Epoch 15/70\n",
            "98/98 [==============================] - 39s 402ms/step - loss: 0.5993 - accuracy: 0.8797\n",
            "epoch  16 : setting learning rate to  0.3928571428571429\n",
            "Epoch 16/70\n",
            "98/98 [==============================] - 39s 402ms/step - loss: 0.5902 - accuracy: 0.8863\n",
            "epoch  17 : setting learning rate to  0.38571428571428573\n",
            "Epoch 17/70\n",
            "98/98 [==============================] - 39s 402ms/step - loss: 0.5653 - accuracy: 0.8965\n",
            "epoch  18 : setting learning rate to  0.3785714285714286\n",
            "Epoch 18/70\n",
            "98/98 [==============================] - 39s 402ms/step - loss: 0.5510 - accuracy: 0.9028\n",
            "epoch  19 : setting learning rate to  0.37142857142857144\n",
            "Epoch 19/70\n",
            "98/98 [==============================] - 39s 402ms/step - loss: 0.5416 - accuracy: 0.9092\n",
            "epoch  20 : setting learning rate to  0.3642857142857143\n",
            "Epoch 20/70\n",
            "98/98 [==============================] - 39s 402ms/step - loss: 0.5330 - accuracy: 0.9117\n",
            "epoch  21 : setting learning rate to  0.35714285714285715\n",
            "Epoch 21/70\n",
            "98/98 [==============================] - 39s 402ms/step - loss: 0.5225 - accuracy: 0.9144\n",
            "epoch  22 : setting learning rate to  0.35000000000000003\n",
            "Epoch 22/70\n",
            "98/98 [==============================] - 39s 402ms/step - loss: 0.5148 - accuracy: 0.9180\n",
            "epoch  23 : setting learning rate to  0.34285714285714286\n",
            "Epoch 23/70\n",
            "98/98 [==============================] - 39s 402ms/step - loss: 0.5123 - accuracy: 0.9190\n",
            "epoch  24 : setting learning rate to  0.33571428571428574\n",
            "Epoch 24/70\n",
            "98/98 [==============================] - 39s 401ms/step - loss: 0.5060 - accuracy: 0.9219\n",
            "epoch  25 : setting learning rate to  0.32857142857142857\n",
            "Epoch 25/70\n",
            "98/98 [==============================] - 39s 402ms/step - loss: 0.4940 - accuracy: 0.9254\n",
            "epoch  26 : setting learning rate to  0.32142857142857145\n",
            "Epoch 26/70\n",
            "98/98 [==============================] - 39s 401ms/step - loss: 0.4861 - accuracy: 0.9284\n",
            "epoch  27 : setting learning rate to  0.3142857142857143\n",
            "Epoch 27/70\n",
            "98/98 [==============================] - 39s 401ms/step - loss: 0.4851 - accuracy: 0.9302\n",
            "epoch  28 : setting learning rate to  0.30714285714285716\n",
            "Epoch 28/70\n",
            "98/98 [==============================] - 39s 402ms/step - loss: 0.4817 - accuracy: 0.9303\n",
            "epoch  29 : setting learning rate to  0.30000000000000004\n",
            "Epoch 29/70\n",
            "98/98 [==============================] - 39s 402ms/step - loss: 0.4665 - accuracy: 0.9352\n",
            "epoch  30 : setting learning rate to  0.29285714285714287\n",
            "Epoch 30/70\n",
            "98/98 [==============================] - 39s 402ms/step - loss: 0.4628 - accuracy: 0.9365\n",
            "epoch  31 : setting learning rate to  0.2857142857142857\n",
            "Epoch 31/70\n",
            "98/98 [==============================] - 39s 401ms/step - loss: 0.4601 - accuracy: 0.9381\n",
            "epoch  32 : setting learning rate to  0.2785714285714286\n",
            "Epoch 32/70\n",
            "98/98 [==============================] - 39s 402ms/step - loss: 0.4533 - accuracy: 0.9405\n",
            "epoch  33 : setting learning rate to  0.27142857142857146\n",
            "Epoch 33/70\n",
            "98/98 [==============================] - 39s 402ms/step - loss: 0.4494 - accuracy: 0.9408\n",
            "epoch  34 : setting learning rate to  0.2642857142857143\n",
            "Epoch 34/70\n",
            "98/98 [==============================] - 39s 402ms/step - loss: 0.4383 - accuracy: 0.9436\n",
            "epoch  35 : setting learning rate to  0.2571428571428571\n",
            "Epoch 35/70\n",
            "98/98 [==============================] - 39s 401ms/step - loss: 0.4309 - accuracy: 0.9460\n",
            "epoch  36 : setting learning rate to  0.25\n",
            "Epoch 36/70\n",
            "98/98 [==============================] - 39s 402ms/step - loss: 0.4314 - accuracy: 0.9465\n",
            "epoch  37 : setting learning rate to  0.24285714285714285\n",
            "Epoch 37/70\n",
            "98/98 [==============================] - 39s 401ms/step - loss: 0.4220 - accuracy: 0.9496\n",
            "epoch  38 : setting learning rate to  0.2357142857142857\n",
            "Epoch 38/70\n",
            "98/98 [==============================] - 39s 402ms/step - loss: 0.4134 - accuracy: 0.9512\n",
            "epoch  39 : setting learning rate to  0.2285714285714286\n",
            "Epoch 39/70\n",
            "98/98 [==============================] - 39s 402ms/step - loss: 0.4093 - accuracy: 0.9528\n",
            "epoch  40 : setting learning rate to  0.22142857142857145\n",
            "Epoch 40/70\n",
            "98/98 [==============================] - 39s 401ms/step - loss: 0.3970 - accuracy: 0.9562\n",
            "epoch  41 : setting learning rate to  0.2142857142857143\n",
            "Epoch 41/70\n",
            "98/98 [==============================] - 39s 402ms/step - loss: 0.3935 - accuracy: 0.9565\n",
            "epoch  42 : setting learning rate to  0.20714285714285716\n",
            "Epoch 42/70\n",
            "98/98 [==============================] - 39s 402ms/step - loss: 0.3861 - accuracy: 0.9587\n",
            "epoch  43 : setting learning rate to  0.2\n",
            "Epoch 43/70\n",
            "98/98 [==============================] - 39s 401ms/step - loss: 0.3782 - accuracy: 0.9600\n",
            "epoch  44 : setting learning rate to  0.19285714285714287\n",
            "Epoch 44/70\n",
            "98/98 [==============================] - 39s 401ms/step - loss: 0.3690 - accuracy: 0.9628\n",
            "epoch  45 : setting learning rate to  0.18571428571428572\n",
            "Epoch 45/70\n",
            "98/98 [==============================] - 39s 401ms/step - loss: 0.3587 - accuracy: 0.9639\n",
            "epoch  46 : setting learning rate to  0.17857142857142858\n",
            "Epoch 46/70\n",
            "98/98 [==============================] - 39s 401ms/step - loss: 0.3594 - accuracy: 0.9633\n",
            "epoch  47 : setting learning rate to  0.17142857142857143\n",
            "Epoch 47/70\n",
            "98/98 [==============================] - 39s 402ms/step - loss: 0.3489 - accuracy: 0.9651\n",
            "epoch  48 : setting learning rate to  0.16428571428571428\n",
            "Epoch 48/70\n",
            "98/98 [==============================] - 39s 402ms/step - loss: 0.3428 - accuracy: 0.9670\n",
            "epoch  49 : setting learning rate to  0.15714285714285714\n",
            "Epoch 49/70\n",
            "98/98 [==============================] - 39s 402ms/step - loss: 0.3285 - accuracy: 0.9715\n",
            "epoch  50 : setting learning rate to  0.15000000000000002\n",
            "Epoch 50/70\n",
            "98/98 [==============================] - 39s 402ms/step - loss: 0.3212 - accuracy: 0.9726\n",
            "epoch  51 : setting learning rate to  0.14285714285714285\n",
            "Epoch 51/70\n",
            "98/98 [==============================] - 39s 402ms/step - loss: 0.3154 - accuracy: 0.9727\n",
            "epoch  52 : setting learning rate to  0.13571428571428573\n",
            "Epoch 52/70\n",
            "98/98 [==============================] - 39s 401ms/step - loss: 0.3037 - accuracy: 0.9756\n",
            "epoch  53 : setting learning rate to  0.12857142857142856\n",
            "Epoch 53/70\n",
            "98/98 [==============================] - 39s 401ms/step - loss: 0.2992 - accuracy: 0.9760\n",
            "epoch  54 : setting learning rate to  0.12142857142857144\n",
            "Epoch 54/70\n",
            "98/98 [==============================] - 39s 402ms/step - loss: 0.2878 - accuracy: 0.9784\n",
            "epoch  55 : setting learning rate to  0.11428571428571427\n",
            "Epoch 55/70\n",
            "98/98 [==============================] - 39s 402ms/step - loss: 0.2777 - accuracy: 0.9803\n",
            "epoch  56 : setting learning rate to  0.10714285714285715\n",
            "Epoch 56/70\n",
            "98/98 [==============================] - 39s 401ms/step - loss: 0.2700 - accuracy: 0.9817\n",
            "epoch  57 : setting learning rate to  0.09999999999999998\n",
            "Epoch 57/70\n",
            "98/98 [==============================] - 39s 401ms/step - loss: 0.2583 - accuracy: 0.9839\n",
            "epoch  58 : setting learning rate to  0.09285714285714286\n",
            "Epoch 58/70\n",
            "98/98 [==============================] - 39s 401ms/step - loss: 0.2500 - accuracy: 0.9854\n",
            "epoch  59 : setting learning rate to  0.08571428571428569\n",
            "Epoch 59/70\n",
            "98/98 [==============================] - 39s 402ms/step - loss: 0.2410 - accuracy: 0.9870\n",
            "epoch  60 : setting learning rate to  0.07857142857142857\n",
            "Epoch 60/70\n",
            "98/98 [==============================] - 39s 402ms/step - loss: 0.2342 - accuracy: 0.9883\n",
            "epoch  61 : setting learning rate to  0.0714285714285714\n",
            "Epoch 61/70\n",
            "98/98 [==============================] - 39s 402ms/step - loss: 0.2257 - accuracy: 0.9891\n",
            "epoch  62 : setting learning rate to  0.06428571428571428\n",
            "Epoch 62/70\n",
            "98/98 [==============================] - 39s 402ms/step - loss: 0.2181 - accuracy: 0.9909\n",
            "epoch  63 : setting learning rate to  0.05714285714285716\n",
            "Epoch 63/70\n",
            "98/98 [==============================] - 39s 401ms/step - loss: 0.2113 - accuracy: 0.9918\n",
            "epoch  64 : setting learning rate to  0.04999999999999999\n",
            "Epoch 64/70\n",
            "98/98 [==============================] - 39s 401ms/step - loss: 0.2013 - accuracy: 0.9939\n",
            "epoch  65 : setting learning rate to  0.04285714285714287\n",
            "Epoch 65/70\n",
            "98/98 [==============================] - 39s 402ms/step - loss: 0.1959 - accuracy: 0.9950\n",
            "epoch  66 : setting learning rate to  0.0357142857142857\n",
            "Epoch 66/70\n",
            "98/98 [==============================] - 39s 401ms/step - loss: 0.1916 - accuracy: 0.9949\n",
            "epoch  67 : setting learning rate to  0.02857142857142858\n",
            "Epoch 67/70\n",
            "98/98 [==============================] - 39s 401ms/step - loss: 0.1867 - accuracy: 0.9959\n",
            "epoch  68 : setting learning rate to  0.021428571428571408\n",
            "Epoch 68/70\n",
            "98/98 [==============================] - 39s 401ms/step - loss: 0.1837 - accuracy: 0.9965\n",
            "epoch  69 : setting learning rate to  0.01428571428571429\n",
            "Epoch 69/70\n",
            "98/98 [==============================] - 39s 402ms/step - loss: 0.1801 - accuracy: 0.9972\n",
            "epoch  70 : setting learning rate to  0.0071428571428571175\n",
            "Epoch 70/70\n",
            "98/98 [==============================] - 39s 401ms/step - loss: 0.1788 - accuracy: 0.9974\n",
            "20/20 [==============================] - 3s 142ms/step - loss: 0.3517 - accuracy: 0.9473\n",
            "val accuracy score at the end of training model type  0 [0.3516610637307167, 0.9472656]\n",
            "=========================================\n",
            "\n",
            "is_training True\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 32, 32, 64)   1728        input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 32, 32, 64)   256         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 32, 32, 64)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 32, 32, 128)  73728       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 32, 32, 128)  512         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 32, 32, 128)  0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 16, 16, 128)  0           activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 16, 16, 128)  147456      max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 16, 16, 128)  512         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 16, 16, 128)  0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 16, 16, 128)  147456      activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 16, 16, 128)  512         conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 16, 16, 128)  0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_2 (TensorFlowOp [(None, 16, 16, 128) 0           max_pooling2d_3[0][0]            \n",
            "                                                                 activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 16, 16, 256)  294912      tf_op_layer_add_2[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 16, 16, 256)  1024        conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 16, 16, 256)  0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 8, 8, 256)    0           activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 8, 8, 256)    0           max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 8, 8, 512)    1179648     lambda_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 8, 8, 512)    2048        conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 8, 8, 512)    0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2D)  (None, 4, 4, 512)    0           activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 4, 4, 512)    2359296     max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 4, 4, 512)    2048        conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 4, 4, 512)    0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 4, 4, 512)    2359296     activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 4, 4, 512)    2048        conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 4, 4, 512)    0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_3 (TensorFlowOp [(None, 4, 4, 512)]  0           max_pooling2d_5[0][0]            \n",
            "                                                                 activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling2d_1 (GlobalM (None, 512)          0           tf_op_layer_add_3[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 10)           5120        global_max_pooling2d_1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "lambda_2 (Lambda)               (None, 10)           0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 10)           0           lambda_2[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 6,577,600\n",
            "Trainable params: 6,573,120\n",
            "Non-trainable params: 4,480\n",
            "__________________________________________________________________________________________________\n",
            "Model will be trained with image augmentation input layer\n",
            "====================\n",
            "Model will also include distortion after Res Blk 2\n",
            "=======================\n",
            "Train for 98.0 steps\n",
            "epoch  1 : setting learning rate to  0.025\n",
            "Epoch 1/70\n",
            "is_training True\n",
            "is_training True\n",
            "98/98 [==============================] - 168s 2s/step - loss: 1.7533 - accuracy: 0.3923\n",
            "epoch  2 : setting learning rate to  0.05178571428571428\n",
            "Epoch 2/70\n",
            "98/98 [==============================] - 164s 2s/step - loss: 1.2561 - accuracy: 0.5808\n",
            "epoch  3 : setting learning rate to  0.07857142857142857\n",
            "Epoch 3/70\n",
            "98/98 [==============================] - 164s 2s/step - loss: 0.9913 - accuracy: 0.6835\n",
            "epoch  4 : setting learning rate to  0.10535714285714284\n",
            "Epoch 4/70\n",
            "98/98 [==============================] - 163s 2s/step - loss: 0.8631 - accuracy: 0.7331\n",
            "epoch  5 : setting learning rate to  0.13214285714285715\n",
            "Epoch 5/70\n",
            "98/98 [==============================] - 162s 2s/step - loss: 0.7905 - accuracy: 0.7642\n",
            "epoch  6 : setting learning rate to  0.15892857142857142\n",
            "Epoch 6/70\n",
            " 6/98 [>.............................] - ETA: 2:30 - loss: 0.7561 - accuracy: 0.7734"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9IdmrAB2IrD",
        "colab_type": "text"
      },
      "source": [
        "### Run time disconnected , so run the interrupted tests once again"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "5946b829-5cda-4af6-fcd9-17f3da858f24",
        "id": "zWwDPdBAdtvl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "for model_params in [5,4]:\n",
        "  is_training=True\n",
        "  #global_step = tf.train.get_or_create_global_step()\n",
        "  model=model=build_model(model_params)\n",
        "  opt=SGD(lr=0.025,momentum=0.9,nesterov=True)\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,metrics=['accuracy']\n",
        "              )\n",
        "  batch_size=512\n",
        "  \n",
        "  if model_params in [0,4,5]:\n",
        "    train_ds=train_ds1\n",
        "    print('Model will be trained with image augmentation input layer\\n====================')\n",
        "    if model_params==4:\n",
        "      print('Model will also include distortion after Res Blk 1\\n=======================')\n",
        "    elif model_params==5:\n",
        "      print('Model will also include distortion after Res Blk 2\\n=======================')\n",
        "      \n",
        "  else:\n",
        "    if model_params==1:\n",
        "      print('Model will be trained with distortion after Res Blk 1\\n=======================')\n",
        "    elif model_params==2:\n",
        "      print('Model will be trained with distortion after Res Blk 2\\n=======================')\n",
        "    elif model_params==3:\n",
        "      print('Model will be trained with distortion after Res Blk 1 and Res Blk 2\\n=======================')    \n",
        "    train_ds=train_ds2  \n",
        "  model.fit(train_ds,epochs=EPOCHS, steps_per_epoch=np.ceil(50000/batch_size), \n",
        "          callbacks=[lr_sched],\n",
        "          verbose=1)\n",
        "  is_training=False\n",
        "  score=model.evaluate(test_ds, steps =np.ceil(10000/batch_size), verbose=1)\n",
        "\n",
        "  del(model)\n",
        "  del(train_ds)\n",
        "  \n",
        "  print('val accuracy score at the end of training model type ',model_params, score)\n",
        "  print(\"=========================================\\n\")\n",
        "\n",
        "#validation_data=test_ds, validation_steps=np.ceil(10000/batch_size),"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "is_training True\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 32, 32, 64)   1728        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 32, 32, 64)   256         conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 32, 32, 64)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 128)  73728       activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 128)  512         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 128)  0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 16, 16, 128)  0           activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 16, 16, 128)  147456      max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 16, 16, 128)  512         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 16, 16, 128)  0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 16, 16, 128)  147456      activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 16, 16, 128)  512         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 16, 16, 128)  0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add (TensorFlowOpLa [(None, 16, 16, 128) 0           max_pooling2d[0][0]              \n",
            "                                                                 activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 16, 16, 256)  294912      tf_op_layer_add[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 16, 16, 256)  1024        conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 16, 16, 256)  0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 8, 8, 256)    0           activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, 8, 8, 256)    0           max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 8, 8, 512)    1179648     lambda[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 8, 8, 512)    2048        conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 8, 8, 512)    0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 4, 4, 512)    0           activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 4, 4, 512)    2359296     max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 4, 4, 512)    2048        conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 4, 4, 512)    0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 4, 4, 512)    2359296     activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 4, 4, 512)    2048        conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 4, 4, 512)    0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_1 (TensorFlowOp [(None, 4, 4, 512)]  0           max_pooling2d_2[0][0]            \n",
            "                                                                 activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling2d (GlobalMax (None, 512)          0           tf_op_layer_add_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 10)           5120        global_max_pooling2d[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 10)           0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 10)           0           lambda_1[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 6,577,600\n",
            "Trainable params: 6,573,120\n",
            "Non-trainable params: 4,480\n",
            "__________________________________________________________________________________________________\n",
            "Model will be trained with image augmentation input layer\n",
            "====================\n",
            "Model will also include distortion after Res Blk 2\n",
            "=======================\n",
            "Train for 98.0 steps\n",
            "epoch  1 : setting learning rate to  0.025\n",
            "Epoch 1/70\n",
            "is_training True\n",
            "is_training True\n",
            "98/98 [==============================] - 167s 2s/step - loss: 1.7426 - accuracy: 0.4017\n",
            "epoch  2 : setting learning rate to  0.05178571428571428\n",
            "Epoch 2/70\n",
            "98/98 [==============================] - 156s 2s/step - loss: 1.2244 - accuracy: 0.5938\n",
            "epoch  3 : setting learning rate to  0.07857142857142857\n",
            "Epoch 3/70\n",
            "98/98 [==============================] - 156s 2s/step - loss: 0.9732 - accuracy: 0.6900\n",
            "epoch  4 : setting learning rate to  0.10535714285714284\n",
            "Epoch 4/70\n",
            "98/98 [==============================] - 155s 2s/step - loss: 0.8527 - accuracy: 0.7396\n",
            "epoch  5 : setting learning rate to  0.13214285714285715\n",
            "Epoch 5/70\n",
            "98/98 [==============================] - 155s 2s/step - loss: 0.7837 - accuracy: 0.7663\n",
            "epoch  6 : setting learning rate to  0.15892857142857142\n",
            "Epoch 6/70\n",
            "98/98 [==============================] - 155s 2s/step - loss: 0.7300 - accuracy: 0.7896\n",
            "epoch  7 : setting learning rate to  0.1857142857142857\n",
            "Epoch 7/70\n",
            "98/98 [==============================] - 155s 2s/step - loss: 0.7004 - accuracy: 0.8040\n",
            "epoch  8 : setting learning rate to  0.2125\n",
            "Epoch 8/70\n",
            "98/98 [==============================] - 155s 2s/step - loss: 0.6838 - accuracy: 0.8158\n",
            "epoch  9 : setting learning rate to  0.23928571428571427\n",
            "Epoch 9/70\n",
            "98/98 [==============================] - 156s 2s/step - loss: 0.6745 - accuracy: 0.8211\n",
            "epoch  10 : setting learning rate to  0.26607142857142857\n",
            "Epoch 10/70\n",
            "98/98 [==============================] - 156s 2s/step - loss: 0.6624 - accuracy: 0.8316\n",
            "epoch  11 : setting learning rate to  0.29285714285714287\n",
            "Epoch 11/70\n",
            "98/98 [==============================] - 156s 2s/step - loss: 0.6591 - accuracy: 0.8364\n",
            "epoch  12 : setting learning rate to  0.3196428571428572\n",
            "Epoch 12/70\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.6491 - accuracy: 0.8442\n",
            "epoch  13 : setting learning rate to  0.3464285714285714\n",
            "Epoch 13/70\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.6535 - accuracy: 0.8481\n",
            "epoch  14 : setting learning rate to  0.3732142857142857\n",
            "Epoch 14/70\n",
            "98/98 [==============================] - 156s 2s/step - loss: 0.6493 - accuracy: 0.8543\n",
            "epoch  15 : setting learning rate to  0.4\n",
            "Epoch 15/70\n",
            "98/98 [==============================] - 155s 2s/step - loss: 0.6654 - accuracy: 0.8524\n",
            "epoch  16 : setting learning rate to  0.3928571428571429\n",
            "Epoch 16/70\n",
            "98/98 [==============================] - 155s 2s/step - loss: 0.6486 - accuracy: 0.8616\n",
            "epoch  17 : setting learning rate to  0.38571428571428573\n",
            "Epoch 17/70\n",
            "98/98 [==============================] - 155s 2s/step - loss: 0.6305 - accuracy: 0.8709\n",
            "epoch  18 : setting learning rate to  0.3785714285714286\n",
            "Epoch 18/70\n",
            "98/98 [==============================] - 154s 2s/step - loss: 0.6224 - accuracy: 0.8753\n",
            "epoch  19 : setting learning rate to  0.37142857142857144\n",
            "Epoch 19/70\n",
            "98/98 [==============================] - 153s 2s/step - loss: 0.6070 - accuracy: 0.8812\n",
            "epoch  20 : setting learning rate to  0.3642857142857143\n",
            "Epoch 20/70\n",
            "98/98 [==============================] - 155s 2s/step - loss: 0.5949 - accuracy: 0.8870\n",
            "epoch  21 : setting learning rate to  0.35714285714285715\n",
            "Epoch 21/70\n",
            "98/98 [==============================] - 155s 2s/step - loss: 0.5920 - accuracy: 0.8884\n",
            "epoch  22 : setting learning rate to  0.35000000000000003\n",
            "Epoch 22/70\n",
            "98/98 [==============================] - 154s 2s/step - loss: 0.5814 - accuracy: 0.8936\n",
            "epoch  23 : setting learning rate to  0.34285714285714286\n",
            "Epoch 23/70\n",
            "98/98 [==============================] - 154s 2s/step - loss: 0.5681 - accuracy: 0.8987\n",
            "epoch  24 : setting learning rate to  0.33571428571428574\n",
            "Epoch 24/70\n",
            "98/98 [==============================] - 154s 2s/step - loss: 0.5717 - accuracy: 0.8961\n",
            "epoch  25 : setting learning rate to  0.32857142857142857\n",
            "Epoch 25/70\n",
            "98/98 [==============================] - 152s 2s/step - loss: 0.5547 - accuracy: 0.9013\n",
            "epoch  26 : setting learning rate to  0.32142857142857145\n",
            "Epoch 26/70\n",
            "98/98 [==============================] - 152s 2s/step - loss: 0.5520 - accuracy: 0.9031\n",
            "epoch  27 : setting learning rate to  0.3142857142857143\n",
            "Epoch 27/70\n",
            "98/98 [==============================] - 154s 2s/step - loss: 0.5471 - accuracy: 0.9058\n",
            "epoch  28 : setting learning rate to  0.30714285714285716\n",
            "Epoch 28/70\n",
            "98/98 [==============================] - 155s 2s/step - loss: 0.5352 - accuracy: 0.9096\n",
            "epoch  29 : setting learning rate to  0.30000000000000004\n",
            "Epoch 29/70\n",
            "98/98 [==============================] - 155s 2s/step - loss: 0.5314 - accuracy: 0.9122\n",
            "epoch  30 : setting learning rate to  0.29285714285714287\n",
            "Epoch 30/70\n",
            "98/98 [==============================] - 154s 2s/step - loss: 0.5262 - accuracy: 0.9143\n",
            "epoch  31 : setting learning rate to  0.2857142857142857\n",
            "Epoch 31/70\n",
            "98/98 [==============================] - 154s 2s/step - loss: 0.5135 - accuracy: 0.9167\n",
            "epoch  32 : setting learning rate to  0.2785714285714286\n",
            "Epoch 32/70\n",
            "98/98 [==============================] - 154s 2s/step - loss: 0.5149 - accuracy: 0.9171\n",
            "epoch  33 : setting learning rate to  0.27142857142857146\n",
            "Epoch 33/70\n",
            "98/98 [==============================] - 154s 2s/step - loss: 0.5063 - accuracy: 0.9208\n",
            "epoch  34 : setting learning rate to  0.2642857142857143\n",
            "Epoch 34/70\n",
            "98/98 [==============================] - 154s 2s/step - loss: 0.4979 - accuracy: 0.9222\n",
            "epoch  35 : setting learning rate to  0.2571428571428571\n",
            "Epoch 35/70\n",
            "98/98 [==============================] - 153s 2s/step - loss: 0.4941 - accuracy: 0.9242\n",
            "epoch  36 : setting learning rate to  0.25\n",
            "Epoch 36/70\n",
            "98/98 [==============================] - 154s 2s/step - loss: 0.4841 - accuracy: 0.9263\n",
            "epoch  37 : setting learning rate to  0.24285714285714285\n",
            "Epoch 37/70\n",
            "98/98 [==============================] - 155s 2s/step - loss: 0.4831 - accuracy: 0.9264\n",
            "epoch  38 : setting learning rate to  0.2357142857142857\n",
            "Epoch 38/70\n",
            "98/98 [==============================] - 155s 2s/step - loss: 0.4745 - accuracy: 0.9304\n",
            "epoch  39 : setting learning rate to  0.2285714285714286\n",
            "Epoch 39/70\n",
            "98/98 [==============================] - 155s 2s/step - loss: 0.4651 - accuracy: 0.9321\n",
            "epoch  40 : setting learning rate to  0.22142857142857145\n",
            "Epoch 40/70\n",
            "98/98 [==============================] - 155s 2s/step - loss: 0.4560 - accuracy: 0.9358\n",
            "epoch  41 : setting learning rate to  0.2142857142857143\n",
            "Epoch 41/70\n",
            "98/98 [==============================] - 154s 2s/step - loss: 0.4489 - accuracy: 0.9375\n",
            "epoch  42 : setting learning rate to  0.20714285714285716\n",
            "Epoch 42/70\n",
            "98/98 [==============================] - 154s 2s/step - loss: 0.4443 - accuracy: 0.9381\n",
            "epoch  43 : setting learning rate to  0.2\n",
            "Epoch 43/70\n",
            "98/98 [==============================] - 154s 2s/step - loss: 0.4369 - accuracy: 0.9394\n",
            "epoch  44 : setting learning rate to  0.19285714285714287\n",
            "Epoch 44/70\n",
            "98/98 [==============================] - 155s 2s/step - loss: 0.4301 - accuracy: 0.9413\n",
            "epoch  45 : setting learning rate to  0.18571428571428572\n",
            "Epoch 45/70\n",
            "98/98 [==============================] - 155s 2s/step - loss: 0.4229 - accuracy: 0.9433\n",
            "epoch  46 : setting learning rate to  0.17857142857142858\n",
            "Epoch 46/70\n",
            "98/98 [==============================] - 155s 2s/step - loss: 0.4140 - accuracy: 0.9451\n",
            "epoch  47 : setting learning rate to  0.17142857142857143\n",
            "Epoch 47/70\n",
            "98/98 [==============================] - 154s 2s/step - loss: 0.4003 - accuracy: 0.9496\n",
            "epoch  48 : setting learning rate to  0.16428571428571428\n",
            "Epoch 48/70\n",
            "98/98 [==============================] - 154s 2s/step - loss: 0.3951 - accuracy: 0.9490\n",
            "epoch  49 : setting learning rate to  0.15714285714285714\n",
            "Epoch 49/70\n",
            "98/98 [==============================] - 154s 2s/step - loss: 0.3892 - accuracy: 0.9502\n",
            "epoch  50 : setting learning rate to  0.15000000000000002\n",
            "Epoch 50/70\n",
            "98/98 [==============================] - 154s 2s/step - loss: 0.3777 - accuracy: 0.9531\n",
            "epoch  51 : setting learning rate to  0.14285714285714285\n",
            "Epoch 51/70\n",
            "98/98 [==============================] - 154s 2s/step - loss: 0.3699 - accuracy: 0.9566\n",
            "epoch  52 : setting learning rate to  0.13571428571428573\n",
            "Epoch 52/70\n",
            "98/98 [==============================] - 154s 2s/step - loss: 0.3626 - accuracy: 0.9576\n",
            "epoch  53 : setting learning rate to  0.12857142857142856\n",
            "Epoch 53/70\n",
            "98/98 [==============================] - 155s 2s/step - loss: 0.3501 - accuracy: 0.9608\n",
            "epoch  54 : setting learning rate to  0.12142857142857144\n",
            "Epoch 54/70\n",
            "98/98 [==============================] - 156s 2s/step - loss: 0.3406 - accuracy: 0.9624\n",
            "epoch  55 : setting learning rate to  0.11428571428571427\n",
            "Epoch 55/70\n",
            "98/98 [==============================] - 156s 2s/step - loss: 0.3328 - accuracy: 0.9648\n",
            "epoch  56 : setting learning rate to  0.10714285714285715\n",
            "Epoch 56/70\n",
            "98/98 [==============================] - 156s 2s/step - loss: 0.3243 - accuracy: 0.9659\n",
            "epoch  57 : setting learning rate to  0.09999999999999998\n",
            "Epoch 57/70\n",
            "98/98 [==============================] - 154s 2s/step - loss: 0.3133 - accuracy: 0.9683\n",
            "epoch  58 : setting learning rate to  0.09285714285714286\n",
            "Epoch 58/70\n",
            "98/98 [==============================] - 156s 2s/step - loss: 0.3011 - accuracy: 0.9721\n",
            "epoch  59 : setting learning rate to  0.08571428571428569\n",
            "Epoch 59/70\n",
            "98/98 [==============================] - 154s 2s/step - loss: 0.2913 - accuracy: 0.9735\n",
            "epoch  60 : setting learning rate to  0.07857142857142857\n",
            "Epoch 60/70\n",
            "98/98 [==============================] - 154s 2s/step - loss: 0.2853 - accuracy: 0.9748\n",
            "epoch  61 : setting learning rate to  0.0714285714285714\n",
            "Epoch 61/70\n",
            "98/98 [==============================] - 153s 2s/step - loss: 0.2733 - accuracy: 0.9780\n",
            "epoch  62 : setting learning rate to  0.06428571428571428\n",
            "Epoch 62/70\n",
            "98/98 [==============================] - 154s 2s/step - loss: 0.2626 - accuracy: 0.9804\n",
            "epoch  63 : setting learning rate to  0.05714285714285716\n",
            "Epoch 63/70\n",
            "98/98 [==============================] - 154s 2s/step - loss: 0.2535 - accuracy: 0.9823\n",
            "epoch  64 : setting learning rate to  0.04999999999999999\n",
            "Epoch 64/70\n",
            "98/98 [==============================] - 153s 2s/step - loss: 0.2447 - accuracy: 0.9843\n",
            "epoch  65 : setting learning rate to  0.04285714285714287\n",
            "Epoch 65/70\n",
            "98/98 [==============================] - 152s 2s/step - loss: 0.2346 - accuracy: 0.9873\n",
            "epoch  66 : setting learning rate to  0.0357142857142857\n",
            "Epoch 66/70\n",
            "98/98 [==============================] - 153s 2s/step - loss: 0.2285 - accuracy: 0.9885\n",
            "epoch  67 : setting learning rate to  0.02857142857142858\n",
            "Epoch 67/70\n",
            "98/98 [==============================] - 155s 2s/step - loss: 0.2228 - accuracy: 0.9892\n",
            "epoch  68 : setting learning rate to  0.021428571428571408\n",
            "Epoch 68/70\n",
            "98/98 [==============================] - 154s 2s/step - loss: 0.2173 - accuracy: 0.9907\n",
            "epoch  69 : setting learning rate to  0.01428571428571429\n",
            "Epoch 69/70\n",
            "98/98 [==============================] - 155s 2s/step - loss: 0.2123 - accuracy: 0.9924\n",
            "epoch  70 : setting learning rate to  0.0071428571428571175\n",
            "Epoch 70/70\n",
            "98/98 [==============================] - 156s 2s/step - loss: 0.2093 - accuracy: 0.9924\n",
            "is_training False\n",
            "inside validation cycle\n",
            "===============\n",
            "\n",
            "20/20 [==============================] - 3s 137ms/step - loss: 0.3870 - accuracy: 0.9450\n",
            "val accuracy score at the end of training model type  5 [0.3869777783751488, 0.94501954]\n",
            "=========================================\n",
            "\n",
            "is_training True\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 32, 32, 64)   1728        input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 32, 32, 64)   256         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 32, 32, 64)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 32, 32, 128)  73728       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 32, 32, 128)  512         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 32, 32, 128)  0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 16, 16, 128)  0           activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 16, 16, 128)  147456      max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 16, 16, 128)  512         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 16, 16, 128)  0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 16, 16, 128)  147456      activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 16, 16, 128)  512         conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 16, 16, 128)  0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_2 (TensorFlowOp [(None, 16, 16, 128) 0           max_pooling2d_3[0][0]            \n",
            "                                                                 activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_2 (Lambda)               (None, 16, 16, 128)  0           tf_op_layer_add_2[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 16, 16, 256)  294912      lambda_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 16, 16, 256)  1024        conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 16, 16, 256)  0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 8, 8, 256)    0           activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 8, 8, 512)    1179648     max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 8, 8, 512)    2048        conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 8, 8, 512)    0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2D)  (None, 4, 4, 512)    0           activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 4, 4, 512)    2359296     max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 4, 4, 512)    2048        conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 4, 4, 512)    0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 4, 4, 512)    2359296     activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 4, 4, 512)    2048        conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 4, 4, 512)    0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_3 (TensorFlowOp [(None, 4, 4, 512)]  0           max_pooling2d_5[0][0]            \n",
            "                                                                 activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling2d_1 (GlobalM (None, 512)          0           tf_op_layer_add_3[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 10)           5120        global_max_pooling2d_1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "lambda_3 (Lambda)               (None, 10)           0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 10)           0           lambda_3[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 6,577,600\n",
            "Trainable params: 6,573,120\n",
            "Non-trainable params: 4,480\n",
            "__________________________________________________________________________________________________\n",
            "Model will be trained with image augmentation input layer\n",
            "====================\n",
            "Model will also include distortion after Res Blk 1\n",
            "=======================\n",
            "Train for 98.0 steps\n",
            "epoch  1 : setting learning rate to  0.025\n",
            "Epoch 1/70\n",
            "is_training True\n",
            "is_training True\n",
            "98/98 [==============================] - 161s 2s/step - loss: 1.7495 - accuracy: 0.3918\n",
            "epoch  2 : setting learning rate to  0.05178571428571428\n",
            "Epoch 2/70\n",
            "98/98 [==============================] - 155s 2s/step - loss: 1.2395 - accuracy: 0.5882\n",
            "epoch  3 : setting learning rate to  0.07857142857142857\n",
            "Epoch 3/70\n",
            "98/98 [==============================] - 156s 2s/step - loss: 0.9875 - accuracy: 0.6859\n",
            "epoch  4 : setting learning rate to  0.10535714285714284\n",
            "Epoch 4/70\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.8611 - accuracy: 0.7358\n",
            "epoch  5 : setting learning rate to  0.13214285714285715\n",
            "Epoch 5/70\n",
            "98/98 [==============================] - 158s 2s/step - loss: 0.7874 - accuracy: 0.7647\n",
            "epoch  6 : setting learning rate to  0.15892857142857142\n",
            "Epoch 6/70\n",
            "98/98 [==============================] - 156s 2s/step - loss: 0.7389 - accuracy: 0.7855\n",
            "epoch  7 : setting learning rate to  0.1857142857142857\n",
            "Epoch 7/70\n",
            "98/98 [==============================] - 158s 2s/step - loss: 0.7221 - accuracy: 0.7966\n",
            "epoch  8 : setting learning rate to  0.2125\n",
            "Epoch 8/70\n",
            "98/98 [==============================] - 158s 2s/step - loss: 0.6922 - accuracy: 0.8119\n",
            "epoch  9 : setting learning rate to  0.23928571428571427\n",
            "Epoch 9/70\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.6772 - accuracy: 0.8218\n",
            "epoch  10 : setting learning rate to  0.26607142857142857\n",
            "Epoch 10/70\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.6657 - accuracy: 0.8290\n",
            "epoch  11 : setting learning rate to  0.29285714285714287\n",
            "Epoch 11/70\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.6593 - accuracy: 0.8375\n",
            "epoch  12 : setting learning rate to  0.3196428571428572\n",
            "Epoch 12/70\n",
            "98/98 [==============================] - 156s 2s/step - loss: 0.6689 - accuracy: 0.8380\n",
            "epoch  13 : setting learning rate to  0.3464285714285714\n",
            "Epoch 13/70\n",
            "98/98 [==============================] - 156s 2s/step - loss: 0.6739 - accuracy: 0.8406\n",
            "epoch  14 : setting learning rate to  0.3732142857142857\n",
            "Epoch 14/70\n",
            "98/98 [==============================] - 158s 2s/step - loss: 0.6615 - accuracy: 0.8511\n",
            "epoch  15 : setting learning rate to  0.4\n",
            "Epoch 15/70\n",
            "98/98 [==============================] - 158s 2s/step - loss: 0.6716 - accuracy: 0.8518\n",
            "epoch  16 : setting learning rate to  0.3928571428571429\n",
            "Epoch 16/70\n",
            "98/98 [==============================] - 158s 2s/step - loss: 0.6502 - accuracy: 0.8618\n",
            "epoch  17 : setting learning rate to  0.38571428571428573\n",
            "Epoch 17/70\n",
            "98/98 [==============================] - 158s 2s/step - loss: 0.6342 - accuracy: 0.8691\n",
            "epoch  18 : setting learning rate to  0.3785714285714286\n",
            "Epoch 18/70\n",
            "98/98 [==============================] - 159s 2s/step - loss: 0.6273 - accuracy: 0.8725\n",
            "epoch  19 : setting learning rate to  0.37142857142857144\n",
            "Epoch 19/70\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.6117 - accuracy: 0.8791\n",
            "epoch  20 : setting learning rate to  0.3642857142857143\n",
            "Epoch 20/70\n",
            "98/98 [==============================] - 159s 2s/step - loss: 0.6068 - accuracy: 0.8835\n",
            "epoch  21 : setting learning rate to  0.35714285714285715\n",
            "Epoch 21/70\n",
            "98/98 [==============================] - 159s 2s/step - loss: 0.5935 - accuracy: 0.8887\n",
            "epoch  22 : setting learning rate to  0.35000000000000003\n",
            "Epoch 22/70\n",
            "98/98 [==============================] - 160s 2s/step - loss: 0.5832 - accuracy: 0.8920\n",
            "epoch  23 : setting learning rate to  0.34285714285714286\n",
            "Epoch 23/70\n",
            "98/98 [==============================] - 159s 2s/step - loss: 0.5691 - accuracy: 0.8972\n",
            "epoch  24 : setting learning rate to  0.33571428571428574\n",
            "Epoch 24/70\n",
            "98/98 [==============================] - 160s 2s/step - loss: 0.5697 - accuracy: 0.8964\n",
            "epoch  25 : setting learning rate to  0.32857142857142857\n",
            "Epoch 25/70\n",
            "98/98 [==============================] - 160s 2s/step - loss: 0.5580 - accuracy: 0.9010\n",
            "epoch  26 : setting learning rate to  0.32142857142857145\n",
            "Epoch 26/70\n",
            "98/98 [==============================] - 158s 2s/step - loss: 0.5521 - accuracy: 0.9052\n",
            "epoch  27 : setting learning rate to  0.3142857142857143\n",
            "Epoch 27/70\n",
            "98/98 [==============================] - 159s 2s/step - loss: 0.5467 - accuracy: 0.9071\n",
            "epoch  28 : setting learning rate to  0.30714285714285716\n",
            "Epoch 28/70\n",
            "98/98 [==============================] - 160s 2s/step - loss: 0.5431 - accuracy: 0.9069\n",
            "epoch  29 : setting learning rate to  0.30000000000000004\n",
            "Epoch 29/70\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.5346 - accuracy: 0.9112\n",
            "epoch  30 : setting learning rate to  0.29285714285714287\n",
            "Epoch 30/70\n",
            "98/98 [==============================] - 158s 2s/step - loss: 0.5321 - accuracy: 0.9131\n",
            "epoch  31 : setting learning rate to  0.2857142857142857\n",
            "Epoch 31/70\n",
            "98/98 [==============================] - 159s 2s/step - loss: 0.5148 - accuracy: 0.9181\n",
            "epoch  32 : setting learning rate to  0.2785714285714286\n",
            "Epoch 32/70\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.5106 - accuracy: 0.9190\n",
            "epoch  33 : setting learning rate to  0.27142857142857146\n",
            "Epoch 33/70\n",
            "98/98 [==============================] - 158s 2s/step - loss: 0.5081 - accuracy: 0.9196\n",
            "epoch  34 : setting learning rate to  0.2642857142857143\n",
            "Epoch 34/70\n",
            "98/98 [==============================] - 158s 2s/step - loss: 0.5000 - accuracy: 0.9233\n",
            "epoch  35 : setting learning rate to  0.2571428571428571\n",
            "Epoch 35/70\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.4951 - accuracy: 0.9234\n",
            "epoch  36 : setting learning rate to  0.25\n",
            "Epoch 36/70\n",
            "98/98 [==============================] - 158s 2s/step - loss: 0.4968 - accuracy: 0.9241\n",
            "epoch  37 : setting learning rate to  0.24285714285714285\n",
            "Epoch 37/70\n",
            "98/98 [==============================] - 159s 2s/step - loss: 0.4741 - accuracy: 0.9306\n",
            "epoch  38 : setting learning rate to  0.2357142857142857\n",
            "Epoch 38/70\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.4740 - accuracy: 0.9306\n",
            "epoch  39 : setting learning rate to  0.2285714285714286\n",
            "Epoch 39/70\n",
            "98/98 [==============================] - 158s 2s/step - loss: 0.4596 - accuracy: 0.9340\n",
            "epoch  40 : setting learning rate to  0.22142857142857145\n",
            "Epoch 40/70\n",
            "98/98 [==============================] - 159s 2s/step - loss: 0.4594 - accuracy: 0.9338\n",
            "epoch  41 : setting learning rate to  0.2142857142857143\n",
            "Epoch 41/70\n",
            "98/98 [==============================] - 159s 2s/step - loss: 0.4518 - accuracy: 0.9356\n",
            "epoch  42 : setting learning rate to  0.20714285714285716\n",
            "Epoch 42/70\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.4450 - accuracy: 0.9380\n",
            "epoch  43 : setting learning rate to  0.2\n",
            "Epoch 43/70\n",
            "98/98 [==============================] - 159s 2s/step - loss: 0.4369 - accuracy: 0.9397\n",
            "epoch  44 : setting learning rate to  0.19285714285714287\n",
            "Epoch 44/70\n",
            "98/98 [==============================] - 158s 2s/step - loss: 0.4289 - accuracy: 0.9418\n",
            "epoch  45 : setting learning rate to  0.18571428571428572\n",
            "Epoch 45/70\n",
            "98/98 [==============================] - 156s 2s/step - loss: 0.4199 - accuracy: 0.9453\n",
            "epoch  46 : setting learning rate to  0.17857142857142858\n",
            "Epoch 46/70\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.4158 - accuracy: 0.9448\n",
            "epoch  47 : setting learning rate to  0.17142857142857143\n",
            "Epoch 47/70\n",
            "98/98 [==============================] - 158s 2s/step - loss: 0.4114 - accuracy: 0.9463\n",
            "epoch  48 : setting learning rate to  0.16428571428571428\n",
            "Epoch 48/70\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.3947 - accuracy: 0.9507\n",
            "epoch  49 : setting learning rate to  0.15714285714285714\n",
            "Epoch 49/70\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.3832 - accuracy: 0.9540\n",
            "epoch  50 : setting learning rate to  0.15000000000000002\n",
            "Epoch 50/70\n",
            "98/98 [==============================] - 158s 2s/step - loss: 0.3805 - accuracy: 0.9536\n",
            "epoch  51 : setting learning rate to  0.14285714285714285\n",
            "Epoch 51/70\n",
            "98/98 [==============================] - 159s 2s/step - loss: 0.3708 - accuracy: 0.9566\n",
            "epoch  52 : setting learning rate to  0.13571428571428573\n",
            "Epoch 52/70\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.3604 - accuracy: 0.9590\n",
            "epoch  53 : setting learning rate to  0.12857142857142856\n",
            "Epoch 53/70\n",
            "98/98 [==============================] - 159s 2s/step - loss: 0.3527 - accuracy: 0.9607\n",
            "epoch  54 : setting learning rate to  0.12142857142857144\n",
            "Epoch 54/70\n",
            "98/98 [==============================] - 158s 2s/step - loss: 0.3378 - accuracy: 0.9653\n",
            "epoch  55 : setting learning rate to  0.11428571428571427\n",
            "Epoch 55/70\n",
            "98/98 [==============================] - 158s 2s/step - loss: 0.3277 - accuracy: 0.9675\n",
            "epoch  56 : setting learning rate to  0.10714285714285715\n",
            "Epoch 56/70\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.3237 - accuracy: 0.9673\n",
            "epoch  57 : setting learning rate to  0.09999999999999998\n",
            "Epoch 57/70\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.3136 - accuracy: 0.9695\n",
            "epoch  58 : setting learning rate to  0.09285714285714286\n",
            "Epoch 58/70\n",
            "98/98 [==============================] - 156s 2s/step - loss: 0.3006 - accuracy: 0.9724\n",
            "epoch  59 : setting learning rate to  0.08571428571428569\n",
            "Epoch 59/70\n",
            "98/98 [==============================] - 156s 2s/step - loss: 0.2968 - accuracy: 0.9725\n",
            "epoch  60 : setting learning rate to  0.07857142857142857\n",
            "Epoch 60/70\n",
            "98/98 [==============================] - 158s 2s/step - loss: 0.2827 - accuracy: 0.9766\n",
            "epoch  61 : setting learning rate to  0.0714285714285714\n",
            "Epoch 61/70\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.2724 - accuracy: 0.9791\n",
            "epoch  62 : setting learning rate to  0.06428571428571428\n",
            "Epoch 62/70\n",
            "98/98 [==============================] - 156s 2s/step - loss: 0.2601 - accuracy: 0.9817\n",
            "epoch  63 : setting learning rate to  0.05714285714285716\n",
            "Epoch 63/70\n",
            "98/98 [==============================] - 159s 2s/step - loss: 0.2534 - accuracy: 0.9832\n",
            "epoch  64 : setting learning rate to  0.04999999999999999\n",
            "Epoch 64/70\n",
            "98/98 [==============================] - 158s 2s/step - loss: 0.2449 - accuracy: 0.9855\n",
            "epoch  65 : setting learning rate to  0.04285714285714287\n",
            "Epoch 65/70\n",
            "98/98 [==============================] - 161s 2s/step - loss: 0.2383 - accuracy: 0.9858\n",
            "epoch  66 : setting learning rate to  0.0357142857142857\n",
            "Epoch 66/70\n",
            "98/98 [==============================] - 159s 2s/step - loss: 0.2268 - accuracy: 0.9891\n",
            "epoch  67 : setting learning rate to  0.02857142857142858\n",
            "Epoch 67/70\n",
            "98/98 [==============================] - 160s 2s/step - loss: 0.2216 - accuracy: 0.9906\n",
            "epoch  68 : setting learning rate to  0.021428571428571408\n",
            "Epoch 68/70\n",
            "98/98 [==============================] - 158s 2s/step - loss: 0.2174 - accuracy: 0.9911\n",
            "epoch  69 : setting learning rate to  0.01428571428571429\n",
            "Epoch 69/70\n",
            "98/98 [==============================] - 158s 2s/step - loss: 0.2132 - accuracy: 0.9920\n",
            "epoch  70 : setting learning rate to  0.0071428571428571175\n",
            "Epoch 70/70\n",
            "98/98 [==============================] - 157s 2s/step - loss: 0.2111 - accuracy: 0.9924\n",
            "is_training False\n",
            "inside validation cycle\n",
            "===============\n",
            "\n",
            "20/20 [==============================] - 3s 128ms/step - loss: 0.3685 - accuracy: 0.9491\n",
            "val accuracy score at the end of training model type  4 [0.3685023710131645, 0.9491211]\n",
            "=========================================\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqkyspQ4R1C6",
        "colab_type": "text"
      },
      "source": [
        "### Summary of Test Results \n",
        "Hyperparameters : Epochs:70, max_lr:0.4, momentum:0.9, L2-wt_decay on Conv Layers :1.25e-4 , Batch_size=512 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzOpzOMqq-gQ",
        "colab_type": "text"
      },
      "source": [
        "| Trial | Augmentation strategy | Train accuracy |Test Accuracy | Hyperparameters |Comments |\n",
        "| :--- | :---: | :---: | :---: | :---: | :--- |\n",
        "| 1 | cutout(flip_lr(pad4_random_crop(inp_image),8)  |99.74  | 94.73 | as above | train-test acc gap 4.88 |\n",
        "| 2 | cutout(flip_lr(pad2_random_crop(res_blk1),4)  |-  | - | \" | Did not run this test |\n",
        "| 3 | cutout(flip_lr(pad1_random_crop(res_blk2),2) | - | - | \" | Did not run this test |\n",
        "| 4 | augmentation of trial 2 + augmentation of trial 3  | -  | - | \" | Did not run this test |\n",
        "| 5 | augmentations of trial 1 + augmentation of trial 2  | 99.24  | **94.91** |\"| train-test acc gap 4.33 |\n",
        "| 6 | augmentations of trial 1 + augmentation of trial 3  | 99.24 | 94.50 |\"|  Train-Test acc gap : 4.74 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7EJpOdo09FV",
        "colab_type": "text"
      },
      "source": [
        "**Distortion of mid/lower layer activation channels, when used in combination with usual Image augmentation , seems to improve Regularization and perhaps we could explore this option for a larger number of epochs. Training for 70 epochs , image-augmentation plus distortion of channels after ResBlk 1 gave 94.91 val accuracy which is more than Val accuracy obtained with Image augmentation alone . But since the tests were interrupted and started again , we need to check if this happens consistently over several trials and also over larger number of epochs**"
      ]
    }
  ]
}