{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Third.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ravindrabharathi/Project1/blob/master/Session4/Third_DNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBzDI9HEsAmu",
        "colab_type": "text"
      },
      "source": [
        "# Build a Convolutional Neural Network with less than 15000 parameters to achieve a validation accuracy of 99.4 or more for MNIST dataset \n",
        "\n",
        "\n",
        "The target is to build a deep learning CNN model with as little parameters as possible and at the same time achieve a high validation accuracy of 99.4 or more . The low parameter count becomes important when deploying the model in memory constrained devices used in edge computing . MNIST is one of the more popular (and simpler) datasets to begin your journey in Vision based Deep learning. We will use this MNIST dataset for this exercise. \n",
        "\n",
        "We will build the model step by step . Broadly speaking we will follow these steps \n",
        "\n",
        "1. Decide on the basic architecture for the network \n",
        "2. Fine tune parameters to comply with the 15000 limit\n",
        "3. Add improvements to the network using Batch Normalization \n",
        "4. See if we can converge faster while learning by using Dropouts to overcome overfitting ,higher   learning rates, etc \n",
        "\n",
        "We are now at step 3 .\n",
        "\n",
        "In the [first iteration](https://github.com/ravindrabharathi/Project1/blob/master/Session4/First_DNN.ipynb)  we fixed the basic architecture without bothering too much about the number of parameters . \n",
        "\n",
        "In [the second iteration](https://colab.research.google.com/github/ravindrabharathi/Project1/blob/master/Session4/Second_DNN.ipynb) we brought down the number of parameters to within the required limit to around 11k . \n",
        "\n",
        "In this iteration we will tune the performance of the model by adding regularization via BatchNormalization .\n",
        "\n",
        "Batch Normalization is a way for the network take care of internal covariate shift in the features and was first introduced in this paper titled Batch Normalization: [Accelerating Deep Network Training by Reducing Internal Covariate Shift ](https://arxiv.org/abs/1502.03167)\n",
        "\n",
        "We will follow how BatchNormalization was used in the paper i.e Convolution followed Batch Normalization and then Activation although recently some practioners prefer using BatchNormalization after activation citing better performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0uvqzIGp5zc",
        "colab_type": "text"
      },
      "source": [
        "###Import necessary libraries / modules\n",
        "Import numpy library for array/ matrix operations\n",
        "\n",
        "Import Sequential Model from keras/models for building the model\n",
        "\n",
        "Import Conv2D , Activation , Flatten , BatchNormalization, MaxPooling2D from keras/layers \n",
        "\n",
        "Import np_utils module from keras/utils for numpy related helper functions\n",
        "\n",
        "Import mnist dataset containing hand-written digits images from keras.datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eso6UHE080D4",
        "colab_type": "code",
        "outputId": "b22b4bdb-5538-4599-d2d0-d36b9641768b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential,load_model\n",
        "from keras.layers import Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zByEi95J86RD",
        "colab_type": "text"
      },
      "source": [
        "### Load pre-shuffled MNIST data into train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eRM0QWN83PV",
        "colab_type": "code",
        "outputId": "a4fd93bd-52ea-4857-bb06-2bdc4dd0b3b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db3hDvFDqpS9",
        "colab_type": "text"
      },
      "source": [
        "###print the shape of training data and also inspect the first image using matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a4Be72j8-ZC",
        "colab_type": "code",
        "outputId": "e60e8e9d-5c99-4b72-febc-92f934f161f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "source": [
        "print (X_train.shape)\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.imshow(X_train[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f42d0f0c6a0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7BHSzLjq6nT",
        "colab_type": "text"
      },
      "source": [
        "####Reshape the training and test dataset to include the channel information.In this case it is a greyscale image and so there is 1 channel . the image data was read in as a 28x28 numpy array and is now reshaped to 28x28x1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkmprriw9AnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3aPHupgrF5a",
        "colab_type": "text"
      },
      "source": [
        "###Cast training data as float32 and normalize/re-scale the values such that they are between 0 and 1 instead of 0 and 255"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2m4YS4E9CRh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmUE1nCXrMua",
        "colab_type": "text"
      },
      "source": [
        "###inspect the first 10 training class labels . They will be some number between 0 and 9 representing the hand-written digit in the corresponding Training data. Each of 0 to 9 represents a class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Mn0vAYD9DvB",
        "colab_type": "code",
        "outputId": "b346130b-bde0-419c-823e-ad37292bbf03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_train[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dld_th_rU9s",
        "colab_type": "text"
      },
      "source": [
        "####One hot encoding of training and test class labels : Convert 1-dimensional class arrays to 10-dimensional class matrices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG8JiXR39FHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYlFRvKS9HMB",
        "colab_type": "code",
        "outputId": "965f30cd-0ba4-42c5-e633-d818d6c7a193",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "Y_train[:10]\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNM1NgZdtQm8",
        "colab_type": "text"
      },
      "source": [
        "###Define a ModelCheckPoint callback which will be called at the end of every training epoch . We will use this callback function to save the model whenever vallidation accuracy improves . We do this so that we can load and use the best model for further predictions after training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Yr6tsrzcSce",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "  \n",
        "#chkpoint_model=ModelCheckpoint(\"/gdrive/My Drive/EVA/Session3/model_customv1_mnist_best.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='max') \n",
        "\n",
        "chkpoint_model=ModelCheckpoint(\"model_custom_v1_mnist_best.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='max') \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmyGmS8oy2qE",
        "colab_type": "text"
      },
      "source": [
        "#Building the model version 3\n",
        "\n",
        "In version 3 of the model we will try and improve the performance of the model by adding BatchNormalization for each layer of convolution \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rdn9MvWmVOtA",
        "colab_type": "code",
        "outputId": "30c25b80-1e45-4283-f582-32a5a6992faa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# to get a certain degeree of predictability when generating random numbers,\n",
        "# set a random seed to initialize the pseudo-random number generator \n",
        "np.random.seed(seed=42)  \n",
        "\n",
        "# instantiate a sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# add the first convolution layer - 10 numbers of 3x3 filters , \n",
        "#This layer sees the input image of 28x28 x 1 channel . \n",
        "\n",
        "model.add(Conv2D(10, (3, 3), input_shape=(28,28,1), use_bias=False))  # remove bias param by setting it to false \n",
        "model.add(BatchNormalization())  #add Batch normalization  \n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "\n",
        "# Now the global receptive field is 3 x 3 \n",
        "\n",
        "# First Convolution Block\n",
        "# Block1 conv layer 1 - 12 filters of shape  3x3x10 \n",
        "# input from previous layer is 26 x 26 x 10 . \n",
        "\n",
        "## Block 1\n",
        "\n",
        "model.add(Conv2D(12, 3, use_bias=False))  # remove bias param by setting it to false \n",
        "model.add(BatchNormalization())  #add Batch normalization \n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "\n",
        "#Global receptive field is 5x5\n",
        "\n",
        "# Add convolution layer - 16 filters of shape  3x3x12 \n",
        "#input from previous layer is 24 x 24 x 12 . \n",
        "\n",
        "model.add(Conv2D(16, 3, use_bias=False))  # remove bias param by setting it to false \n",
        "model.add(BatchNormalization())  #add Batch normalization\n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "\n",
        "\n",
        "#Global receptive field is 7x7\n",
        "\n",
        "##  Transition block \n",
        "\n",
        "# Perform 2x2 max pooling  . \n",
        "#Input from previous layer is 22 x 22 X 16 \n",
        "\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "\n",
        "# After max pooling , 2D spatial dimension reduces by half , i.e it becomes 11 x 11 . \n",
        "# Maxpooling (withpool size 2 and stride 1) doubles receptive field . \n",
        "# So global receptive field after max pooling is 14 x 14\n",
        "\n",
        "# add 1x1 convolution to reduce the channel numbers to 10 \n",
        "\n",
        "model.add(Conv2D(10, 1, use_bias=False))  # remove bias param by setting it to false \n",
        "model.add(BatchNormalization())  #add Batch normalization \n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "\n",
        "\n",
        "#Convolution Block 2\n",
        "\n",
        "# Add convolution layer - 12 filters of shape 3x3x10\n",
        "#input from transition layer is 11 x 11 x 10 .  \n",
        "\n",
        "model.add(Conv2D(12, 3,  use_bias=False))  # remove bias param by setting it to false \n",
        "model.add(BatchNormalization())  #add Batch normalization\n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "\n",
        "#Global receptive field is now 16 x 16 \n",
        "\n",
        "# Add convolution layer - 16 filters of shape 3x3x12 \n",
        "#input coming from previous layer is 9 x 9 x 12 . \n",
        "\n",
        "model.add(Conv2D(16, 3,  use_bias=False)) # remove bias param by setting it to false \n",
        "model.add(BatchNormalization())  #add Batch normalization \n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "\n",
        "#  Global receptive field is now 18 x 18 \n",
        "\n",
        "# Add 1x1 convolution to reduce number of channels to 10  \n",
        "#input coming from previous layer is 7 x 7 x 16 .\n",
        "\n",
        "model.add(Conv2D(10, 1,  use_bias=False))  # remove bias param by setting it to false \n",
        "model.add(BatchNormalization())  #add Batch normalization \n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "\n",
        "#Global receptive field is now 18 x 18 \n",
        "\n",
        "# Last layer :  Add convolution layer - 10 filters of shape 7x7x10 \n",
        "#input coming from previous layer is 7 x 7 x 10 .\n",
        "\n",
        "model.add(Conv2D(10, 7,  use_bias=False))  # remove bias param by setting it to false \n",
        "model.add(BatchNormalization())  #add Batch normalization\n",
        "# Note absence of ReLU activation here \n",
        "\n",
        "model.add(Flatten())  # Flatten the 2d array to 1d input for the softmax activation \n",
        "model.add(Activation('softmax'))   # Softmax activation to out class probabilities "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTdVH726xhN3",
        "colab_type": "code",
        "outputId": "88ff8969-9549-4a92-debf-0035924e071e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1020
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 26, 26, 10)        90        \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 26, 26, 10)        40        \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 26, 26, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 24, 24, 12)        1080      \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 24, 24, 12)        48        \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 24, 24, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 22, 22, 16)        1728      \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 22, 22, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 22, 22, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 11, 11, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 11, 11, 10)        160       \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 11, 11, 10)        40        \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 11, 11, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 9, 9, 12)          1080      \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 9, 9, 12)          48        \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 9, 9, 12)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 7, 7, 16)          1728      \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 7, 7, 16)          64        \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 7, 7, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 7, 7, 10)          160       \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 7, 7, 10)          40        \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 7, 7, 10)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 1, 1, 10)          4900      \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 1, 1, 10)          40        \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 11,310\n",
            "Trainable params: 11,118\n",
            "Non-trainable params: 192\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7A0AhG_xrjw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqappyT1x7sf",
        "colab_type": "code",
        "outputId": "c642bce8-0a50-4d53-cd74-8a2b9d3a17da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2162
        }
      },
      "source": [
        "model.fit(X_train, Y_train, validation_data=(X_test,Y_test),batch_size=32, epochs=30, verbose=1, callbacks=[chkpoint_model])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/30\n",
            "60000/60000 [==============================] - 25s 417us/step - loss: 0.3935 - acc: 0.9287 - val_loss: 0.1202 - val_acc: 0.9816\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.98160, saving model to model_custom_v1_mnist_best.h5\n",
            "Epoch 2/30\n",
            "60000/60000 [==============================] - 21s 356us/step - loss: 0.1506 - acc: 0.9708 - val_loss: 0.0630 - val_acc: 0.9879\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.98160 to 0.98790, saving model to model_custom_v1_mnist_best.h5\n",
            "Epoch 3/30\n",
            "60000/60000 [==============================] - 22s 359us/step - loss: 0.0992 - acc: 0.9784 - val_loss: 0.0497 - val_acc: 0.9876\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.98790\n",
            "Epoch 4/30\n",
            "60000/60000 [==============================] - 22s 368us/step - loss: 0.0773 - acc: 0.9823 - val_loss: 0.0401 - val_acc: 0.9910\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.98790 to 0.99100, saving model to model_custom_v1_mnist_best.h5\n",
            "Epoch 5/30\n",
            "60000/60000 [==============================] - 21s 356us/step - loss: 0.0639 - acc: 0.9848 - val_loss: 0.0336 - val_acc: 0.9916\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.99100 to 0.99160, saving model to model_custom_v1_mnist_best.h5\n",
            "Epoch 6/30\n",
            "60000/60000 [==============================] - 21s 354us/step - loss: 0.0540 - acc: 0.9866 - val_loss: 0.0289 - val_acc: 0.9925\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.99160 to 0.99250, saving model to model_custom_v1_mnist_best.h5\n",
            "Epoch 7/30\n",
            "60000/60000 [==============================] - 22s 372us/step - loss: 0.0493 - acc: 0.9879 - val_loss: 0.0310 - val_acc: 0.9911\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.99250\n",
            "Epoch 8/30\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0434 - acc: 0.9885 - val_loss: 0.0285 - val_acc: 0.9917\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.99250\n",
            "Epoch 9/30\n",
            "60000/60000 [==============================] - 24s 396us/step - loss: 0.0411 - acc: 0.9892 - val_loss: 0.0253 - val_acc: 0.9930\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.99250 to 0.99300, saving model to model_custom_v1_mnist_best.h5\n",
            "Epoch 10/30\n",
            "60000/60000 [==============================] - 23s 376us/step - loss: 0.0352 - acc: 0.9909 - val_loss: 0.0334 - val_acc: 0.9911\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.99300\n",
            "Epoch 11/30\n",
            "60000/60000 [==============================] - 23s 376us/step - loss: 0.0336 - acc: 0.9912 - val_loss: 0.0328 - val_acc: 0.9896\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.99300\n",
            "Epoch 12/30\n",
            "60000/60000 [==============================] - 22s 368us/step - loss: 0.0309 - acc: 0.9916 - val_loss: 0.0286 - val_acc: 0.9918\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.99300\n",
            "Epoch 13/30\n",
            "60000/60000 [==============================] - 23s 376us/step - loss: 0.0290 - acc: 0.9927 - val_loss: 0.0267 - val_acc: 0.9924\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.99300\n",
            "Epoch 14/30\n",
            "60000/60000 [==============================] - 22s 370us/step - loss: 0.0282 - acc: 0.9919 - val_loss: 0.0232 - val_acc: 0.9932\n",
            "\n",
            "Epoch 00014: val_acc improved from 0.99300 to 0.99320, saving model to model_custom_v1_mnist_best.h5\n",
            "Epoch 15/30\n",
            "60000/60000 [==============================] - 23s 385us/step - loss: 0.0263 - acc: 0.9927 - val_loss: 0.0301 - val_acc: 0.9914\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.99320\n",
            "Epoch 16/30\n",
            "60000/60000 [==============================] - 23s 377us/step - loss: 0.0248 - acc: 0.9933 - val_loss: 0.0263 - val_acc: 0.9922\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.99320\n",
            "Epoch 17/30\n",
            "60000/60000 [==============================] - 22s 366us/step - loss: 0.0245 - acc: 0.9933 - val_loss: 0.0242 - val_acc: 0.9924\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.99320\n",
            "Epoch 18/30\n",
            "60000/60000 [==============================] - 22s 373us/step - loss: 0.0220 - acc: 0.9940 - val_loss: 0.0220 - val_acc: 0.9937\n",
            "\n",
            "Epoch 00018: val_acc improved from 0.99320 to 0.99370, saving model to model_custom_v1_mnist_best.h5\n",
            "Epoch 19/30\n",
            "60000/60000 [==============================] - 21s 352us/step - loss: 0.0226 - acc: 0.9933 - val_loss: 0.0289 - val_acc: 0.9915\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.99370\n",
            "Epoch 20/30\n",
            "60000/60000 [==============================] - 21s 351us/step - loss: 0.0194 - acc: 0.9945 - val_loss: 0.0221 - val_acc: 0.9941\n",
            "\n",
            "Epoch 00020: val_acc improved from 0.99370 to 0.99410, saving model to model_custom_v1_mnist_best.h5\n",
            "Epoch 21/30\n",
            "60000/60000 [==============================] - 21s 354us/step - loss: 0.0188 - acc: 0.9946 - val_loss: 0.0222 - val_acc: 0.9934\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.99410\n",
            "Epoch 22/30\n",
            "60000/60000 [==============================] - 22s 375us/step - loss: 0.0197 - acc: 0.9945 - val_loss: 0.0221 - val_acc: 0.9940\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.99410\n",
            "Epoch 23/30\n",
            "60000/60000 [==============================] - 23s 383us/step - loss: 0.0183 - acc: 0.9948 - val_loss: 0.0237 - val_acc: 0.9931\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.99410\n",
            "Epoch 24/30\n",
            "60000/60000 [==============================] - 21s 354us/step - loss: 0.0171 - acc: 0.9953 - val_loss: 0.0234 - val_acc: 0.9927\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.99410\n",
            "Epoch 25/30\n",
            "60000/60000 [==============================] - 22s 364us/step - loss: 0.0165 - acc: 0.9956 - val_loss: 0.0240 - val_acc: 0.9936\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.99410\n",
            "Epoch 26/30\n",
            "60000/60000 [==============================] - 22s 362us/step - loss: 0.0165 - acc: 0.9952 - val_loss: 0.0229 - val_acc: 0.9922\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.99410\n",
            "Epoch 27/30\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0158 - acc: 0.9958 - val_loss: 0.0212 - val_acc: 0.9937\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.99410\n",
            "Epoch 28/30\n",
            "60000/60000 [==============================] - 21s 349us/step - loss: 0.0152 - acc: 0.9962 - val_loss: 0.0228 - val_acc: 0.9933\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.99410\n",
            "Epoch 29/30\n",
            "60000/60000 [==============================] - 22s 370us/step - loss: 0.0139 - acc: 0.9961 - val_loss: 0.0239 - val_acc: 0.9928\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.99410\n",
            "Epoch 30/30\n",
            "60000/60000 [==============================] - 21s 349us/step - loss: 0.0146 - acc: 0.9960 - val_loss: 0.0211 - val_acc: 0.9929\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.99410\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f42a15863c8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAo7lmRA0FIE",
        "colab_type": "text"
      },
      "source": [
        "###We trained the model for 30 epochs and it has a max validation accuracy of 99.41. \n",
        "\n",
        "###We also observe that the time for each epoch has increased from 9 seconds to 21 seconds due to the Regularization computations involved in BatchNormalization.  \n",
        "\n",
        "### In the next iteration we will add a couple of more techniques like adding Dropouts to narrow the gap between training and validation accuracy . Since we will be using dropouts , we could also try a larger learning rate compared to the default of 0.001 for Adam optimizer "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7fa3y3MszKQ",
        "colab_type": "text"
      },
      "source": [
        "### Let us load the Model with best validation accuracy and print the evaluation score "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvdXCXK2l9KQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model=load_model(\"model_custom_v1_mnist_best.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtsH-lLk-eLb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score = model.evaluate(X_test, Y_test, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkX8JMv79q9r",
        "colab_type": "code",
        "outputId": "4037da7b-d4d1-4dbb-f65a-50d3848cf078",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(score)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.022091330846282654, 0.9941]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Acfgy31CtWYc",
        "colab_type": "text"
      },
      "source": [
        "### Predict the classes using model.predict and print predicted probabilities and categorical array for True test classes "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCWoJkwE9suh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym7iCFBm9uBs",
        "colab_type": "code",
        "outputId": "46265bf4-bad7-488e-e1ba-3cdc70edcc79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "print(y_pred[:9])\n",
        "print(Y_test[:9])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[4.4945727e-07 4.0951913e-06 1.5057884e-06 2.8017450e-06 6.4072306e-06\n",
            "  1.3115699e-07 9.2514611e-06 9.9997401e-01 2.4173877e-07 1.0559955e-06]\n",
            " [1.2424377e-07 8.9297311e-07 9.9988818e-01 1.2660471e-06 7.4542463e-08\n",
            "  7.2068240e-10 1.0782537e-04 1.9890088e-07 1.0866357e-06 2.3843123e-07]\n",
            " [1.4239157e-06 9.9986660e-01 7.7201298e-07 2.4174201e-06 8.2630077e-06\n",
            "  1.0227302e-05 3.5353700e-05 2.3723784e-05 4.8153110e-05 3.0213514e-06]\n",
            " [9.9771321e-01 3.6677851e-05 3.1524178e-04 8.1629776e-05 4.3937005e-05\n",
            "  3.0508163e-04 4.4671347e-04 3.7273247e-05 1.7200081e-04 8.4823120e-04]\n",
            " [4.0568311e-06 2.4899067e-05 2.5087045e-06 3.2747241e-06 9.9991345e-01\n",
            "  2.5871114e-07 2.6965649e-06 1.1384093e-06 5.7728462e-06 4.2022410e-05]\n",
            " [2.0638140e-06 9.9985600e-01 1.0458700e-06 7.0689674e-07 2.0332665e-05\n",
            "  4.0227533e-06 3.6387344e-05 6.0578077e-05 1.5782631e-05 3.0460528e-06]\n",
            " [4.5419633e-06 3.3019594e-05 1.8524504e-05 2.7431017e-07 9.9983549e-01\n",
            "  1.2330828e-05 4.1305480e-05 1.3740265e-06 4.2140287e-05 1.1048961e-05]\n",
            " [3.1802905e-05 1.8950957e-05 1.3876226e-04 5.1905296e-05 2.0075464e-05\n",
            "  1.0004818e-03 5.9949241e-07 5.4669972e-06 3.1556777e-04 9.9841642e-01]\n",
            " [3.4055321e-03 1.7826549e-04 8.9902867e-04 2.3617860e-05 8.4702311e-05\n",
            "  3.2483986e-01 6.6629219e-01 2.1179800e-03 2.0983836e-03 6.0438135e-05]]\n",
            "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYPrkzLvt2do",
        "colab_type": "text"
      },
      "source": [
        "### Let us visualize some of the filters in the first convolution layer 'conv2d_1'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CT--y98_dr2T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# form a layer dictionary {name : layer} of all layers in the model \n",
        "\n",
        "layer_dict = dict([(layer.name, layer) for layer in model.layers])  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GY4Upv4dsUR",
        "colab_type": "code",
        "outputId": "a3decc00-9799-4d24-8982-00f3d6c54e0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 771
        }
      },
      "source": [
        " # use matplotlib to visualize the filter arrays \n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from keras import backend as K\n",
        "%matplotlib inline\n",
        "# util function to convert a tensor into a valid image\n",
        "def deprocess_image(x):\n",
        "    # normalize tensor: center on 0., ensure std is 0.1\n",
        "    x -= x.mean()\n",
        "    x /= (x.std() + 1e-5)\n",
        "    x *= 0.1\n",
        "\n",
        "    # clip to [0, 1]\n",
        "    x += 0.5\n",
        "    x = np.clip(x, 0, 1)\n",
        "\n",
        "    # convert to RGB array\n",
        "    x *= 255\n",
        "    #x = x.transpose((1, 2, 0))\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x\n",
        "\n",
        "def vis_img_in_filter(img = np.array(X_train[2]).reshape((1, 28, 28, 1)).astype(np.float64), \n",
        "                      layer_name = 'conv2d_1'):\n",
        "    layer_output = layer_dict[layer_name].output\n",
        "    img_ascs = list()\n",
        "    for filter_index in range(layer_output.shape[3]):\n",
        "        # build a loss function that maximizes the activation\n",
        "        # of the nth filter of the layer considered\n",
        "        loss = K.mean(layer_output[:, :, :, filter_index])\n",
        "\n",
        "        # compute the gradient of the input picture wrt this loss\n",
        "        grads = K.gradients(loss, model.input)[0]\n",
        "\n",
        "        # normalization trick: we normalize the gradient\n",
        "        grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)\n",
        "\n",
        "        # this function returns the loss and grads given the input picture\n",
        "        iterate = K.function([model.input], [loss, grads])\n",
        "\n",
        "        # step size for gradient ascent\n",
        "        step = 5.\n",
        "\n",
        "        img_asc = np.array(img)\n",
        "        # run gradient ascent for 20 steps\n",
        "        for i in range(20):\n",
        "            loss_value, grads_value = iterate([img_asc])\n",
        "            img_asc += grads_value * step\n",
        "\n",
        "        img_asc = img_asc[0]\n",
        "        img_ascs.append(deprocess_image(img_asc).reshape((28, 28)))\n",
        "        \n",
        "    if layer_output.shape[3] >= 35:\n",
        "        plot_x, plot_y = 6, 6\n",
        "    elif layer_output.shape[3] >= 23:\n",
        "        plot_x, plot_y = 4, 6\n",
        "    elif layer_output.shape[3] >= 11:\n",
        "        plot_x, plot_y = 2, 6\n",
        "    elif layer_output.shape[3] >= 8:\n",
        "        plot_x, plot_y = 2, 4   \n",
        "    else:\n",
        "        \n",
        "        plot_x, plot_y = 2, 2\n",
        "    fig, ax = plt.subplots(plot_x, plot_y, figsize = (12, 12))\n",
        "    \n",
        "    ax[0,0].imshow(img.reshape((28, 28)), cmap = 'gray')\n",
        "    ax[0,0].set_title('Input image')\n",
        "    fig.suptitle('Input image and %s filters' % (layer_name,))\n",
        "    fig.tight_layout(pad = 0.3, rect = [0, 0, 0.9, 0.9])\n",
        "    for (x, y) in [(i, j) for i in range(plot_x) for j in range(plot_y)]:\n",
        "        if x == 0 and y == 0:\n",
        "            continue\n",
        "        ax[x,y].imshow(img_ascs[x * plot_y + y - 1], cmap = 'gray')\n",
        "        ax[x,y].set_title('filter %d' % (x * plot_y + y - 1))\n",
        "\n",
        "vis_img_in_filter()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwcAAALyCAYAAACPcKhRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIInmAAAA6GgOAACAJMn/ACBNANPuP7raAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 864x864 with 8 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2q6fJ67qvlsY",
        "colab_type": "text"
      },
      "source": [
        "### We have now completed the third step in building our model where we have crossed the necessary accuracy of 99.4. \n",
        "\n",
        "###We used BatchNormalization in this step to improve the model and reached a max validation accuracy of 99.41 in the 20th epoch. \n",
        "\n",
        "###But there might still be room for improvement in this model and so we will try a few more improvement techniques like Dropouts and tweaking learning rate in the next iteration  \n",
        "\n"
      ]
    }
  ]
}