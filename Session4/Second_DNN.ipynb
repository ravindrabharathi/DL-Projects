{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Second.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ravindrabharathi/Project1/blob/master/Session4/Second_DNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBzDI9HEsAmu",
        "colab_type": "text"
      },
      "source": [
        "# Build a Convolutional Neural Network with less than 15000 parameters to achieve a validation accuracy of 99.4 or more for MNIST dataset \n",
        "\n",
        "\n",
        "The target is to build a deep learning CNN model with as little parameters as possible and at the same time achieve a high validation accuracy of 99.4 or more . The low parameter count becomes important when deploying the model in memory constrained devices used in edge computing . MNIST is one of the more popular (and simpler) datasets to begin your journey in Vision based Deep learning. We will use this MNIST dataset for this exercise. \n",
        "\n",
        "We will build the model step by step . Broadly speaking we will follow these steps \n",
        "\n",
        "1. Decide on the basic architecture for the network \n",
        "2. Fine tune parameters to comply with the 15000 limit\n",
        "3. Add improvements to the network using Batch Normalization \n",
        "4. See if we can converge faster while learning by using Dropouts to overcome overfitting ,higher   learning rates, etc \n",
        "\n",
        "We are now at step 2 .\n",
        "\n",
        "In the [previous iteration](https://github.com/ravindrabharathi/Project1/blob/master/Session4/First_DNN.ipynb)  we fixed the basic architecture without bothering too much about the number of parameters . In this iteration we will bring the parameters to within the required limit of 15k . We will do this by reducing the number of kernels in each layer but the architecture will remain the same ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0uvqzIGp5zc",
        "colab_type": "text"
      },
      "source": [
        "###Import necessary libraries / modules\n",
        "Import numpy library for array/ matrix operations\n",
        "\n",
        "Import Sequential Model from keras/models for building the model\n",
        "\n",
        "Import Conv2D , Activation , Flatten , BatchNormalization, MaxPooling2D from keras/layers \n",
        "\n",
        "Import np_utils module from keras/utils for numpy related helper functions\n",
        "\n",
        "Import mnist dataset containing hand-written digits images from keras.datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eso6UHE080D4",
        "colab_type": "code",
        "outputId": "16ba01bf-2e82-45fb-d634-608650ce0676",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential,load_model\n",
        "from keras.layers import Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zByEi95J86RD",
        "colab_type": "text"
      },
      "source": [
        "### Load pre-shuffled MNIST data into train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eRM0QWN83PV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db3hDvFDqpS9",
        "colab_type": "text"
      },
      "source": [
        "###print the shape of training data and also inspect the first image using matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a4Be72j8-ZC",
        "colab_type": "code",
        "outputId": "eaf7b8a5-c3ec-4f3a-9e35-1ba5ac217266",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "source": [
        "print (X_train.shape)\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.imshow(X_train[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fd5dfb29198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7BHSzLjq6nT",
        "colab_type": "text"
      },
      "source": [
        "####Reshape the training and test dataset to include the channel information.In this case it is a greyscale image and so there is 1 channel . the image data was read in as a 28x28 numpy array and is now reshaped to 28x28x1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkmprriw9AnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3aPHupgrF5a",
        "colab_type": "text"
      },
      "source": [
        "###Cast training data as float32 and normalize/re-scale the values such that they are between 0 and 1 instead of 0 and 255"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2m4YS4E9CRh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmUE1nCXrMua",
        "colab_type": "text"
      },
      "source": [
        "###inspect the first 10 training class labels . They will be some number between 0 and 9 representing the hand-written digit in the corresponding Training data. Each of 0 to 9 represents a class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Mn0vAYD9DvB",
        "colab_type": "code",
        "outputId": "fef19cdd-6e06-4361-e391-4d6581e7e8d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_train[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dld_th_rU9s",
        "colab_type": "text"
      },
      "source": [
        "####One hot encoding of training and test class labels : Convert 1-dimensional class arrays to 10-dimensional class matrices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG8JiXR39FHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYlFRvKS9HMB",
        "colab_type": "code",
        "outputId": "a3e348a2-69ff-4d7f-d3a8-b8a51907c3c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "Y_train[:10]\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNM1NgZdtQm8",
        "colab_type": "text"
      },
      "source": [
        "###Define a ModelCheckPoint callback which will be called at the end of every training epoch . We will use this callback function to save the model whenever vallidation accuracy improves . We do this so that we can load and use the best model for further predictions after training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Yr6tsrzcSce",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "  \n",
        "#chkpoint_model=ModelCheckpoint(\"/gdrive/My Drive/EVA/Session3/model_customv1_mnist_best.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='max') \n",
        "\n",
        "chkpoint_model=ModelCheckpoint(\"model_custom_v1_mnist_best.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='max') \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmyGmS8oy2qE",
        "colab_type": "text"
      },
      "source": [
        "#Building the model version 2\n",
        "\n",
        "In version 2 of the model we will try and get the number of parameters down below the 15k limit \n",
        "\n",
        "We will reduce the number of kernels in the following manner\n",
        "\n",
        "First convolution layer :\n",
        "\n",
        "The first convolution layer will have 10 numbers 3x3 kernels instead of 32 \n",
        "\n",
        "First Convolution Block :\n",
        "\n",
        "The number of kernels in the first layer will be changed to 12 instead of 64 and the second layer will now have 16 kernels instead of 128 kernels . With a global receptive field of 7x7 , the network should be able to learn enough about edges and gradients . \n",
        "\n",
        "Transition Block : \n",
        "since we want to start the next convolution block at a smaller number of kernels , we will use a 1x1 convolution layer to reduce the number of channels after the first convolution block . 1x1 kernel convolution is an effective  way of combining a large number of channels to form a set of smaller number of channels. We will use 10 numbers of 1x1 kernels to bring the number of channels down to 10\n",
        "We will also spatially downsample the channels by using maxpooling of size 2x2. Maxpooling of size 2x2 will reduce the channel size by half while doubling the global receptive field \n",
        "\n",
        "Second convolution block : \n",
        "\n",
        "The second block will allow the network to form the important parts that make up the digits . This will contain two layers of 3x3 kernels . First layer will have 12 kernels (down from 64) and the second will have 16 kernels (down from 128 kernels ) . \n",
        "\n",
        "1x1 convolution to transition to 10 channels :\n",
        "\n",
        "After the second convolution block we add a convolution layer of 10 1x1 kernels .  Since we have only have 10 classes , we will combine the 16 channels from earlier layers to form 10 channels .\n",
        "\n",
        "Last Layer : 7x7 kernel \n",
        "\n",
        "We will retain last layer with the 7x7 kernel convolution \n",
        "It is important not to have ReLU activation for this 1x1 layer since we want all values from the convolution to go to the Softmax activation to make its prediction . If we use a ReLu activation , the -ve values will be suppressed and the network will be unable to train in an optimal manner.\n",
        "\n",
        "Flatten: \n",
        "\n",
        "These 10 channel outputs are fed to a Flatten layer that converts the 2d array representation to a 1d shape . \n",
        "\n",
        "Softmax activation: \n",
        "\n",
        "A softmax activation layer at the end outputs the class probabilities of these 10 classes which in our case are the digits 0 to 9 . \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rdn9MvWmVOtA",
        "colab_type": "code",
        "outputId": "a2f4e7ca-68c2-4bb6-f98f-2b9dbec55ebf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# to get a certain degeree of predictability when generating random numbers,\n",
        "# set a random seed to initialize the pseudo-random number generator \n",
        "np.random.seed(seed=42)  \n",
        "\n",
        "# instantiate a sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# add the first convolution layer - 10 numbers of 3x3 filters , \n",
        "#This layer sees the input image of 28x28 x 1 channel . \n",
        "\n",
        "model.add(Conv2D(10, (3, 3), input_shape=(28,28,1), use_bias=False))  # remove bias param by setting it to false \n",
        "  \n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "\n",
        "# Now the global receptive field is 3 x 3 \n",
        "\n",
        "# First Convolution Block\n",
        "# Block1 conv layer 1 - 12 filters of shape  3x3x10 \n",
        "# input from previous layer is 26 x 26 x 10 . \n",
        "\n",
        "## Block 1\n",
        "\n",
        "model.add(Conv2D(12, 3, use_bias=False))  # remove bias param by setting it to false \n",
        " \n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "\n",
        "#Global receptive field is 5x5\n",
        "\n",
        "# Add convolution layer - 16 filters of shape  3x3x12 \n",
        "#input from previous layer is 24 x 24 x 12 . \n",
        "\n",
        "model.add(Conv2D(16, 3, use_bias=False))  # remove bias param by setting it to false \n",
        "\n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "\n",
        "\n",
        "#Global receptive field is 7x7\n",
        "\n",
        "##  Transition block \n",
        "\n",
        "# Perform 2x2 max pooling  . \n",
        "#Input from previous layer is 22 x 22 X 16 \n",
        "\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "\n",
        "# After max pooling , 2D spatial dimension reduces by half , i.e it becomes 11 x 11 . \n",
        "# Maxpooling (withpool size 2 and stride 1) doubles receptive field . \n",
        "# So global receptive field after max pooling is 14 x 14\n",
        "\n",
        "# add 1x1 convolution to reduce the channel numbers to 10 \n",
        "\n",
        "model.add(Conv2D(10, 1, use_bias=False))  # remove bias param by setting it to false \n",
        "\n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "\n",
        "\n",
        "#Convolution Block 2\n",
        "\n",
        "# Add convolution layer - 12 filters of shape 3x3x10\n",
        "#input from transition layer is 11 x 11 x 10 .  \n",
        "\n",
        "model.add(Conv2D(12, 3,  use_bias=False))  # remove bias param by setting it to false \n",
        "\n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "\n",
        "#Global receptive field is now 16 x 16 \n",
        "\n",
        "# Add convolution layer - 16 filters of shape 3x3x12 \n",
        "#input coming from previous layer is 9 x 9 x 12 . \n",
        "\n",
        "model.add(Conv2D(16, 3,  use_bias=False)) # remove bias param by setting it to false \n",
        " \n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "\n",
        "#  Global receptive field is now 18 x 18 \n",
        "\n",
        "# Add 1x1 convolution to reduce number of channels to 10  \n",
        "#input coming from previous layer is 7 x 7 x 16 .\n",
        "\n",
        "model.add(Conv2D(10, 1,  use_bias=False))  # remove bias param by setting it to false \n",
        "\n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "\n",
        "#Global receptive field is now 18 x 18 \n",
        "\n",
        "# Last layer :  Add convolution layer - 10 filters of shape 7x7x10 \n",
        "#input coming from previous layer is 7 x 7 x 10 .\n",
        "\n",
        "model.add(Conv2D(10, 7,  use_bias=False))  # remove bias param by setting it to false \n",
        "# Note absence of ReLU activation here \n",
        "\n",
        "model.add(Flatten())  # Flatten the 2d array to 1d input for the softmax activation \n",
        "model.add(Activation('softmax'))   # Softmax activation to out class probabilities "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTdVH726xhN3",
        "colab_type": "code",
        "outputId": "aab4e975-4819-4743-ecfb-c8da00da8e43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 26, 26, 10)        90        \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 26, 26, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 24, 24, 12)        1080      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 24, 24, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 22, 22, 16)        1728      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 22, 22, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 11, 11, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 11, 11, 10)        160       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 11, 11, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 9, 9, 12)          1080      \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 9, 9, 12)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 7, 7, 16)          1728      \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 7, 7, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 7, 7, 10)          160       \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 7, 7, 10)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 1, 1, 10)          4900      \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 10,926\n",
            "Trainable params: 10,926\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7A0AhG_xrjw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqappyT1x7sf",
        "colab_type": "code",
        "outputId": "50a8bc3d-0037-4e5d-d2c3-dddd06468276",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1482
        }
      },
      "source": [
        "model.fit(X_train, Y_train, validation_data=(X_test,Y_test),batch_size=32, epochs=20, verbose=1, callbacks=[chkpoint_model])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 11s 182us/step - loss: 0.1986 - acc: 0.9375 - val_loss: 0.0709 - val_acc: 0.9785\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.97850, saving model to model_custom_v1_mnist_best.h5\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 9s 145us/step - loss: 0.0659 - acc: 0.9799 - val_loss: 0.0463 - val_acc: 0.9856\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.97850 to 0.98560, saving model to model_custom_v1_mnist_best.h5\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 9s 145us/step - loss: 0.0506 - acc: 0.9845 - val_loss: 0.0397 - val_acc: 0.9867\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.98560 to 0.98670, saving model to model_custom_v1_mnist_best.h5\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 10s 161us/step - loss: 0.0398 - acc: 0.9875 - val_loss: 0.0358 - val_acc: 0.9874\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.98670 to 0.98740, saving model to model_custom_v1_mnist_best.h5\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 9s 149us/step - loss: 0.0350 - acc: 0.9889 - val_loss: 0.0300 - val_acc: 0.9900\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.98740 to 0.99000, saving model to model_custom_v1_mnist_best.h5\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 9s 145us/step - loss: 0.0303 - acc: 0.9906 - val_loss: 0.0304 - val_acc: 0.9909\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.99000 to 0.99090, saving model to model_custom_v1_mnist_best.h5\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 9s 145us/step - loss: 0.0276 - acc: 0.9910 - val_loss: 0.0314 - val_acc: 0.9900\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.99090\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 9s 144us/step - loss: 0.0242 - acc: 0.9924 - val_loss: 0.0305 - val_acc: 0.9909\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.99090\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 9s 145us/step - loss: 0.0220 - acc: 0.9929 - val_loss: 0.0306 - val_acc: 0.9902\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.99090\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 9s 144us/step - loss: 0.0198 - acc: 0.9932 - val_loss: 0.0403 - val_acc: 0.9881\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.99090\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 9s 144us/step - loss: 0.0177 - acc: 0.9941 - val_loss: 0.0345 - val_acc: 0.9900\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.99090\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 9s 144us/step - loss: 0.0158 - acc: 0.9949 - val_loss: 0.0374 - val_acc: 0.9894\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.99090\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 10s 160us/step - loss: 0.0146 - acc: 0.9950 - val_loss: 0.0334 - val_acc: 0.9911\n",
            "\n",
            "Epoch 00013: val_acc improved from 0.99090 to 0.99110, saving model to model_custom_v1_mnist_best.h5\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 9s 154us/step - loss: 0.0136 - acc: 0.9952 - val_loss: 0.0300 - val_acc: 0.9910\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.99110\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 9s 144us/step - loss: 0.0136 - acc: 0.9956 - val_loss: 0.0326 - val_acc: 0.9912\n",
            "\n",
            "Epoch 00015: val_acc improved from 0.99110 to 0.99120, saving model to model_custom_v1_mnist_best.h5\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 9s 145us/step - loss: 0.0120 - acc: 0.9960 - val_loss: 0.0320 - val_acc: 0.9907\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.99120\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 9s 145us/step - loss: 0.0117 - acc: 0.9961 - val_loss: 0.0396 - val_acc: 0.9906\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.99120\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 9s 144us/step - loss: 0.0099 - acc: 0.9966 - val_loss: 0.0341 - val_acc: 0.9915\n",
            "\n",
            "Epoch 00018: val_acc improved from 0.99120 to 0.99150, saving model to model_custom_v1_mnist_best.h5\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 9s 145us/step - loss: 0.0118 - acc: 0.9962 - val_loss: 0.0347 - val_acc: 0.9912\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.99150\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 9s 145us/step - loss: 0.0089 - acc: 0.9972 - val_loss: 0.0290 - val_acc: 0.9917\n",
            "\n",
            "Epoch 00020: val_acc improved from 0.99150 to 0.99170, saving model to model_custom_v1_mnist_best.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd5dc9896a0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAo7lmRA0FIE",
        "colab_type": "text"
      },
      "source": [
        "**We trained the model for 20 epochs and it has a max validation accuracy of 99.17. The number of parameters is now within limits at 10,926 and there is no danger of this going beyond 15k even if we add other improvements like BatchNormalization to this network. In the next iteration we will add some regularization to the activation channels by using BatchNormalization for all Convolutions** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7fa3y3MszKQ",
        "colab_type": "text"
      },
      "source": [
        "### Let us load the Model with best validation accuracy and print the evaluation score "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvdXCXK2l9KQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model=load_model(\"model_custom_v1_mnist_best.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtsH-lLk-eLb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score = model.evaluate(X_test, Y_test, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkX8JMv79q9r",
        "colab_type": "code",
        "outputId": "fa5426da-99ca-4e22-ac30-66a78ddb1982",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(score)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.028998460919291848, 0.9917]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Acfgy31CtWYc",
        "colab_type": "text"
      },
      "source": [
        "### Predict the classes using model.predict and print predicted probabilities and categorical array for True test classes "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCWoJkwE9suh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym7iCFBm9uBs",
        "colab_type": "code",
        "outputId": "9ec1e676-210f-4b4b-9c31-a10a7f2e0876",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "print(y_pred[:9])\n",
        "print(Y_test[:9])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[7.1328967e-16 1.6497064e-18 7.0145300e-13 8.6290208e-10 2.9231091e-21\n",
            "  6.5024591e-16 2.6110504e-27 1.0000000e+00 5.7033545e-15 1.2486217e-11]\n",
            " [1.4566471e-14 5.4250791e-11 1.0000000e+00 4.7123649e-15 2.4019770e-17\n",
            "  7.3914262e-22 1.0524716e-12 4.3250759e-19 2.7840074e-15 1.6577724e-21]\n",
            " [1.5629359e-12 9.9996650e-01 1.2259686e-06 1.3096528e-09 5.3383566e-08\n",
            "  1.4841415e-09 1.2044962e-10 3.2295186e-05 5.8488494e-09 2.0099948e-08]\n",
            " [9.9999809e-01 1.9170551e-13 6.2805117e-12 1.0294146e-14 2.1651891e-13\n",
            "  1.1058679e-10 1.9191548e-06 1.0302653e-10 3.3271738e-12 3.6333863e-11]\n",
            " [6.0134104e-16 2.0842046e-16 5.7508116e-16 3.5434508e-13 9.9999762e-01\n",
            "  1.9671525e-16 2.5608188e-17 1.3860315e-14 1.7122700e-12 2.3725204e-06]\n",
            " [1.5014568e-12 9.9983716e-01 3.4400452e-07 1.1561541e-11 8.4603471e-08\n",
            "  2.9480183e-13 1.2652894e-12 1.6210828e-04 8.6853253e-09 1.7753491e-07]\n",
            " [3.7513060e-25 3.6012307e-17 1.0476508e-12 3.2797100e-16 1.0000000e+00\n",
            "  1.1060327e-17 1.3964237e-25 1.1748590e-11 1.3433016e-10 1.1121396e-10]\n",
            " [1.7661083e-15 6.3786706e-12 3.1531932e-12 4.3364290e-09 5.2480306e-07\n",
            "  2.3315047e-10 9.4840941e-19 5.0723449e-12 6.0620148e-10 9.9999952e-01]\n",
            " [1.2485057e-09 7.2026136e-17 8.1046621e-11 1.3405331e-08 2.5279434e-15\n",
            "  9.8434967e-01 1.5598262e-02 1.4222942e-16 5.2122239e-05 2.5998262e-10]]\n",
            "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYPrkzLvt2do",
        "colab_type": "text"
      },
      "source": [
        "### Let us visualize some of the filters in the first convolution layer 'conv2d_1'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CT--y98_dr2T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# form a layer dictionary {name : layer} of all layers in the model \n",
        "\n",
        "layer_dict = dict([(layer.name, layer) for layer in model.layers])  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GY4Upv4dsUR",
        "colab_type": "code",
        "outputId": "134d231b-fd17-4562-dee8-406834fc000c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 771
        }
      },
      "source": [
        " # use matplotlib to visualize the filter arrays \n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from keras import backend as K\n",
        "%matplotlib inline\n",
        "# util function to convert a tensor into a valid image\n",
        "def deprocess_image(x):\n",
        "    # normalize tensor: center on 0., ensure std is 0.1\n",
        "    x -= x.mean()\n",
        "    x /= (x.std() + 1e-5)\n",
        "    x *= 0.1\n",
        "\n",
        "    # clip to [0, 1]\n",
        "    x += 0.5\n",
        "    x = np.clip(x, 0, 1)\n",
        "\n",
        "    # convert to RGB array\n",
        "    x *= 255\n",
        "    #x = x.transpose((1, 2, 0))\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x\n",
        "\n",
        "def vis_img_in_filter(img = np.array(X_train[2]).reshape((1, 28, 28, 1)).astype(np.float64), \n",
        "                      layer_name = 'conv2d_1'):\n",
        "    layer_output = layer_dict[layer_name].output\n",
        "    img_ascs = list()\n",
        "    for filter_index in range(layer_output.shape[3]):\n",
        "        # build a loss function that maximizes the activation\n",
        "        # of the nth filter of the layer considered\n",
        "        loss = K.mean(layer_output[:, :, :, filter_index])\n",
        "\n",
        "        # compute the gradient of the input picture wrt this loss\n",
        "        grads = K.gradients(loss, model.input)[0]\n",
        "\n",
        "        # normalization trick: we normalize the gradient\n",
        "        grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)\n",
        "\n",
        "        # this function returns the loss and grads given the input picture\n",
        "        iterate = K.function([model.input], [loss, grads])\n",
        "\n",
        "        # step size for gradient ascent\n",
        "        step = 5.\n",
        "\n",
        "        img_asc = np.array(img)\n",
        "        # run gradient ascent for 20 steps\n",
        "        for i in range(20):\n",
        "            loss_value, grads_value = iterate([img_asc])\n",
        "            img_asc += grads_value * step\n",
        "\n",
        "        img_asc = img_asc[0]\n",
        "        img_ascs.append(deprocess_image(img_asc).reshape((28, 28)))\n",
        "        \n",
        "    if layer_output.shape[3] >= 35:\n",
        "        plot_x, plot_y = 6, 6\n",
        "    elif layer_output.shape[3] >= 23:\n",
        "        plot_x, plot_y = 4, 6\n",
        "    elif layer_output.shape[3] >= 11:\n",
        "        plot_x, plot_y = 2, 6\n",
        "    elif layer_output.shape[3] >= 8:\n",
        "        plot_x, plot_y = 2, 4   \n",
        "    else:\n",
        "        \n",
        "        plot_x, plot_y = 2, 2\n",
        "    fig, ax = plt.subplots(plot_x, plot_y, figsize = (12, 12))\n",
        "    \n",
        "    ax[0,0].imshow(img.reshape((28, 28)), cmap = 'gray')\n",
        "    ax[0,0].set_title('Input image')\n",
        "    fig.suptitle('Input image and %s filters' % (layer_name,))\n",
        "    fig.tight_layout(pad = 0.3, rect = [0, 0, 0.9, 0.9])\n",
        "    for (x, y) in [(i, j) for i in range(plot_x) for j in range(plot_y)]:\n",
        "        if x == 0 and y == 0:\n",
        "            continue\n",
        "        ax[x,y].imshow(img_ascs[x * plot_y + y - 1], cmap = 'gray')\n",
        "        ax[x,y].set_title('filter %d' % (x * plot_y + y - 1))\n",
        "\n",
        "vis_img_in_filter()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwcAAALyCAYAAACPcKhRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ntIrWaqjmAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 864x864 with 8 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2q6fJ67qvlsY",
        "colab_type": "text"
      },
      "source": [
        "### We have now completed the second step in building our model optimized for memory constrained devices. We are at comfortable 11k mark for the number of parameters but we will have to improve the validation score by using regularization techniques like BatchNormalization  in the next iteration \n",
        "\n"
      ]
    }
  ]
}