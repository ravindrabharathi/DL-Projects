{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1st DNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBzDI9HEsAmu",
        "colab_type": "text"
      },
      "source": [
        "# Build a Convolutional Neural Network with less than 20000 parameters to achieve a validation accuracy of 99.4 or more for MNIST dataset \n",
        "\n",
        "\n",
        "The target is to build a deep learning CNN model with as little parameters as possible and at the same time achieve a high validation accuracy of 99.4 or more . The low parameter count becomes important when deploying the model in memory constrained devices used in edge computing . MNIST is one of the more popular (and simpler) datasets to begin your journey in Vision based Deep learning. We will use this dataset for this exercise. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNyZv-Ec52ot",
        "colab_type": "text"
      },
      "source": [
        "###**Import Libraries and modules**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfPw4auKppc9",
        "colab_type": "text"
      },
      "source": [
        "###Install Keras framework (use -q option to do a quiet install) and import keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m3w1Cw49Zkt",
        "colab_type": "code",
        "outputId": "074f0fd7-6252-4388-b546-c71741d07e10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# https://keras.io/\n",
        "!pip install -q keras\n",
        "import keras"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0uvqzIGp5zc",
        "colab_type": "text"
      },
      "source": [
        "###Import other necessary libraries / modules\n",
        "Import numpy library for array/ matrix operations\n",
        "\n",
        "Import Sequential Model from keras/models for building the model\n",
        "\n",
        "Import Conv2D , Activation , Flatten , BatchNormalization, MaxPooling2D from keras/layers \n",
        "\n",
        "Import np_utils module from keras/utils for numpy related helper functions\n",
        "\n",
        "Import mnist dataset containing hand-written digits images from keras.datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eso6UHE080D4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential,load_model\n",
        "from keras.layers import Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zByEi95J86RD",
        "colab_type": "text"
      },
      "source": [
        "### Load pre-shuffled MNIST data into train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eRM0QWN83PV",
        "colab_type": "code",
        "outputId": "d524f7d1-6a7d-497c-d0cb-77958153698d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db3hDvFDqpS9",
        "colab_type": "text"
      },
      "source": [
        "###print the shape of training data and also inspect the first image using matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a4Be72j8-ZC",
        "colab_type": "code",
        "outputId": "83866d0e-f32b-40ca-bc4b-d0b8ba103d8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "source": [
        "print (X_train.shape)\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.imshow(X_train[0])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f2e83aefd68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7BHSzLjq6nT",
        "colab_type": "text"
      },
      "source": [
        "####Reshape the training and test dataset to include the channel information.In this case it is a greyscale image and so there is 1 channel . the image data was read in as a 28x28 numpy array and is now reshaped to 28x28x1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkmprriw9AnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3aPHupgrF5a",
        "colab_type": "text"
      },
      "source": [
        "###Cast training data as float32 and normalize/re-scale the values such that they are between 0 and 1 instead of 0 and 255"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2m4YS4E9CRh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmUE1nCXrMua",
        "colab_type": "text"
      },
      "source": [
        "###inspect the first 10 training class labels . They will be some number between 0 and 9 representing the hand-written digit in the corresponding Training data. Each of 0 to 9 represents a class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Mn0vAYD9DvB",
        "colab_type": "code",
        "outputId": "6b0bc19b-b7a9-48dd-8299-3a55bbd8af4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_train[:10]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dld_th_rU9s",
        "colab_type": "text"
      },
      "source": [
        "####One hot encoding of training and test class labels : Convert 1-dimensional class arrays to 10-dimensional class matrices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG8JiXR39FHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYlFRvKS9HMB",
        "colab_type": "code",
        "outputId": "ca2a6a0e-c156-4928-d270-68d0645c1c46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "Y_train[:10]\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aK9JKOndrmh7",
        "colab_type": "text"
      },
      "source": [
        "###Mount Google Drive to save model files \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ld96Famhjg1w",
        "colab_type": "code",
        "outputId": "dc6e6fc8-9c95-472d-d507-a7fe0987c013",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "def mount_drive():\n",
        "  from google.colab import drive\n",
        "  drive.mount('/gdrive',force_remount=True)\n",
        "\n",
        "mount_drive()\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNM1NgZdtQm8",
        "colab_type": "text"
      },
      "source": [
        "###Define a ModelCheckPoint callback which will be called at the end of every training epoch . We will use this callback function to save the model whenever vallidation accuracy improves . We do this so that we can load and use the best model for further predictions after training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Yr6tsrzcSce",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "  \n",
        "chkpoint_model=ModelCheckpoint(\"/gdrive/My Drive/EVA/Session3/model_customv1_mnist_best.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='max')  \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmyGmS8oy2qE",
        "colab_type": "text"
      },
      "source": [
        "#Building the model version 1\n",
        "\n",
        "We will try a model that starts with a convolution layer containing 8 numbers of  3x3 size  kernels and repeat this 2 more times . \n",
        "\n",
        "All these convolution layers will use ReLU activation function . Using an activation function introduces non-linearity in the network allowing them to learn complex functions .Without these non-linear activation functions the network will only be a stack of linear functions . ReLU activation is one of the simplest and most popular activations used in CNN . It essentially suppresses -ve values from moving forward giving the network a simple rule for retaining or discarding features that it is learning - work towards making values positive if you want something retained and make the values negative if you want to drop something. \n",
        "\n",
        "So we now have 3 convolution layers containing 8 numbers of 3x3 filters . At this point , we have a global receptive field of 7x7 . It is always good to build a network that has a global receptive field equal to or more than the size of the object that you are trying to  detect / learn \n",
        "\n",
        "Let us do some down-sampling using Maxpooling . We will use pool size of 2x2 . This reduces the channel dimensions to half and doubles the global receptive field . \n",
        "\n",
        "After maxpooling , let us add 5 more convolution layers of size 3x3 with ReLU activation. Each layer will have 16 kernels . \n",
        "\n",
        "\n",
        "Finally add a 1x1 convolution layer of 10 filters . 1x1 kernel convolution is an effective  way of combining a large number of channels to form a set of smaller number of channels.  Since we have only 10 classes , we will combine the 16 channels from earlier layers to form 10 channels . It is important not to have ReLU activation for this 1x1 layer since we want all values from the convolution to go to the Softmax activation to make its prediction . If we use a ReLu activation , the -ve values will be suppressed and the network will be unable to train in an optimal manner.\n",
        "\n",
        "These 10 channel outputs are fed to a Flatten layer that converts the 2d array representation to a 1d shape . \n",
        "\n",
        "A softmax activation layer at the end outputs the class probabilities of these 10 classes which in our case are the digits 0 to 9 . \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rdn9MvWmVOtA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "48c13332-6842-4dac-9cf8-418a9eb8f55d"
      },
      "source": [
        "np.random.seed(seed=42)  # set a random seed for reproducing random values \n",
        "\n",
        "# instantiate a sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# add the first convolution layer - 8 numbers of 3x3 filters , \n",
        "#This layer sees the input image of 28x28 x 1 channel . \n",
        "\n",
        "model.add(Conv2D(8, (3, 3), input_shape=(28,28,1), use_bias=False))  # remove bias param by setting it to false \n",
        "  \n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "\n",
        "# Now the global receptive field is 3 x 3 \n",
        "\n",
        "# second convolution layer - 8 filters of shape  3x3x8 \n",
        "#input from previous layer is 26 x 26 x 8 . \n",
        "\n",
        "model.add(Conv2D(8, 3, use_bias=False))  # remove bias param by setting it to false \n",
        " \n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "\n",
        "#Global receptive field is 5x5\n",
        "\n",
        "# Add convolution layer - 8 filters of shape  3x3x8 \n",
        "#input from previous layer is 24 x 24 x 8 . \n",
        "\n",
        "model.add(Conv2D(8, 3, use_bias=False))  # remove bias param by setting it to false \n",
        " \n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "\n",
        "\n",
        "#Global receptive field is 7x7\n",
        "\n",
        "# Perform 2x2 max pooling  . \n",
        "#Input from previous layer is 22 x 22 X 8 \n",
        "\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "\n",
        "# After max pooling , dimensions reduce by half , i.e they become 11 x 11 . \n",
        "# Maxpooling (withpool size 2 and stride 1) doubles receptive field . \n",
        "# So global receptive field after max pooling is 14 x 14\n",
        "\n",
        "# Add convolution layer - 16 filters of shape 3x3x8 \n",
        "#input from max pooling operation is 11 x 11 x 8 .  \n",
        "\n",
        "model.add(Conv2D(16, 3,  use_bias=False))  # remove bias param by setting it to false \n",
        "\n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "\n",
        "#Global receptive field is now 16 x 16 \n",
        "\n",
        "# Add convolution layer - 16 filters of shape 3x3x16 \n",
        "#input coming from previous layer is 9 x 9 x 16 . \n",
        "\n",
        "model.add(Conv2D(16, 3,  use_bias=False)) # remove bias param by setting it to false \n",
        " \n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "\n",
        "#  Global receptive field is now 18 x 18 \n",
        "\n",
        "# Add convolution layer - 16 filters of shape 3x3x16 \n",
        "#input coming from previous layer is 7 x 7 x 16 .\n",
        "\n",
        "model.add(Conv2D(16, 3,  use_bias=False))  # remove bias param by setting it to false \n",
        "model.add(BatchNormalization())  #add Batch normalization  \n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "\n",
        "#Global receptive field is now 20 x 20 \n",
        "\n",
        "# Add convolution layer - 16 filters of shape 3x3x16 \n",
        "#input coming from previous layer is 5 x 5 x 16 .\n",
        "\n",
        "model.add(Conv2D(16, 3,  use_bias=False))  # remove bias param by setting it to false \n",
        "\n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "\n",
        "#Global receptive field is now 22 x 22 \n",
        "\n",
        "# Add convolution layer - 16 filters of shape 3x3x16 \n",
        "#input coming from previous layer is 3 x 3 x 16 .\n",
        "\n",
        "model.add(Conv2D(16, 3,  use_bias=False))  # remove bias param by setting it to false \n",
        " \n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "\n",
        "#Global receptive field is now 24 x 24 \n",
        "\n",
        "# Add 1x1 convolution layer - 10 filters of shape 1x1x16 . \n",
        "# This combines the 16 channels from previous layer to 10 channels \n",
        "\n",
        "model.add(Conv2D(10, 1,  use_bias=False))  # remove bias param by setting it to false . \n",
        "# Note absence of ReLU activation here \n",
        "\n",
        "model.add(Flatten())  # Flatten the 2d array to 1d input for the softmax activation \n",
        "model.add(Activation('softmax'))   # Softmax activation to out class probabilities "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTdVH726xhN3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "outputId": "0552e153-9fb0-4d56-c8bc-717db533dd95"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 26, 26, 8)         72        \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 26, 26, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 24, 24, 8)         576       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 24, 24, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 22, 22, 8)         576       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 22, 22, 8)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 11, 11, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 9, 9, 16)          1152      \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 9, 9, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 7, 7, 16)          2304      \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 7, 7, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 5, 5, 16)          2304      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 5, 5, 16)          64        \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 5, 5, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 3, 3, 16)          2304      \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 3, 3, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 1, 1, 16)          2304      \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 1, 1, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 1, 1, 10)          160       \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 11,816\n",
            "Trainable params: 11,784\n",
            "Non-trainable params: 32\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7A0AhG_xrjw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqappyT1x7sf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6922
        },
        "outputId": "f7e44945-cf5d-4263-b7c8-07e5835c4923"
      },
      "source": [
        "model.fit(X_train, Y_train, validation_data=(X_test,Y_test),batch_size=32, epochs=100, verbose=1, callbacks=[chkpoint_model])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/100\n",
            "60000/60000 [==============================] - 15s 247us/step - loss: 0.2087 - acc: 0.9346 - val_loss: 0.0816 - val_acc: 0.9740\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.97400, saving model to /gdrive/My Drive/EVA/Session3/model_customv1_mnist_best.h5\n",
            "Epoch 2/100\n",
            "60000/60000 [==============================] - 11s 183us/step - loss: 0.0678 - acc: 0.9790 - val_loss: 0.0737 - val_acc: 0.9772\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.97400 to 0.97720, saving model to /gdrive/My Drive/EVA/Session3/model_customv1_mnist_best.h5\n",
            "Epoch 3/100\n",
            "60000/60000 [==============================] - 11s 182us/step - loss: 0.0545 - acc: 0.9833 - val_loss: 0.0469 - val_acc: 0.9855\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.97720 to 0.98550, saving model to /gdrive/My Drive/EVA/Session3/model_customv1_mnist_best.h5\n",
            "Epoch 4/100\n",
            "60000/60000 [==============================] - 11s 184us/step - loss: 0.0480 - acc: 0.9850 - val_loss: 0.0389 - val_acc: 0.9871\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.98550 to 0.98710, saving model to /gdrive/My Drive/EVA/Session3/model_customv1_mnist_best.h5\n",
            "Epoch 5/100\n",
            "60000/60000 [==============================] - 11s 182us/step - loss: 0.0410 - acc: 0.9871 - val_loss: 0.0382 - val_acc: 0.9866\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.98710\n",
            "Epoch 6/100\n",
            "60000/60000 [==============================] - 11s 183us/step - loss: 0.0350 - acc: 0.9889 - val_loss: 0.0326 - val_acc: 0.9897\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.98710 to 0.98970, saving model to /gdrive/My Drive/EVA/Session3/model_customv1_mnist_best.h5\n",
            "Epoch 7/100\n",
            "60000/60000 [==============================] - 11s 185us/step - loss: 0.0330 - acc: 0.9897 - val_loss: 0.0399 - val_acc: 0.9874\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.98970\n",
            "Epoch 8/100\n",
            "60000/60000 [==============================] - 12s 198us/step - loss: 0.0302 - acc: 0.9907 - val_loss: 0.0448 - val_acc: 0.9857\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.98970\n",
            "Epoch 9/100\n",
            "60000/60000 [==============================] - 11s 181us/step - loss: 0.0270 - acc: 0.9916 - val_loss: 0.0343 - val_acc: 0.9884\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.98970\n",
            "Epoch 10/100\n",
            "60000/60000 [==============================] - 11s 182us/step - loss: 0.0253 - acc: 0.9917 - val_loss: 0.0304 - val_acc: 0.9904\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.98970 to 0.99040, saving model to /gdrive/My Drive/EVA/Session3/model_customv1_mnist_best.h5\n",
            "Epoch 11/100\n",
            "60000/60000 [==============================] - 11s 180us/step - loss: 0.0232 - acc: 0.9926 - val_loss: 0.0264 - val_acc: 0.9910\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.99040 to 0.99100, saving model to /gdrive/My Drive/EVA/Session3/model_customv1_mnist_best.h5\n",
            "Epoch 12/100\n",
            "60000/60000 [==============================] - 11s 182us/step - loss: 0.0223 - acc: 0.9929 - val_loss: 0.0290 - val_acc: 0.9908\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.99100\n",
            "Epoch 13/100\n",
            "60000/60000 [==============================] - 11s 181us/step - loss: 0.0206 - acc: 0.9932 - val_loss: 0.0307 - val_acc: 0.9898\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.99100\n",
            "Epoch 14/100\n",
            "60000/60000 [==============================] - 11s 180us/step - loss: 0.0192 - acc: 0.9939 - val_loss: 0.0377 - val_acc: 0.9884\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.99100\n",
            "Epoch 15/100\n",
            "60000/60000 [==============================] - 12s 199us/step - loss: 0.0165 - acc: 0.9948 - val_loss: 0.0385 - val_acc: 0.9881\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.99100\n",
            "Epoch 16/100\n",
            "60000/60000 [==============================] - 11s 191us/step - loss: 0.0166 - acc: 0.9945 - val_loss: 0.0377 - val_acc: 0.9898\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.99100\n",
            "Epoch 17/100\n",
            "60000/60000 [==============================] - 12s 196us/step - loss: 0.0163 - acc: 0.9946 - val_loss: 0.0292 - val_acc: 0.9911\n",
            "\n",
            "Epoch 00017: val_acc improved from 0.99100 to 0.99110, saving model to /gdrive/My Drive/EVA/Session3/model_customv1_mnist_best.h5\n",
            "Epoch 18/100\n",
            "60000/60000 [==============================] - 11s 180us/step - loss: 0.0153 - acc: 0.9949 - val_loss: 0.0293 - val_acc: 0.9915\n",
            "\n",
            "Epoch 00018: val_acc improved from 0.99110 to 0.99150, saving model to /gdrive/My Drive/EVA/Session3/model_customv1_mnist_best.h5\n",
            "Epoch 19/100\n",
            "60000/60000 [==============================] - 11s 180us/step - loss: 0.0134 - acc: 0.9959 - val_loss: 0.0323 - val_acc: 0.9899\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.99150\n",
            "Epoch 20/100\n",
            "60000/60000 [==============================] - 11s 180us/step - loss: 0.0123 - acc: 0.9958 - val_loss: 0.0263 - val_acc: 0.9925\n",
            "\n",
            "Epoch 00020: val_acc improved from 0.99150 to 0.99250, saving model to /gdrive/My Drive/EVA/Session3/model_customv1_mnist_best.h5\n",
            "Epoch 21/100\n",
            "60000/60000 [==============================] - 12s 192us/step - loss: 0.0139 - acc: 0.9953 - val_loss: 0.0338 - val_acc: 0.9909\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.99250\n",
            "Epoch 22/100\n",
            "60000/60000 [==============================] - 12s 200us/step - loss: 0.0119 - acc: 0.9957 - val_loss: 0.0314 - val_acc: 0.9914\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.99250\n",
            "Epoch 23/100\n",
            "60000/60000 [==============================] - 11s 179us/step - loss: 0.0117 - acc: 0.9959 - val_loss: 0.0428 - val_acc: 0.9886\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.99250\n",
            "Epoch 24/100\n",
            "60000/60000 [==============================] - 11s 179us/step - loss: 0.0109 - acc: 0.9962 - val_loss: 0.0409 - val_acc: 0.9887\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.99250\n",
            "Epoch 25/100\n",
            "60000/60000 [==============================] - 11s 179us/step - loss: 0.0109 - acc: 0.9961 - val_loss: 0.0380 - val_acc: 0.9909\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.99250\n",
            "Epoch 26/100\n",
            "60000/60000 [==============================] - 11s 179us/step - loss: 0.0110 - acc: 0.9963 - val_loss: 0.0437 - val_acc: 0.9909\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.99250\n",
            "Epoch 27/100\n",
            "60000/60000 [==============================] - 11s 178us/step - loss: 0.0104 - acc: 0.9965 - val_loss: 0.0335 - val_acc: 0.9914\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.99250\n",
            "Epoch 28/100\n",
            "60000/60000 [==============================] - 11s 179us/step - loss: 0.0107 - acc: 0.9965 - val_loss: 0.0353 - val_acc: 0.9914\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.99250\n",
            "Epoch 29/100\n",
            "60000/60000 [==============================] - 11s 190us/step - loss: 0.0091 - acc: 0.9973 - val_loss: 0.0338 - val_acc: 0.9919\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.99250\n",
            "Epoch 30/100\n",
            "60000/60000 [==============================] - 11s 187us/step - loss: 0.0089 - acc: 0.9969 - val_loss: 0.0394 - val_acc: 0.9913\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.99250\n",
            "Epoch 31/100\n",
            "60000/60000 [==============================] - 11s 179us/step - loss: 0.0081 - acc: 0.9972 - val_loss: 0.0483 - val_acc: 0.9910\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.99250\n",
            "Epoch 32/100\n",
            "60000/60000 [==============================] - 11s 179us/step - loss: 0.0091 - acc: 0.9969 - val_loss: 0.0349 - val_acc: 0.9910\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.99250\n",
            "Epoch 33/100\n",
            "60000/60000 [==============================] - 11s 178us/step - loss: 0.0075 - acc: 0.9978 - val_loss: 0.0419 - val_acc: 0.9907\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.99250\n",
            "Epoch 34/100\n",
            "60000/60000 [==============================] - 11s 178us/step - loss: 0.0086 - acc: 0.9972 - val_loss: 0.0390 - val_acc: 0.9909\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.99250\n",
            "Epoch 35/100\n",
            "60000/60000 [==============================] - 11s 178us/step - loss: 0.0077 - acc: 0.9974 - val_loss: 0.0383 - val_acc: 0.9901\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.99250\n",
            "Epoch 36/100\n",
            "60000/60000 [==============================] - 11s 180us/step - loss: 0.0068 - acc: 0.9977 - val_loss: 0.0500 - val_acc: 0.9891\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.99250\n",
            "Epoch 37/100\n",
            "60000/60000 [==============================] - 12s 197us/step - loss: 0.0086 - acc: 0.9969 - val_loss: 0.0329 - val_acc: 0.9917\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.99250\n",
            "Epoch 38/100\n",
            "60000/60000 [==============================] - 11s 180us/step - loss: 0.0058 - acc: 0.9979 - val_loss: 0.0382 - val_acc: 0.9914\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.99250\n",
            "Epoch 39/100\n",
            "60000/60000 [==============================] - 11s 178us/step - loss: 0.0071 - acc: 0.9976 - val_loss: 0.0450 - val_acc: 0.9893\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.99250\n",
            "Epoch 40/100\n",
            "60000/60000 [==============================] - 11s 178us/step - loss: 0.0070 - acc: 0.9977 - val_loss: 0.0323 - val_acc: 0.9927\n",
            "\n",
            "Epoch 00040: val_acc improved from 0.99250 to 0.99270, saving model to /gdrive/My Drive/EVA/Session3/model_customv1_mnist_best.h5\n",
            "Epoch 41/100\n",
            "60000/60000 [==============================] - 11s 178us/step - loss: 0.0064 - acc: 0.9980 - val_loss: 0.0324 - val_acc: 0.9920\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.99270\n",
            "Epoch 42/100\n",
            "60000/60000 [==============================] - 11s 177us/step - loss: 0.0072 - acc: 0.9977 - val_loss: 0.0384 - val_acc: 0.9919\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.99270\n",
            "Epoch 43/100\n",
            "60000/60000 [==============================] - 11s 180us/step - loss: 0.0067 - acc: 0.9980 - val_loss: 0.0405 - val_acc: 0.9904\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.99270\n",
            "Epoch 44/100\n",
            "60000/60000 [==============================] - 12s 196us/step - loss: 0.0056 - acc: 0.9981 - val_loss: 0.0497 - val_acc: 0.9904\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.99270\n",
            "Epoch 45/100\n",
            "60000/60000 [==============================] - 12s 201us/step - loss: 0.0064 - acc: 0.9979 - val_loss: 0.0572 - val_acc: 0.9884\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.99270\n",
            "Epoch 46/100\n",
            "60000/60000 [==============================] - 11s 179us/step - loss: 0.0069 - acc: 0.9977 - val_loss: 0.0416 - val_acc: 0.9906\n",
            "\n",
            "Epoch 00046: val_acc did not improve from 0.99270\n",
            "Epoch 47/100\n",
            "60000/60000 [==============================] - 11s 178us/step - loss: 0.0045 - acc: 0.9985 - val_loss: 0.0454 - val_acc: 0.9904\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.99270\n",
            "Epoch 48/100\n",
            "60000/60000 [==============================] - 11s 178us/step - loss: 0.0065 - acc: 0.9980 - val_loss: 0.0383 - val_acc: 0.9924\n",
            "\n",
            "Epoch 00048: val_acc did not improve from 0.99270\n",
            "Epoch 49/100\n",
            "60000/60000 [==============================] - 12s 192us/step - loss: 0.0064 - acc: 0.9981 - val_loss: 0.0440 - val_acc: 0.9915\n",
            "\n",
            "Epoch 00049: val_acc did not improve from 0.99270\n",
            "Epoch 50/100\n",
            "60000/60000 [==============================] - 11s 178us/step - loss: 0.0052 - acc: 0.9981 - val_loss: 0.0389 - val_acc: 0.9922\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.99270\n",
            "Epoch 51/100\n",
            "60000/60000 [==============================] - 11s 190us/step - loss: 0.0055 - acc: 0.9983 - val_loss: 0.0428 - val_acc: 0.9913\n",
            "\n",
            "Epoch 00051: val_acc did not improve from 0.99270\n",
            "Epoch 52/100\n",
            "60000/60000 [==============================] - 11s 187us/step - loss: 0.0046 - acc: 0.9984 - val_loss: 0.0529 - val_acc: 0.9909\n",
            "\n",
            "Epoch 00052: val_acc did not improve from 0.99270\n",
            "Epoch 53/100\n",
            "60000/60000 [==============================] - 11s 179us/step - loss: 0.0059 - acc: 0.9981 - val_loss: 0.0429 - val_acc: 0.9915\n",
            "\n",
            "Epoch 00053: val_acc did not improve from 0.99270\n",
            "Epoch 54/100\n",
            "60000/60000 [==============================] - 11s 181us/step - loss: 0.0043 - acc: 0.9985 - val_loss: 0.0443 - val_acc: 0.9908\n",
            "\n",
            "Epoch 00054: val_acc did not improve from 0.99270\n",
            "Epoch 55/100\n",
            "60000/60000 [==============================] - 11s 179us/step - loss: 0.0063 - acc: 0.9981 - val_loss: 0.0388 - val_acc: 0.9920\n",
            "\n",
            "Epoch 00055: val_acc did not improve from 0.99270\n",
            "Epoch 56/100\n",
            "60000/60000 [==============================] - 11s 178us/step - loss: 0.0055 - acc: 0.9982 - val_loss: 0.0462 - val_acc: 0.9901\n",
            "\n",
            "Epoch 00056: val_acc did not improve from 0.99270\n",
            "Epoch 57/100\n",
            "60000/60000 [==============================] - 11s 180us/step - loss: 0.0045 - acc: 0.9985 - val_loss: 0.0682 - val_acc: 0.9869\n",
            "\n",
            "Epoch 00057: val_acc did not improve from 0.99270\n",
            "Epoch 58/100\n",
            "60000/60000 [==============================] - 11s 182us/step - loss: 0.0050 - acc: 0.9985 - val_loss: 0.0393 - val_acc: 0.9913\n",
            "\n",
            "Epoch 00058: val_acc did not improve from 0.99270\n",
            "Epoch 59/100\n",
            "60000/60000 [==============================] - 12s 196us/step - loss: 0.0053 - acc: 0.9981 - val_loss: 0.0376 - val_acc: 0.9914\n",
            "\n",
            "Epoch 00059: val_acc did not improve from 0.99270\n",
            "Epoch 60/100\n",
            "60000/60000 [==============================] - 11s 179us/step - loss: 0.0052 - acc: 0.9983 - val_loss: 0.0410 - val_acc: 0.9915\n",
            "\n",
            "Epoch 00060: val_acc did not improve from 0.99270\n",
            "Epoch 61/100\n",
            "60000/60000 [==============================] - 11s 178us/step - loss: 0.0036 - acc: 0.9988 - val_loss: 0.0374 - val_acc: 0.9912\n",
            "\n",
            "Epoch 00061: val_acc did not improve from 0.99270\n",
            "Epoch 62/100\n",
            "60000/60000 [==============================] - 11s 177us/step - loss: 0.0062 - acc: 0.9981 - val_loss: 0.0495 - val_acc: 0.9906\n",
            "\n",
            "Epoch 00062: val_acc did not improve from 0.99270\n",
            "Epoch 63/100\n",
            "60000/60000 [==============================] - 11s 178us/step - loss: 0.0061 - acc: 0.9982 - val_loss: 0.0391 - val_acc: 0.9925\n",
            "\n",
            "Epoch 00063: val_acc did not improve from 0.99270\n",
            "Epoch 64/100\n",
            "60000/60000 [==============================] - 11s 178us/step - loss: 0.0032 - acc: 0.9990 - val_loss: 0.0460 - val_acc: 0.9918\n",
            "\n",
            "Epoch 00064: val_acc did not improve from 0.99270\n",
            "Epoch 65/100\n",
            "60000/60000 [==============================] - 11s 178us/step - loss: 0.0066 - acc: 0.9981 - val_loss: 0.0468 - val_acc: 0.9912\n",
            "\n",
            "Epoch 00065: val_acc did not improve from 0.99270\n",
            "Epoch 66/100\n",
            "60000/60000 [==============================] - 12s 195us/step - loss: 0.0042 - acc: 0.9989 - val_loss: 0.0483 - val_acc: 0.9904\n",
            "\n",
            "Epoch 00066: val_acc did not improve from 0.99270\n",
            "Epoch 67/100\n",
            "60000/60000 [==============================] - 11s 179us/step - loss: 0.0045 - acc: 0.9985 - val_loss: 0.0472 - val_acc: 0.9910\n",
            "\n",
            "Epoch 00067: val_acc did not improve from 0.99270\n",
            "Epoch 68/100\n",
            "60000/60000 [==============================] - 11s 176us/step - loss: 0.0038 - acc: 0.9986 - val_loss: 0.0406 - val_acc: 0.9915\n",
            "\n",
            "Epoch 00068: val_acc did not improve from 0.99270\n",
            "Epoch 69/100\n",
            "60000/60000 [==============================] - 11s 177us/step - loss: 0.0045 - acc: 0.9984 - val_loss: 0.0479 - val_acc: 0.9909\n",
            "\n",
            "Epoch 00069: val_acc did not improve from 0.99270\n",
            "Epoch 70/100\n",
            "60000/60000 [==============================] - 11s 177us/step - loss: 0.0044 - acc: 0.9985 - val_loss: 0.0530 - val_acc: 0.9902\n",
            "\n",
            "Epoch 00070: val_acc did not improve from 0.99270\n",
            "Epoch 71/100\n",
            "60000/60000 [==============================] - 11s 176us/step - loss: 0.0038 - acc: 0.9987 - val_loss: 0.0520 - val_acc: 0.9902\n",
            "\n",
            "Epoch 00071: val_acc did not improve from 0.99270\n",
            "Epoch 72/100\n",
            "60000/60000 [==============================] - 11s 177us/step - loss: 0.0051 - acc: 0.9983 - val_loss: 0.0530 - val_acc: 0.9906\n",
            "\n",
            "Epoch 00072: val_acc did not improve from 0.99270\n",
            "Epoch 73/100\n",
            "60000/60000 [==============================] - 12s 199us/step - loss: 0.0045 - acc: 0.9986 - val_loss: 0.0400 - val_acc: 0.9923\n",
            "\n",
            "Epoch 00073: val_acc did not improve from 0.99270\n",
            "Epoch 74/100\n",
            "60000/60000 [==============================] - 11s 191us/step - loss: 0.0029 - acc: 0.9990 - val_loss: 0.0441 - val_acc: 0.9917\n",
            "\n",
            "Epoch 00074: val_acc did not improve from 0.99270\n",
            "Epoch 75/100\n",
            "60000/60000 [==============================] - 11s 177us/step - loss: 0.0028 - acc: 0.9991 - val_loss: 0.0539 - val_acc: 0.9899\n",
            "\n",
            "Epoch 00075: val_acc did not improve from 0.99270\n",
            "Epoch 76/100\n",
            "60000/60000 [==============================] - 11s 177us/step - loss: 0.0050 - acc: 0.9985 - val_loss: 0.0450 - val_acc: 0.9925\n",
            "\n",
            "Epoch 00076: val_acc did not improve from 0.99270\n",
            "Epoch 77/100\n",
            "60000/60000 [==============================] - 11s 189us/step - loss: 0.0041 - acc: 0.9988 - val_loss: 0.0613 - val_acc: 0.9894\n",
            "\n",
            "Epoch 00077: val_acc did not improve from 0.99270\n",
            "Epoch 78/100\n",
            "60000/60000 [==============================] - 11s 176us/step - loss: 0.0045 - acc: 0.9985 - val_loss: 0.0495 - val_acc: 0.9906\n",
            "\n",
            "Epoch 00078: val_acc did not improve from 0.99270\n",
            "Epoch 79/100\n",
            "60000/60000 [==============================] - 11s 175us/step - loss: 0.0038 - acc: 0.9987 - val_loss: 0.0497 - val_acc: 0.9906\n",
            "\n",
            "Epoch 00079: val_acc did not improve from 0.99270\n",
            "Epoch 80/100\n",
            "60000/60000 [==============================] - 11s 176us/step - loss: 0.0035 - acc: 0.9987 - val_loss: 0.0456 - val_acc: 0.9910\n",
            "\n",
            "Epoch 00080: val_acc did not improve from 0.99270\n",
            "Epoch 81/100\n",
            "60000/60000 [==============================] - 12s 197us/step - loss: 0.0039 - acc: 0.9987 - val_loss: 0.0544 - val_acc: 0.9900\n",
            "\n",
            "Epoch 00081: val_acc did not improve from 0.99270\n",
            "Epoch 82/100\n",
            "60000/60000 [==============================] - 11s 177us/step - loss: 0.0033 - acc: 0.9988 - val_loss: 0.0565 - val_acc: 0.9894\n",
            "\n",
            "Epoch 00082: val_acc did not improve from 0.99270\n",
            "Epoch 83/100\n",
            "60000/60000 [==============================] - 11s 176us/step - loss: 0.0035 - acc: 0.9990 - val_loss: 0.0557 - val_acc: 0.9902\n",
            "\n",
            "Epoch 00083: val_acc did not improve from 0.99270\n",
            "Epoch 84/100\n",
            "60000/60000 [==============================] - 11s 176us/step - loss: 0.0037 - acc: 0.9989 - val_loss: 0.0478 - val_acc: 0.9908\n",
            "\n",
            "Epoch 00084: val_acc did not improve from 0.99270\n",
            "Epoch 85/100\n",
            "60000/60000 [==============================] - 11s 176us/step - loss: 0.0034 - acc: 0.9987 - val_loss: 0.0483 - val_acc: 0.9906\n",
            "\n",
            "Epoch 00085: val_acc did not improve from 0.99270\n",
            "Epoch 86/100\n",
            "60000/60000 [==============================] - 11s 176us/step - loss: 0.0039 - acc: 0.9987 - val_loss: 0.0589 - val_acc: 0.9895\n",
            "\n",
            "Epoch 00086: val_acc did not improve from 0.99270\n",
            "Epoch 87/100\n",
            "60000/60000 [==============================] - 11s 176us/step - loss: 0.0029 - acc: 0.9991 - val_loss: 0.0505 - val_acc: 0.9919\n",
            "\n",
            "Epoch 00087: val_acc did not improve from 0.99270\n",
            "Epoch 88/100\n",
            "60000/60000 [==============================] - 11s 187us/step - loss: 0.0048 - acc: 0.9985 - val_loss: 0.0661 - val_acc: 0.9895\n",
            "\n",
            "Epoch 00088: val_acc did not improve from 0.99270\n",
            "Epoch 89/100\n",
            "60000/60000 [==============================] - 11s 186us/step - loss: 0.0049 - acc: 0.9986 - val_loss: 0.0487 - val_acc: 0.9915\n",
            "\n",
            "Epoch 00089: val_acc did not improve from 0.99270\n",
            "Epoch 90/100\n",
            "60000/60000 [==============================] - 11s 176us/step - loss: 0.0037 - acc: 0.9990 - val_loss: 0.0523 - val_acc: 0.9894\n",
            "\n",
            "Epoch 00090: val_acc did not improve from 0.99270\n",
            "Epoch 91/100\n",
            "60000/60000 [==============================] - 11s 175us/step - loss: 0.0030 - acc: 0.9991 - val_loss: 0.0607 - val_acc: 0.9881\n",
            "\n",
            "Epoch 00091: val_acc did not improve from 0.99270\n",
            "Epoch 92/100\n",
            "60000/60000 [==============================] - 11s 175us/step - loss: 0.0045 - acc: 0.9988 - val_loss: 0.0440 - val_acc: 0.9916\n",
            "\n",
            "Epoch 00092: val_acc did not improve from 0.99270\n",
            "Epoch 93/100\n",
            "60000/60000 [==============================] - 11s 176us/step - loss: 0.0044 - acc: 0.9986 - val_loss: 0.0431 - val_acc: 0.9916\n",
            "\n",
            "Epoch 00093: val_acc did not improve from 0.99270\n",
            "Epoch 94/100\n",
            "60000/60000 [==============================] - 11s 176us/step - loss: 0.0021 - acc: 0.9993 - val_loss: 0.0529 - val_acc: 0.9908\n",
            "\n",
            "Epoch 00094: val_acc did not improve from 0.99270\n",
            "Epoch 95/100\n",
            "60000/60000 [==============================] - 11s 175us/step - loss: 0.0034 - acc: 0.9990 - val_loss: 0.0473 - val_acc: 0.9915\n",
            "\n",
            "Epoch 00095: val_acc did not improve from 0.99270\n",
            "Epoch 96/100\n",
            "60000/60000 [==============================] - 12s 197us/step - loss: 0.0033 - acc: 0.9990 - val_loss: 0.0618 - val_acc: 0.9896\n",
            "\n",
            "Epoch 00096: val_acc did not improve from 0.99270\n",
            "Epoch 97/100\n",
            "60000/60000 [==============================] - 11s 176us/step - loss: 0.0029 - acc: 0.9990 - val_loss: 0.0495 - val_acc: 0.9918\n",
            "\n",
            "Epoch 00097: val_acc did not improve from 0.99270\n",
            "Epoch 98/100\n",
            "60000/60000 [==============================] - 11s 176us/step - loss: 0.0027 - acc: 0.9993 - val_loss: 0.0615 - val_acc: 0.9903\n",
            "\n",
            "Epoch 00098: val_acc did not improve from 0.99270\n",
            "Epoch 99/100\n",
            "60000/60000 [==============================] - 11s 176us/step - loss: 0.0047 - acc: 0.9986 - val_loss: 0.0443 - val_acc: 0.9918\n",
            "\n",
            "Epoch 00099: val_acc did not improve from 0.99270\n",
            "Epoch 100/100\n",
            "60000/60000 [==============================] - 11s 176us/step - loss: 0.0027 - acc: 0.9992 - val_loss: 0.0583 - val_acc: 0.9905\n",
            "\n",
            "Epoch 00100: val_acc did not improve from 0.99270\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2e70fb9630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAo7lmRA0FIE",
        "colab_type": "text"
      },
      "source": [
        "**Unfortunately we are stuck at 99.27 validation accuracy for this network** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5NXpqKXblmH",
        "colab_type": "text"
      },
      "source": [
        "###Model version 2 : Add Batch Normalization \n",
        "Now let us add add Batch Normalization to the same network to see if there is improvement . Batch Normalization is a way for the network take care of internal covariate shift in the features and was first introduced in this paper titled **Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift** https://arxiv.org/abs/1502.03167\n",
        "\n",
        "We will follow how BatchNormalization was used in the paper i.e Convolution followed Batch Normalization and then Activation although recently some practioners have started using BatchNormalization after activation citing better performance.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osKqT73Q9JJB",
        "colab_type": "code",
        "outputId": "f221838d-22c7-40e6-c326-e3153d00380c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "np.random.seed(seed=42)  # set a random seed for reproducing random values \n",
        "\n",
        "# instantiate a sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# add the first convolution layer - 8 numbers of 3x3 filters , \n",
        "#This layer sees the input image of 28x28 x 1 channel . \n",
        "\n",
        "model.add(Conv2D(8, (3, 3), input_shape=(28,28,1), use_bias=False))  # remove bias param by setting it to false \n",
        "model.add(BatchNormalization())  #add Batch normalization  \n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "\n",
        "# Now the global receptive field is 3 x 3 \n",
        "\n",
        "# second convolution layer - 8 filters of shape  3x3x8 \n",
        "#input from previous layer is 26 x 26 x 8 . \n",
        "\n",
        "model.add(Conv2D(8, 3, use_bias=False))  # remove bias param by setting it to false \n",
        "model.add(BatchNormalization())  #add Batch normalization  \n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "\n",
        "#Global receptive field is 5x5\n",
        "\n",
        "# Add convolution layer - 8 filters of shape  3x3x8 \n",
        "#input from previous layer is 24 x 24 x 8 . \n",
        "\n",
        "model.add(Conv2D(8, 3, use_bias=False))  # remove bias param by setting it to false \n",
        "model.add(BatchNormalization())  #add Batch normalization  \n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "\n",
        "\n",
        "#Global receptive field is 7x7\n",
        "\n",
        "# Perform 2x2 max pooling  . \n",
        "#Input from previous layer is 22 x 22 X 8 \n",
        "\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "#model.add(Dropout(0.33))   # add Dropout for 1/3 of the nodes \n",
        "\n",
        "# After max pooling , dimensions reduce by half , i.e they become 11 x 11 . \n",
        "# Maxpooling (withpool size 2 and stride 1) doubles receptive field . \n",
        "# So global receptive field after max pooling is 14 x 14\n",
        "\n",
        "# Add convolution layer - 16 filters of shape 3x3x8 \n",
        "#input from max pooling operation is 11 x 11 x 8 .  \n",
        "\n",
        "model.add(Conv2D(16, 3,  use_bias=False))  # remove bias param by setting it to false \n",
        "model.add(BatchNormalization())  #add Batch normalization  \n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "\n",
        "#Global receptive field is now 16 x 16 \n",
        "\n",
        "# Add convolution layer - 16 filters of shape 3x3x16 \n",
        "#input coming from previous layer is 9 x 9 x 16 . \n",
        "\n",
        "model.add(Conv2D(16, 3,  use_bias=False)) # remove bias param by setting it to false \n",
        "model.add(BatchNormalization())  #add Batch normalization  \n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "\n",
        "#  Global receptive field is now 18 x 18 \n",
        "\n",
        "# Add convolution layer - 16 filters of shape 3x3x16 \n",
        "#input coming from previous layer is 7 x 7 x 16 .\n",
        "\n",
        "model.add(Conv2D(16, 3,  use_bias=False))  # remove bias param by setting it to false \n",
        "model.add(BatchNormalization())  #add Batch normalization  \n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "\n",
        "#Global receptive field is now 20 x 20 \n",
        "\n",
        "#model.add(Dropout(0.33)) # add Dropout for 1/3 of the nodes\n",
        "\n",
        "# Add convolution layer - 16 filters of shape 3x3x16 \n",
        "#input coming from previous layer is 5 x 5 x 16 .\n",
        "\n",
        "model.add(Conv2D(16, 3,  use_bias=False))  # remove bias param by setting it to false \n",
        "model.add(BatchNormalization())  #add Batch normalization  \n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "\n",
        "#Global receptive field is now 22 x 22 \n",
        "\n",
        "# Add convolution layer - 16 filters of shape 3x3x16 \n",
        "#input coming from previous layer is 3 x 3 x 16 .\n",
        "\n",
        "model.add(Conv2D(16, 3,  use_bias=False))  # remove bias param by setting it to false \n",
        "model.add(BatchNormalization())  #add Batch normalization  \n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "\n",
        "#Global receptive field is now 24 x 24 \n",
        "\n",
        "# Add 1x1 convolution layer - 10 filters of shape 1x1x16 . \n",
        "# This combines the 16 channels from previous layer to 10 channels \n",
        "\n",
        "model.add(Conv2D(10, 1,  use_bias=False))  # remove bias param by setting it to false . \n",
        "# Note absence of ReLU activation here \n",
        "\n",
        "model.add(Flatten())  # Flatten the 2d array to 1d input for the softmax activation \n",
        "model.add(Activation('softmax'))   # Softmax activation to out class probabilities "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "551c2766-852c-4ee6-b67a-5e28d48f3311",
        "id": "NgdlhgQSL3-h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1156
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_10 (Conv2D)           (None, 26, 26, 8)         72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 26, 26, 8)         32        \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 26, 26, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 24, 24, 8)         576       \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 24, 24, 8)         32        \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 24, 24, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 22, 22, 8)         576       \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 22, 22, 8)         32        \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 22, 22, 8)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 11, 11, 8)         0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 11, 11, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 9, 9, 16)          1152      \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 9, 9, 16)          64        \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 9, 9, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 7, 7, 16)          2304      \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 7, 7, 16)          64        \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 7, 7, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 5, 5, 16)          2304      \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 5, 5, 16)          64        \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 5, 5, 16)          0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 5, 5, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 3, 3, 16)          2304      \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 3, 3, 16)          64        \n",
            "_________________________________________________________________\n",
            "activation_16 (Activation)   (None, 3, 3, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 1, 1, 16)          2304      \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 1, 1, 16)          64        \n",
            "_________________________________________________________________\n",
            "activation_17 (Activation)   (None, 1, 1, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 1, 1, 10)          160       \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_18 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 12,168\n",
            "Trainable params: 11,960\n",
            "Non-trainable params: 208\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FQ-S1ndiM2P8",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "  \n",
        "chkpoint_model=ModelCheckpoint(\"/gdrive/My Drive/EVA/Session3/model_customv2_mnist_best.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='max')  \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MsB6AHz6NAhu",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f090a806-c88a-4166-eef7-686f10f9e6fb",
        "id": "DwXTQ1soNAhw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6871
        }
      },
      "source": [
        "model.fit(X_train, Y_train, validation_data=(X_test,Y_test),batch_size=32, epochs=100, verbose=1, callbacks=[chkpoint_model])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/100\n",
            "60000/60000 [==============================] - 21s 358us/step - loss: 0.5314 - acc: 0.8464 - val_loss: 0.0865 - val_acc: 0.9742\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.97420, saving model to /gdrive/My Drive/EVA/Session3/model_customv2_mnist_best.h5\n",
            "Epoch 2/100\n",
            "60000/60000 [==============================] - 20s 340us/step - loss: 0.1373 - acc: 0.9586 - val_loss: 0.0539 - val_acc: 0.9837\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.97420 to 0.98370, saving model to /gdrive/My Drive/EVA/Session3/model_customv2_mnist_best.h5\n",
            "Epoch 3/100\n",
            "60000/60000 [==============================] - 22s 360us/step - loss: 0.1007 - acc: 0.9689 - val_loss: 0.0430 - val_acc: 0.9867\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.98370 to 0.98670, saving model to /gdrive/My Drive/EVA/Session3/model_customv2_mnist_best.h5\n",
            "Epoch 4/100\n",
            "60000/60000 [==============================] - 20s 341us/step - loss: 0.0887 - acc: 0.9731 - val_loss: 0.0401 - val_acc: 0.9875\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.98670 to 0.98750, saving model to /gdrive/My Drive/EVA/Session3/model_customv2_mnist_best.h5\n",
            "Epoch 5/100\n",
            "60000/60000 [==============================] - 20s 340us/step - loss: 0.0769 - acc: 0.9768 - val_loss: 0.0345 - val_acc: 0.9890\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.98750 to 0.98900, saving model to /gdrive/My Drive/EVA/Session3/model_customv2_mnist_best.h5\n",
            "Epoch 6/100\n",
            "60000/60000 [==============================] - 20s 341us/step - loss: 0.0746 - acc: 0.9771 - val_loss: 0.0312 - val_acc: 0.9903\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.98900 to 0.99030, saving model to /gdrive/My Drive/EVA/Session3/model_customv2_mnist_best.h5\n",
            "Epoch 7/100\n",
            "60000/60000 [==============================] - 22s 361us/step - loss: 0.0659 - acc: 0.9797 - val_loss: 0.0288 - val_acc: 0.9902\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.99030\n",
            "Epoch 8/100\n",
            "60000/60000 [==============================] - 20s 340us/step - loss: 0.0643 - acc: 0.9805 - val_loss: 0.0282 - val_acc: 0.9914\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.99030 to 0.99140, saving model to /gdrive/My Drive/EVA/Session3/model_customv2_mnist_best.h5\n",
            "Epoch 9/100\n",
            "60000/60000 [==============================] - 20s 338us/step - loss: 0.0616 - acc: 0.9805 - val_loss: 0.0336 - val_acc: 0.9897\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.99140\n",
            "Epoch 10/100\n",
            "60000/60000 [==============================] - 20s 339us/step - loss: 0.0591 - acc: 0.9812 - val_loss: 0.0268 - val_acc: 0.9910\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.99140\n",
            "Epoch 11/100\n",
            "60000/60000 [==============================] - 22s 360us/step - loss: 0.0555 - acc: 0.9828 - val_loss: 0.0232 - val_acc: 0.9931\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.99140 to 0.99310, saving model to /gdrive/My Drive/EVA/Session3/model_customv2_mnist_best.h5\n",
            "Epoch 12/100\n",
            "60000/60000 [==============================] - 21s 352us/step - loss: 0.0533 - acc: 0.9834 - val_loss: 0.0249 - val_acc: 0.9926\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.99310\n",
            "Epoch 13/100\n",
            "60000/60000 [==============================] - 20s 338us/step - loss: 0.0511 - acc: 0.9844 - val_loss: 0.0262 - val_acc: 0.9913\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.99310\n",
            "Epoch 14/100\n",
            "60000/60000 [==============================] - 20s 339us/step - loss: 0.0513 - acc: 0.9842 - val_loss: 0.0250 - val_acc: 0.9920\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.99310\n",
            "Epoch 15/100\n",
            "60000/60000 [==============================] - 22s 368us/step - loss: 0.0484 - acc: 0.9844 - val_loss: 0.0257 - val_acc: 0.9913\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.99310\n",
            "Epoch 16/100\n",
            "60000/60000 [==============================] - 20s 338us/step - loss: 0.0463 - acc: 0.9854 - val_loss: 0.0263 - val_acc: 0.9925\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.99310\n",
            "Epoch 17/100\n",
            "60000/60000 [==============================] - 20s 337us/step - loss: 0.0465 - acc: 0.9855 - val_loss: 0.0227 - val_acc: 0.9923\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.99310\n",
            "Epoch 18/100\n",
            "60000/60000 [==============================] - 20s 337us/step - loss: 0.0447 - acc: 0.9862 - val_loss: 0.0274 - val_acc: 0.9915\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.99310\n",
            "Epoch 19/100\n",
            "60000/60000 [==============================] - 22s 361us/step - loss: 0.0452 - acc: 0.9862 - val_loss: 0.0231 - val_acc: 0.9925\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.99310\n",
            "Epoch 20/100\n",
            "60000/60000 [==============================] - 20s 338us/step - loss: 0.0431 - acc: 0.9872 - val_loss: 0.0259 - val_acc: 0.9923\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.99310\n",
            "Epoch 21/100\n",
            "60000/60000 [==============================] - 20s 338us/step - loss: 0.0411 - acc: 0.9873 - val_loss: 0.0232 - val_acc: 0.9925\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.99310\n",
            "Epoch 22/100\n",
            "60000/60000 [==============================] - 21s 342us/step - loss: 0.0436 - acc: 0.9863 - val_loss: 0.0215 - val_acc: 0.9933\n",
            "\n",
            "Epoch 00022: val_acc improved from 0.99310 to 0.99330, saving model to /gdrive/My Drive/EVA/Session3/model_customv2_mnist_best.h5\n",
            "Epoch 23/100\n",
            "60000/60000 [==============================] - 21s 357us/step - loss: 0.0402 - acc: 0.9872 - val_loss: 0.0287 - val_acc: 0.9911\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.99330\n",
            "Epoch 24/100\n",
            "60000/60000 [==============================] - 20s 339us/step - loss: 0.0381 - acc: 0.9881 - val_loss: 0.0253 - val_acc: 0.9923\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.99330\n",
            "Epoch 25/100\n",
            "60000/60000 [==============================] - 20s 339us/step - loss: 0.0407 - acc: 0.9877 - val_loss: 0.0227 - val_acc: 0.9933\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.99330\n",
            "Epoch 26/100\n",
            "60000/60000 [==============================] - 21s 349us/step - loss: 0.0387 - acc: 0.9875 - val_loss: 0.0246 - val_acc: 0.9920\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.99330\n",
            "Epoch 27/100\n",
            "60000/60000 [==============================] - 21s 352us/step - loss: 0.0376 - acc: 0.9880 - val_loss: 0.0229 - val_acc: 0.9931\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.99330\n",
            "Epoch 28/100\n",
            "60000/60000 [==============================] - 20s 340us/step - loss: 0.0411 - acc: 0.9872 - val_loss: 0.0206 - val_acc: 0.9937\n",
            "\n",
            "Epoch 00028: val_acc improved from 0.99330 to 0.99370, saving model to /gdrive/My Drive/EVA/Session3/model_customv2_mnist_best.h5\n",
            "Epoch 29/100\n",
            "60000/60000 [==============================] - 20s 340us/step - loss: 0.0389 - acc: 0.9878 - val_loss: 0.0200 - val_acc: 0.9941\n",
            "\n",
            "Epoch 00029: val_acc improved from 0.99370 to 0.99410, saving model to /gdrive/My Drive/EVA/Session3/model_customv2_mnist_best.h5\n",
            "Epoch 30/100\n",
            "60000/60000 [==============================] - 23s 389us/step - loss: 0.0380 - acc: 0.9879 - val_loss: 0.0222 - val_acc: 0.9924\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.99410\n",
            "Epoch 31/100\n",
            "60000/60000 [==============================] - 20s 339us/step - loss: 0.0365 - acc: 0.9888 - val_loss: 0.0201 - val_acc: 0.9937\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.99410\n",
            "Epoch 32/100\n",
            "60000/60000 [==============================] - 20s 339us/step - loss: 0.0376 - acc: 0.9883 - val_loss: 0.0216 - val_acc: 0.9923\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.99410\n",
            "Epoch 33/100\n",
            "60000/60000 [==============================] - 20s 339us/step - loss: 0.0369 - acc: 0.9882 - val_loss: 0.0212 - val_acc: 0.9921\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.99410\n",
            "Epoch 34/100\n",
            "60000/60000 [==============================] - 22s 362us/step - loss: 0.0350 - acc: 0.9887 - val_loss: 0.0188 - val_acc: 0.9938\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.99410\n",
            "Epoch 35/100\n",
            "60000/60000 [==============================] - 20s 340us/step - loss: 0.0348 - acc: 0.9887 - val_loss: 0.0210 - val_acc: 0.9934\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.99410\n",
            "Epoch 36/100\n",
            "60000/60000 [==============================] - 20s 337us/step - loss: 0.0343 - acc: 0.9892 - val_loss: 0.0213 - val_acc: 0.9927\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.99410\n",
            "Epoch 37/100\n",
            "60000/60000 [==============================] - 20s 338us/step - loss: 0.0352 - acc: 0.9888 - val_loss: 0.0206 - val_acc: 0.9936\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.99410\n",
            "Epoch 38/100\n",
            "60000/60000 [==============================] - 22s 361us/step - loss: 0.0338 - acc: 0.9892 - val_loss: 0.0194 - val_acc: 0.9937\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.99410\n",
            "Epoch 39/100\n",
            "60000/60000 [==============================] - 20s 338us/step - loss: 0.0331 - acc: 0.9897 - val_loss: 0.0181 - val_acc: 0.9937\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.99410\n",
            "Epoch 40/100\n",
            "60000/60000 [==============================] - 20s 339us/step - loss: 0.0319 - acc: 0.9897 - val_loss: 0.0200 - val_acc: 0.9931\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.99410\n",
            "Epoch 41/100\n",
            "60000/60000 [==============================] - 21s 351us/step - loss: 0.0338 - acc: 0.9892 - val_loss: 0.0180 - val_acc: 0.9941\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.99410\n",
            "Epoch 42/100\n",
            "60000/60000 [==============================] - 22s 361us/step - loss: 0.0322 - acc: 0.9897 - val_loss: 0.0206 - val_acc: 0.9934\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.99410\n",
            "Epoch 43/100\n",
            "60000/60000 [==============================] - 20s 339us/step - loss: 0.0339 - acc: 0.9889 - val_loss: 0.0199 - val_acc: 0.9941\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.99410\n",
            "Epoch 44/100\n",
            "60000/60000 [==============================] - 20s 339us/step - loss: 0.0323 - acc: 0.9896 - val_loss: 0.0198 - val_acc: 0.9925\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.99410\n",
            "Epoch 45/100\n",
            "60000/60000 [==============================] - 22s 366us/step - loss: 0.0326 - acc: 0.9899 - val_loss: 0.0223 - val_acc: 0.9932\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.99410\n",
            "Epoch 46/100\n",
            "60000/60000 [==============================] - 22s 362us/step - loss: 0.0320 - acc: 0.9897 - val_loss: 0.0226 - val_acc: 0.9931\n",
            "\n",
            "Epoch 00046: val_acc did not improve from 0.99410\n",
            "Epoch 47/100\n",
            "60000/60000 [==============================] - 20s 339us/step - loss: 0.0314 - acc: 0.9899 - val_loss: 0.0198 - val_acc: 0.9936\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.99410\n",
            "Epoch 48/100\n",
            "60000/60000 [==============================] - 20s 338us/step - loss: 0.0320 - acc: 0.9894 - val_loss: 0.0216 - val_acc: 0.9927\n",
            "\n",
            "Epoch 00048: val_acc did not improve from 0.99410\n",
            "Epoch 49/100\n",
            "60000/60000 [==============================] - 21s 344us/step - loss: 0.0312 - acc: 0.9894 - val_loss: 0.0183 - val_acc: 0.9943\n",
            "\n",
            "Epoch 00049: val_acc improved from 0.99410 to 0.99430, saving model to /gdrive/My Drive/EVA/Session3/model_customv2_mnist_best.h5\n",
            "Epoch 50/100\n",
            "60000/60000 [==============================] - 21s 356us/step - loss: 0.0312 - acc: 0.9898 - val_loss: 0.0199 - val_acc: 0.9936\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.99430\n",
            "Epoch 51/100\n",
            "60000/60000 [==============================] - 20s 339us/step - loss: 0.0300 - acc: 0.9905 - val_loss: 0.0217 - val_acc: 0.9927\n",
            "\n",
            "Epoch 00051: val_acc did not improve from 0.99430\n",
            "Epoch 52/100\n",
            "60000/60000 [==============================] - 20s 340us/step - loss: 0.0300 - acc: 0.9908 - val_loss: 0.0204 - val_acc: 0.9939\n",
            "\n",
            "Epoch 00052: val_acc did not improve from 0.99430\n",
            "Epoch 53/100\n",
            "60000/60000 [==============================] - 21s 350us/step - loss: 0.0296 - acc: 0.9902 - val_loss: 0.0204 - val_acc: 0.9936\n",
            "\n",
            "Epoch 00053: val_acc did not improve from 0.99430\n",
            "Epoch 54/100\n",
            "60000/60000 [==============================] - 21s 348us/step - loss: 0.0304 - acc: 0.9903 - val_loss: 0.0206 - val_acc: 0.9932\n",
            "\n",
            "Epoch 00054: val_acc did not improve from 0.99430\n",
            "Epoch 55/100\n",
            "60000/60000 [==============================] - 20s 340us/step - loss: 0.0283 - acc: 0.9907 - val_loss: 0.0203 - val_acc: 0.9932\n",
            "\n",
            "Epoch 00055: val_acc did not improve from 0.99430\n",
            "Epoch 56/100\n",
            "60000/60000 [==============================] - 21s 351us/step - loss: 0.0290 - acc: 0.9904 - val_loss: 0.0217 - val_acc: 0.9933\n",
            "\n",
            "Epoch 00056: val_acc did not improve from 0.99430\n",
            "Epoch 57/100\n",
            "60000/60000 [==============================] - 21s 358us/step - loss: 0.0305 - acc: 0.9905 - val_loss: 0.0209 - val_acc: 0.9935\n",
            "\n",
            "Epoch 00057: val_acc did not improve from 0.99430\n",
            "Epoch 58/100\n",
            "60000/60000 [==============================] - 20s 341us/step - loss: 0.0311 - acc: 0.9900 - val_loss: 0.0193 - val_acc: 0.9942\n",
            "\n",
            "Epoch 00058: val_acc did not improve from 0.99430\n",
            "Epoch 59/100\n",
            "60000/60000 [==============================] - 21s 345us/step - loss: 0.0293 - acc: 0.9905 - val_loss: 0.0191 - val_acc: 0.9932\n",
            "\n",
            "Epoch 00059: val_acc did not improve from 0.99430\n",
            "Epoch 60/100\n",
            "60000/60000 [==============================] - 22s 359us/step - loss: 0.0288 - acc: 0.9909 - val_loss: 0.0192 - val_acc: 0.9934\n",
            "\n",
            "Epoch 00060: val_acc did not improve from 0.99430\n",
            "Epoch 61/100\n",
            "60000/60000 [==============================] - 22s 360us/step - loss: 0.0277 - acc: 0.9910 - val_loss: 0.0185 - val_acc: 0.9947\n",
            "\n",
            "Epoch 00061: val_acc improved from 0.99430 to 0.99470, saving model to /gdrive/My Drive/EVA/Session3/model_customv2_mnist_best.h5\n",
            "Epoch 62/100\n",
            "60000/60000 [==============================] - 20s 339us/step - loss: 0.0297 - acc: 0.9901 - val_loss: 0.0172 - val_acc: 0.9950\n",
            "\n",
            "Epoch 00062: val_acc improved from 0.99470 to 0.99500, saving model to /gdrive/My Drive/EVA/Session3/model_customv2_mnist_best.h5\n",
            "Epoch 63/100\n",
            "60000/60000 [==============================] - 20s 339us/step - loss: 0.0278 - acc: 0.9910 - val_loss: 0.0202 - val_acc: 0.9939\n",
            "\n",
            "Epoch 00063: val_acc did not improve from 0.99500\n",
            "Epoch 64/100\n",
            "60000/60000 [==============================] - 20s 338us/step - loss: 0.0303 - acc: 0.9905 - val_loss: 0.0183 - val_acc: 0.9935\n",
            "\n",
            "Epoch 00064: val_acc did not improve from 0.99500\n",
            "Epoch 65/100\n",
            "60000/60000 [==============================] - 22s 360us/step - loss: 0.0289 - acc: 0.9909 - val_loss: 0.0201 - val_acc: 0.9932\n",
            "\n",
            "Epoch 00065: val_acc did not improve from 0.99500\n",
            "Epoch 66/100\n",
            "60000/60000 [==============================] - 20s 338us/step - loss: 0.0274 - acc: 0.9908 - val_loss: 0.0202 - val_acc: 0.9936\n",
            "\n",
            "Epoch 00066: val_acc did not improve from 0.99500\n",
            "Epoch 67/100\n",
            "60000/60000 [==============================] - 20s 339us/step - loss: 0.0292 - acc: 0.9904 - val_loss: 0.0201 - val_acc: 0.9934\n",
            "\n",
            "Epoch 00067: val_acc did not improve from 0.99500\n",
            "Epoch 68/100\n",
            "60000/60000 [==============================] - 20s 338us/step - loss: 0.0280 - acc: 0.9912 - val_loss: 0.0190 - val_acc: 0.9935\n",
            "\n",
            "Epoch 00068: val_acc did not improve from 0.99500\n",
            "Epoch 69/100\n",
            "60000/60000 [==============================] - 22s 361us/step - loss: 0.0263 - acc: 0.9915 - val_loss: 0.0200 - val_acc: 0.9933\n",
            "\n",
            "Epoch 00069: val_acc did not improve from 0.99500\n",
            "Epoch 70/100\n",
            "60000/60000 [==============================] - 21s 350us/step - loss: 0.0287 - acc: 0.9909 - val_loss: 0.0223 - val_acc: 0.9930\n",
            "\n",
            "Epoch 00070: val_acc did not improve from 0.99500\n",
            "Epoch 71/100\n",
            "60000/60000 [==============================] - 20s 339us/step - loss: 0.0260 - acc: 0.9915 - val_loss: 0.0200 - val_acc: 0.9937\n",
            "\n",
            "Epoch 00071: val_acc did not improve from 0.99500\n",
            "Epoch 72/100\n",
            "60000/60000 [==============================] - 20s 339us/step - loss: 0.0273 - acc: 0.9914 - val_loss: 0.0211 - val_acc: 0.9938\n",
            "\n",
            "Epoch 00072: val_acc did not improve from 0.99500\n",
            "Epoch 73/100\n",
            "60000/60000 [==============================] - 22s 360us/step - loss: 0.0278 - acc: 0.9910 - val_loss: 0.0210 - val_acc: 0.9941\n",
            "\n",
            "Epoch 00073: val_acc did not improve from 0.99500\n",
            "Epoch 74/100\n",
            "60000/60000 [==============================] - 21s 353us/step - loss: 0.0264 - acc: 0.9916 - val_loss: 0.0196 - val_acc: 0.9936\n",
            "\n",
            "Epoch 00074: val_acc did not improve from 0.99500\n",
            "Epoch 75/100\n",
            "60000/60000 [==============================] - 21s 351us/step - loss: 0.0275 - acc: 0.9915 - val_loss: 0.0207 - val_acc: 0.9932\n",
            "\n",
            "Epoch 00075: val_acc did not improve from 0.99500\n",
            "Epoch 76/100\n",
            "60000/60000 [==============================] - 21s 346us/step - loss: 0.0269 - acc: 0.9912 - val_loss: 0.0206 - val_acc: 0.9930\n",
            "\n",
            "Epoch 00076: val_acc did not improve from 0.99500\n",
            "Epoch 77/100\n",
            "60000/60000 [==============================] - 21s 353us/step - loss: 0.0275 - acc: 0.9910 - val_loss: 0.0230 - val_acc: 0.9927\n",
            "\n",
            "Epoch 00077: val_acc did not improve from 0.99500\n",
            "Epoch 78/100\n",
            "60000/60000 [==============================] - 20s 339us/step - loss: 0.0263 - acc: 0.9917 - val_loss: 0.0200 - val_acc: 0.9939\n",
            "\n",
            "Epoch 00078: val_acc did not improve from 0.99500\n",
            "Epoch 79/100\n",
            "60000/60000 [==============================] - 20s 338us/step - loss: 0.0265 - acc: 0.9916 - val_loss: 0.0186 - val_acc: 0.9946\n",
            "\n",
            "Epoch 00079: val_acc did not improve from 0.99500\n",
            "Epoch 80/100\n",
            "60000/60000 [==============================] - 21s 350us/step - loss: 0.0259 - acc: 0.9916 - val_loss: 0.0213 - val_acc: 0.9935\n",
            "\n",
            "Epoch 00080: val_acc did not improve from 0.99500\n",
            "Epoch 81/100\n",
            "60000/60000 [==============================] - 21s 349us/step - loss: 0.0257 - acc: 0.9917 - val_loss: 0.0202 - val_acc: 0.9939\n",
            "\n",
            "Epoch 00081: val_acc did not improve from 0.99500\n",
            "Epoch 82/100\n",
            "60000/60000 [==============================] - 20s 339us/step - loss: 0.0263 - acc: 0.9914 - val_loss: 0.0186 - val_acc: 0.9940\n",
            "\n",
            "Epoch 00082: val_acc did not improve from 0.99500\n",
            "Epoch 83/100\n",
            "60000/60000 [==============================] - 20s 339us/step - loss: 0.0248 - acc: 0.9920 - val_loss: 0.0184 - val_acc: 0.9941\n",
            "\n",
            "Epoch 00083: val_acc did not improve from 0.99500\n",
            "Epoch 84/100\n",
            "60000/60000 [==============================] - 21s 356us/step - loss: 0.0246 - acc: 0.9918 - val_loss: 0.0181 - val_acc: 0.9947\n",
            "\n",
            "Epoch 00084: val_acc did not improve from 0.99500\n",
            "Epoch 85/100\n",
            "60000/60000 [==============================] - 21s 355us/step - loss: 0.0266 - acc: 0.9918 - val_loss: 0.0207 - val_acc: 0.9932\n",
            "\n",
            "Epoch 00085: val_acc did not improve from 0.99500\n",
            "Epoch 86/100\n",
            "60000/60000 [==============================] - 20s 339us/step - loss: 0.0258 - acc: 0.9917 - val_loss: 0.0181 - val_acc: 0.9942\n",
            "\n",
            "Epoch 00086: val_acc did not improve from 0.99500\n",
            "Epoch 87/100\n",
            "60000/60000 [==============================] - 20s 337us/step - loss: 0.0258 - acc: 0.9914 - val_loss: 0.0195 - val_acc: 0.9933\n",
            "\n",
            "Epoch 00087: val_acc did not improve from 0.99500\n",
            "Epoch 88/100\n",
            "60000/60000 [==============================] - 22s 360us/step - loss: 0.0246 - acc: 0.9923 - val_loss: 0.0210 - val_acc: 0.9939\n",
            "\n",
            "Epoch 00088: val_acc did not improve from 0.99500\n",
            "Epoch 89/100\n",
            "60000/60000 [==============================] - 22s 360us/step - loss: 0.0251 - acc: 0.9918 - val_loss: 0.0183 - val_acc: 0.9938\n",
            "\n",
            "Epoch 00089: val_acc did not improve from 0.99500\n",
            "Epoch 90/100\n",
            "60000/60000 [==============================] - 21s 345us/step - loss: 0.0254 - acc: 0.9915 - val_loss: 0.0189 - val_acc: 0.9938\n",
            "\n",
            "Epoch 00090: val_acc did not improve from 0.99500\n",
            "Epoch 91/100\n",
            "60000/60000 [==============================] - 20s 338us/step - loss: 0.0266 - acc: 0.9914 - val_loss: 0.0202 - val_acc: 0.9938\n",
            "\n",
            "Epoch 00091: val_acc did not improve from 0.99500\n",
            "Epoch 92/100\n",
            "60000/60000 [==============================] - 22s 362us/step - loss: 0.0246 - acc: 0.9917 - val_loss: 0.0210 - val_acc: 0.9934\n",
            "\n",
            "Epoch 00092: val_acc did not improve from 0.99500\n",
            "Epoch 93/100\n",
            "60000/60000 [==============================] - 20s 338us/step - loss: 0.0243 - acc: 0.9923 - val_loss: 0.0188 - val_acc: 0.9941\n",
            "\n",
            "Epoch 00093: val_acc did not improve from 0.99500\n",
            "Epoch 94/100\n",
            "60000/60000 [==============================] - 20s 339us/step - loss: 0.0244 - acc: 0.9919 - val_loss: 0.0187 - val_acc: 0.9940\n",
            "\n",
            "Epoch 00094: val_acc did not improve from 0.99500\n",
            "Epoch 95/100\n",
            "60000/60000 [==============================] - 20s 339us/step - loss: 0.0247 - acc: 0.9919 - val_loss: 0.0199 - val_acc: 0.9936\n",
            "\n",
            "Epoch 00095: val_acc did not improve from 0.99500\n",
            "Epoch 96/100\n",
            "60000/60000 [==============================] - 22s 362us/step - loss: 0.0258 - acc: 0.9914 - val_loss: 0.0197 - val_acc: 0.9940\n",
            "\n",
            "Epoch 00096: val_acc did not improve from 0.99500\n",
            "Epoch 97/100\n",
            "60000/60000 [==============================] - 20s 339us/step - loss: 0.0235 - acc: 0.9925 - val_loss: 0.0213 - val_acc: 0.9935\n",
            "\n",
            "Epoch 00097: val_acc did not improve from 0.99500\n",
            "Epoch 98/100\n",
            "60000/60000 [==============================] - 20s 339us/step - loss: 0.0225 - acc: 0.9927 - val_loss: 0.0187 - val_acc: 0.9941\n",
            "\n",
            "Epoch 00098: val_acc did not improve from 0.99500\n",
            "Epoch 99/100\n",
            "60000/60000 [==============================] - 20s 338us/step - loss: 0.0243 - acc: 0.9922 - val_loss: 0.0204 - val_acc: 0.9935\n",
            "\n",
            "Epoch 00099: val_acc did not improve from 0.99500\n",
            "Epoch 100/100\n",
            "60000/60000 [==============================] - 22s 362us/step - loss: 0.0228 - acc: 0.9925 - val_loss: 0.0202 - val_acc: 0.9944\n",
            "\n",
            "Epoch 00100: val_acc did not improve from 0.99500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2e70160978>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzdBpx25rT1B",
        "colab_type": "text"
      },
      "source": [
        "####Using Batch Normalization we obtained a validation accuracy of 99.5 (reached in epoch 62) after running 100 epochs . "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOJiX2HlZY25",
        "colab_type": "text"
      },
      "source": [
        "### Model Version 3 : Let us try the same network with Dropout\n",
        "\n",
        "Basically Dropout helps in regularizing a network against overrfitting by randomly dropping a proportion of the signals while training. The absence of some of the units force the rest of the active units to learn better . \n",
        "The concept of Dropout was first present in this paper http://jmlr.org/papers/v15/srivastava14a.html.\n",
        "Let us drop 1/3 of the signals at a couple of places where we feel the network might have already learnt something useful and may have some redundancy . "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "X3tmXaz5ZBMC",
        "colab": {}
      },
      "source": [
        "np.random.seed(seed=42)  # set a random seed for reproducing random values \n",
        "\n",
        "# instantiate a sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# add the first convolution layer - 8 numbers of 3x3 filters , \n",
        "#This layer sees the input image of 28x28 x 1 channel . \n",
        "\n",
        "model.add(Conv2D(8, (3, 3), input_shape=(28,28,1), use_bias=False))  # remove bias param by setting it to false \n",
        "model.add(BatchNormalization())  #add Batch normalization  \n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "\n",
        "# Now the global receptive field is 3 x 3 \n",
        "\n",
        "# second convolution layer - 8 filters of shape  3x3x8 \n",
        "#input from previous layer is 26 x 26 x 8 . \n",
        "\n",
        "model.add(Conv2D(8, 3, use_bias=False))  # remove bias param by setting it to false \n",
        "model.add(BatchNormalization())  #add Batch normalization  \n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "\n",
        "#Global receptive field is 5x5\n",
        "\n",
        "# Add convolution layer - 8 filters of shape  3x3x8 \n",
        "#input from previous layer is 24 x 24 x 8 . \n",
        "\n",
        "model.add(Conv2D(8, 3, use_bias=False))  # remove bias param by setting it to false \n",
        "model.add(BatchNormalization())  #add Batch normalization  \n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "\n",
        "\n",
        "#Global receptive field is 7x7\n",
        "\n",
        "# Perform 2x2 max pooling  . \n",
        "#Input from previous layer is 22 x 22 X 8 \n",
        "\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "model.add(Dropout(0.33))   # add Dropout for 1/3 of the nodes \n",
        "\n",
        "# After max pooling , dimensions reduce by half , i.e they become 11 x 11 . \n",
        "# Maxpooling (withpool size 2 and stride 1) doubles receptive field . \n",
        "# So global receptive field after max pooling is 14 x 14\n",
        "\n",
        "# Add convolution layer - 16 filters of shape 3x3x8 \n",
        "#input from max pooling operation is 11 x 11 x 8 .  \n",
        "\n",
        "model.add(Conv2D(16, 3,  use_bias=False))  # remove bias param by setting it to false \n",
        "model.add(BatchNormalization())  #add Batch normalization  \n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "\n",
        "#Global receptive field is now 16 x 16 \n",
        "\n",
        "# Add convolution layer - 16 filters of shape 3x3x16 \n",
        "#input coming from previous layer is 9 x 9 x 16 . \n",
        "\n",
        "model.add(Conv2D(16, 3,  use_bias=False)) # remove bias param by setting it to false \n",
        "model.add(BatchNormalization())  #add Batch normalization  \n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "\n",
        "#  Global receptive field is now 18 x 18 \n",
        "\n",
        "# Add convolution layer - 16 filters of shape 3x3x16 \n",
        "#input coming from previous layer is 7 x 7 x 16 .\n",
        "\n",
        "model.add(Conv2D(16, 3,  use_bias=False))  # remove bias param by setting it to false \n",
        "model.add(BatchNormalization())  #add Batch normalization  \n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "\n",
        "#Global receptive field is now 20 x 20 \n",
        "\n",
        "model.add(Dropout(0.33)) # add Dropout for 1/3 of the nodes\n",
        "\n",
        "# Add convolution layer - 16 filters of shape 3x3x16 \n",
        "#input coming from previous layer is 5 x 5 x 16 .\n",
        "\n",
        "model.add(Conv2D(16, 3,  use_bias=False))  # remove bias param by setting it to false \n",
        "model.add(BatchNormalization())  #add Batch normalization  \n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "\n",
        "#Global receptive field is now 22 x 22 \n",
        "\n",
        "# Add convolution layer - 16 filters of shape 3x3x16 \n",
        "#input coming from previous layer is 3 x 3 x 16 .\n",
        "\n",
        "model.add(Conv2D(16, 3,  use_bias=False))  # remove bias param by setting it to false \n",
        "model.add(BatchNormalization())  #add Batch normalization  \n",
        "model.add(Activation('relu'))    # use ReLU activation function .\n",
        "\n",
        "#Global receptive field is now 24 x 24 \n",
        "\n",
        "# Add 1x1 convolution layer - 10 filters of shape 1x1x16 . \n",
        "# This combines the 16 channels from previous layer to 10 channels \n",
        "\n",
        "model.add(Conv2D(10, 1,  use_bias=False))  # remove bias param by setting it to false . \n",
        "# Note absence of ReLU activation here \n",
        "\n",
        "model.add(Flatten())  # Flatten the 2d array to 1d input for the softmax activation \n",
        "model.add(Activation('softmax'))   # Softmax activation to out class probabilities "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "6620b2c4-097e-4622-8453-1c26b2c10dd6",
        "id": "GqBWFl8laMUQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1156
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_19 (Conv2D)           (None, 26, 26, 8)         72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 26, 26, 8)         32        \n",
            "_________________________________________________________________\n",
            "activation_19 (Activation)   (None, 26, 26, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 24, 24, 8)         576       \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 24, 24, 8)         32        \n",
            "_________________________________________________________________\n",
            "activation_20 (Activation)   (None, 24, 24, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_21 (Conv2D)           (None, 22, 22, 8)         576       \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 22, 22, 8)         32        \n",
            "_________________________________________________________________\n",
            "activation_21 (Activation)   (None, 22, 22, 8)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 11, 11, 8)         0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 11, 11, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 9, 9, 16)          1152      \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 9, 9, 16)          64        \n",
            "_________________________________________________________________\n",
            "activation_22 (Activation)   (None, 9, 9, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 7, 7, 16)          2304      \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 7, 7, 16)          64        \n",
            "_________________________________________________________________\n",
            "activation_23 (Activation)   (None, 7, 7, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_24 (Conv2D)           (None, 5, 5, 16)          2304      \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 5, 5, 16)          64        \n",
            "_________________________________________________________________\n",
            "activation_24 (Activation)   (None, 5, 5, 16)          0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 5, 5, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_25 (Conv2D)           (None, 3, 3, 16)          2304      \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 3, 3, 16)          64        \n",
            "_________________________________________________________________\n",
            "activation_25 (Activation)   (None, 3, 3, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_26 (Conv2D)           (None, 1, 1, 16)          2304      \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 1, 1, 16)          64        \n",
            "_________________________________________________________________\n",
            "activation_26 (Activation)   (None, 1, 1, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_27 (Conv2D)           (None, 1, 1, 10)          160       \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_27 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 12,168\n",
            "Trainable params: 11,960\n",
            "Non-trainable params: 208\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gfliPlleaMUT",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "  \n",
        "chkpoint_model=ModelCheckpoint(\"/gdrive/My Drive/EVA/Session3/model_customv3_mnist_best.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='max')  \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "anGLyyJJaMUV",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "7271de7c-9952-4999-dcbf-d5f2659c52c2",
        "id": "2_vP9l9raMUW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6871
        }
      },
      "source": [
        "model.fit(X_train, Y_train, validation_data=(X_test,Y_test),batch_size=32, epochs=100, verbose=1, callbacks=[chkpoint_model])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/100\n",
            "60000/60000 [==============================] - 22s 365us/step - loss: 0.5319 - acc: 0.8466 - val_loss: 0.0820 - val_acc: 0.9761\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.97610, saving model to /gdrive/My Drive/EVA/Session3/model_customv3_mnist_best.h5\n",
            "Epoch 2/100\n",
            "60000/60000 [==============================] - 22s 359us/step - loss: 0.1389 - acc: 0.9581 - val_loss: 0.0531 - val_acc: 0.9837\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.97610 to 0.98370, saving model to /gdrive/My Drive/EVA/Session3/model_customv3_mnist_best.h5\n",
            "Epoch 3/100\n",
            "60000/60000 [==============================] - 21s 344us/step - loss: 0.1030 - acc: 0.9685 - val_loss: 0.0429 - val_acc: 0.9867\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.98370 to 0.98670, saving model to /gdrive/My Drive/EVA/Session3/model_customv3_mnist_best.h5\n",
            "Epoch 4/100\n",
            "60000/60000 [==============================] - 20s 341us/step - loss: 0.0898 - acc: 0.9732 - val_loss: 0.0472 - val_acc: 0.9868\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.98670 to 0.98680, saving model to /gdrive/My Drive/EVA/Session3/model_customv3_mnist_best.h5\n",
            "Epoch 5/100\n",
            "60000/60000 [==============================] - 20s 341us/step - loss: 0.0790 - acc: 0.9759 - val_loss: 0.0391 - val_acc: 0.9871\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.98680 to 0.98710, saving model to /gdrive/My Drive/EVA/Session3/model_customv3_mnist_best.h5\n",
            "Epoch 6/100\n",
            "60000/60000 [==============================] - 22s 367us/step - loss: 0.0769 - acc: 0.9762 - val_loss: 0.0374 - val_acc: 0.9880\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.98710 to 0.98800, saving model to /gdrive/My Drive/EVA/Session3/model_customv3_mnist_best.h5\n",
            "Epoch 7/100\n",
            "60000/60000 [==============================] - 20s 342us/step - loss: 0.0667 - acc: 0.9799 - val_loss: 0.0309 - val_acc: 0.9892\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.98800 to 0.98920, saving model to /gdrive/My Drive/EVA/Session3/model_customv3_mnist_best.h5\n",
            "Epoch 8/100\n",
            "60000/60000 [==============================] - 20s 341us/step - loss: 0.0649 - acc: 0.9796 - val_loss: 0.0286 - val_acc: 0.9913\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.98920 to 0.99130, saving model to /gdrive/My Drive/EVA/Session3/model_customv3_mnist_best.h5\n",
            "Epoch 9/100\n",
            "60000/60000 [==============================] - 20s 341us/step - loss: 0.0635 - acc: 0.9804 - val_loss: 0.0310 - val_acc: 0.9913\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.99130\n",
            "Epoch 10/100\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0593 - acc: 0.9813 - val_loss: 0.0303 - val_acc: 0.9897\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.99130\n",
            "Epoch 11/100\n",
            "60000/60000 [==============================] - 21s 342us/step - loss: 0.0574 - acc: 0.9824 - val_loss: 0.0257 - val_acc: 0.9915\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.99130 to 0.99150, saving model to /gdrive/My Drive/EVA/Session3/model_customv3_mnist_best.h5\n",
            "Epoch 12/100\n",
            "60000/60000 [==============================] - 22s 367us/step - loss: 0.0532 - acc: 0.9835 - val_loss: 0.0250 - val_acc: 0.9922\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.99150 to 0.99220, saving model to /gdrive/My Drive/EVA/Session3/model_customv3_mnist_best.h5\n",
            "Epoch 13/100\n",
            "60000/60000 [==============================] - 20s 342us/step - loss: 0.0534 - acc: 0.9834 - val_loss: 0.0277 - val_acc: 0.9906\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.99220\n",
            "Epoch 14/100\n",
            "60000/60000 [==============================] - 22s 362us/step - loss: 0.0520 - acc: 0.9837 - val_loss: 0.0241 - val_acc: 0.9926\n",
            "\n",
            "Epoch 00014: val_acc improved from 0.99220 to 0.99260, saving model to /gdrive/My Drive/EVA/Session3/model_customv3_mnist_best.h5\n",
            "Epoch 15/100\n",
            "60000/60000 [==============================] - 20s 341us/step - loss: 0.0478 - acc: 0.9852 - val_loss: 0.0272 - val_acc: 0.9916\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.99260\n",
            "Epoch 16/100\n",
            "60000/60000 [==============================] - 20s 341us/step - loss: 0.0470 - acc: 0.9852 - val_loss: 0.0244 - val_acc: 0.9924\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.99260\n",
            "Epoch 17/100\n",
            "60000/60000 [==============================] - 21s 351us/step - loss: 0.0470 - acc: 0.9855 - val_loss: 0.0222 - val_acc: 0.9928\n",
            "\n",
            "Epoch 00017: val_acc improved from 0.99260 to 0.99280, saving model to /gdrive/My Drive/EVA/Session3/model_customv3_mnist_best.h5\n",
            "Epoch 18/100\n",
            "60000/60000 [==============================] - 21s 352us/step - loss: 0.0457 - acc: 0.9856 - val_loss: 0.0242 - val_acc: 0.9926\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.99280\n",
            "Epoch 19/100\n",
            "60000/60000 [==============================] - 20s 341us/step - loss: 0.0461 - acc: 0.9862 - val_loss: 0.0209 - val_acc: 0.9931\n",
            "\n",
            "Epoch 00019: val_acc improved from 0.99280 to 0.99310, saving model to /gdrive/My Drive/EVA/Session3/model_customv3_mnist_best.h5\n",
            "Epoch 20/100\n",
            "60000/60000 [==============================] - 21s 354us/step - loss: 0.0429 - acc: 0.9864 - val_loss: 0.0217 - val_acc: 0.9930\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.99310\n",
            "Epoch 21/100\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0424 - acc: 0.9867 - val_loss: 0.0208 - val_acc: 0.9935\n",
            "\n",
            "Epoch 00021: val_acc improved from 0.99310 to 0.99350, saving model to /gdrive/My Drive/EVA/Session3/model_customv3_mnist_best.h5\n",
            "Epoch 22/100\n",
            "60000/60000 [==============================] - 20s 342us/step - loss: 0.0433 - acc: 0.9861 - val_loss: 0.0231 - val_acc: 0.9926\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.99350\n",
            "Epoch 23/100\n",
            "60000/60000 [==============================] - 21s 342us/step - loss: 0.0407 - acc: 0.9870 - val_loss: 0.0279 - val_acc: 0.9919\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.99350\n",
            "Epoch 24/100\n",
            "60000/60000 [==============================] - 20s 341us/step - loss: 0.0391 - acc: 0.9876 - val_loss: 0.0238 - val_acc: 0.9925\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.99350\n",
            "Epoch 25/100\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0416 - acc: 0.9870 - val_loss: 0.0231 - val_acc: 0.9934\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.99350\n",
            "Epoch 26/100\n",
            "60000/60000 [==============================] - 22s 362us/step - loss: 0.0390 - acc: 0.9877 - val_loss: 0.0196 - val_acc: 0.9937\n",
            "\n",
            "Epoch 00026: val_acc improved from 0.99350 to 0.99370, saving model to /gdrive/My Drive/EVA/Session3/model_customv3_mnist_best.h5\n",
            "Epoch 27/100\n",
            "60000/60000 [==============================] - 21s 347us/step - loss: 0.0386 - acc: 0.9881 - val_loss: 0.0209 - val_acc: 0.9931\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.99370\n",
            "Epoch 28/100\n",
            "60000/60000 [==============================] - 20s 340us/step - loss: 0.0408 - acc: 0.9870 - val_loss: 0.0222 - val_acc: 0.9934\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.99370\n",
            "Epoch 29/100\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0373 - acc: 0.9885 - val_loss: 0.0215 - val_acc: 0.9933\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.99370\n",
            "Epoch 30/100\n",
            "60000/60000 [==============================] - 20s 341us/step - loss: 0.0373 - acc: 0.9883 - val_loss: 0.0259 - val_acc: 0.9916\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.99370\n",
            "Epoch 31/100\n",
            "60000/60000 [==============================] - 20s 341us/step - loss: 0.0366 - acc: 0.9883 - val_loss: 0.0222 - val_acc: 0.9927\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.99370\n",
            "Epoch 32/100\n",
            "60000/60000 [==============================] - 20s 341us/step - loss: 0.0375 - acc: 0.9886 - val_loss: 0.0231 - val_acc: 0.9925\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.99370\n",
            "Epoch 33/100\n",
            "60000/60000 [==============================] - 22s 364us/step - loss: 0.0362 - acc: 0.9880 - val_loss: 0.0209 - val_acc: 0.9934\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.99370\n",
            "Epoch 34/100\n",
            "60000/60000 [==============================] - 20s 341us/step - loss: 0.0353 - acc: 0.9889 - val_loss: 0.0201 - val_acc: 0.9941\n",
            "\n",
            "Epoch 00034: val_acc improved from 0.99370 to 0.99410, saving model to /gdrive/My Drive/EVA/Session3/model_customv3_mnist_best.h5\n",
            "Epoch 35/100\n",
            "60000/60000 [==============================] - 21s 354us/step - loss: 0.0349 - acc: 0.9889 - val_loss: 0.0198 - val_acc: 0.9939\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.99410\n",
            "Epoch 36/100\n",
            "60000/60000 [==============================] - 21s 348us/step - loss: 0.0346 - acc: 0.9892 - val_loss: 0.0207 - val_acc: 0.9932\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.99410\n",
            "Epoch 37/100\n",
            "60000/60000 [==============================] - 21s 357us/step - loss: 0.0354 - acc: 0.9887 - val_loss: 0.0210 - val_acc: 0.9922\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.99410\n",
            "Epoch 38/100\n",
            "60000/60000 [==============================] - 20s 341us/step - loss: 0.0331 - acc: 0.9899 - val_loss: 0.0198 - val_acc: 0.9935\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.99410\n",
            "Epoch 39/100\n",
            "60000/60000 [==============================] - 20s 341us/step - loss: 0.0348 - acc: 0.9892 - val_loss: 0.0195 - val_acc: 0.9943\n",
            "\n",
            "Epoch 00039: val_acc improved from 0.99410 to 0.99430, saving model to /gdrive/My Drive/EVA/Session3/model_customv3_mnist_best.h5\n",
            "Epoch 40/100\n",
            "60000/60000 [==============================] - 21s 356us/step - loss: 0.0324 - acc: 0.9900 - val_loss: 0.0186 - val_acc: 0.9939\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.99430\n",
            "Epoch 41/100\n",
            "60000/60000 [==============================] - 23s 376us/step - loss: 0.0338 - acc: 0.9896 - val_loss: 0.0193 - val_acc: 0.9942\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.99430\n",
            "Epoch 42/100\n",
            "60000/60000 [==============================] - 21s 342us/step - loss: 0.0333 - acc: 0.9894 - val_loss: 0.0211 - val_acc: 0.9933\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.99430\n",
            "Epoch 43/100\n",
            "60000/60000 [==============================] - 21s 342us/step - loss: 0.0331 - acc: 0.9886 - val_loss: 0.0225 - val_acc: 0.9926\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.99430\n",
            "Epoch 44/100\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0318 - acc: 0.9896 - val_loss: 0.0238 - val_acc: 0.9921\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.99430\n",
            "Epoch 45/100\n",
            "60000/60000 [==============================] - 20s 341us/step - loss: 0.0324 - acc: 0.9901 - val_loss: 0.0219 - val_acc: 0.9932\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.99430\n",
            "Epoch 46/100\n",
            "60000/60000 [==============================] - 21s 342us/step - loss: 0.0308 - acc: 0.9899 - val_loss: 0.0193 - val_acc: 0.9933\n",
            "\n",
            "Epoch 00046: val_acc did not improve from 0.99430\n",
            "Epoch 47/100\n",
            "60000/60000 [==============================] - 20s 341us/step - loss: 0.0324 - acc: 0.9900 - val_loss: 0.0190 - val_acc: 0.9940\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.99430\n",
            "Epoch 48/100\n",
            "60000/60000 [==============================] - 22s 365us/step - loss: 0.0333 - acc: 0.9892 - val_loss: 0.0215 - val_acc: 0.9931\n",
            "\n",
            "Epoch 00048: val_acc did not improve from 0.99430\n",
            "Epoch 49/100\n",
            "60000/60000 [==============================] - 21s 353us/step - loss: 0.0303 - acc: 0.9903 - val_loss: 0.0208 - val_acc: 0.9930\n",
            "\n",
            "Epoch 00049: val_acc did not improve from 0.99430\n",
            "Epoch 50/100\n",
            "60000/60000 [==============================] - 21s 342us/step - loss: 0.0310 - acc: 0.9901 - val_loss: 0.0194 - val_acc: 0.9938\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.99430\n",
            "Epoch 51/100\n",
            "60000/60000 [==============================] - 20s 340us/step - loss: 0.0304 - acc: 0.9901 - val_loss: 0.0219 - val_acc: 0.9922\n",
            "\n",
            "Epoch 00051: val_acc did not improve from 0.99430\n",
            "Epoch 52/100\n",
            "60000/60000 [==============================] - 22s 364us/step - loss: 0.0308 - acc: 0.9902 - val_loss: 0.0184 - val_acc: 0.9934\n",
            "\n",
            "Epoch 00052: val_acc did not improve from 0.99430\n",
            "Epoch 53/100\n",
            "60000/60000 [==============================] - 20s 341us/step - loss: 0.0295 - acc: 0.9907 - val_loss: 0.0220 - val_acc: 0.9923\n",
            "\n",
            "Epoch 00053: val_acc did not improve from 0.99430\n",
            "Epoch 54/100\n",
            "60000/60000 [==============================] - 20s 341us/step - loss: 0.0309 - acc: 0.9902 - val_loss: 0.0214 - val_acc: 0.9930\n",
            "\n",
            "Epoch 00054: val_acc did not improve from 0.99430\n",
            "Epoch 55/100\n",
            "60000/60000 [==============================] - 20s 341us/step - loss: 0.0298 - acc: 0.9901 - val_loss: 0.0180 - val_acc: 0.9945\n",
            "\n",
            "Epoch 00055: val_acc improved from 0.99430 to 0.99450, saving model to /gdrive/My Drive/EVA/Session3/model_customv3_mnist_best.h5\n",
            "Epoch 56/100\n",
            "60000/60000 [==============================] - 23s 380us/step - loss: 0.0288 - acc: 0.9907 - val_loss: 0.0204 - val_acc: 0.9930\n",
            "\n",
            "Epoch 00056: val_acc did not improve from 0.99450\n",
            "Epoch 57/100\n",
            "60000/60000 [==============================] - 21s 342us/step - loss: 0.0297 - acc: 0.9904 - val_loss: 0.0193 - val_acc: 0.9941\n",
            "\n",
            "Epoch 00057: val_acc did not improve from 0.99450\n",
            "Epoch 58/100\n",
            "60000/60000 [==============================] - 21s 342us/step - loss: 0.0313 - acc: 0.9898 - val_loss: 0.0195 - val_acc: 0.9946\n",
            "\n",
            "Epoch 00058: val_acc improved from 0.99450 to 0.99460, saving model to /gdrive/My Drive/EVA/Session3/model_customv3_mnist_best.h5\n",
            "Epoch 59/100\n",
            "60000/60000 [==============================] - 21s 351us/step - loss: 0.0295 - acc: 0.9903 - val_loss: 0.0190 - val_acc: 0.9936\n",
            "\n",
            "Epoch 00059: val_acc did not improve from 0.99460\n",
            "Epoch 60/100\n",
            "60000/60000 [==============================] - 21s 354us/step - loss: 0.0305 - acc: 0.9904 - val_loss: 0.0201 - val_acc: 0.9936\n",
            "\n",
            "Epoch 00060: val_acc did not improve from 0.99460\n",
            "Epoch 61/100\n",
            "60000/60000 [==============================] - 21s 342us/step - loss: 0.0294 - acc: 0.9906 - val_loss: 0.0187 - val_acc: 0.9939\n",
            "\n",
            "Epoch 00061: val_acc did not improve from 0.99460\n",
            "Epoch 62/100\n",
            "60000/60000 [==============================] - 21s 342us/step - loss: 0.0289 - acc: 0.9907 - val_loss: 0.0179 - val_acc: 0.9947\n",
            "\n",
            "Epoch 00062: val_acc improved from 0.99460 to 0.99470, saving model to /gdrive/My Drive/EVA/Session3/model_customv3_mnist_best.h5\n",
            "Epoch 63/100\n",
            "60000/60000 [==============================] - 22s 360us/step - loss: 0.0278 - acc: 0.9909 - val_loss: 0.0187 - val_acc: 0.9941\n",
            "\n",
            "Epoch 00063: val_acc did not improve from 0.99470\n",
            "Epoch 64/100\n",
            "60000/60000 [==============================] - 21s 358us/step - loss: 0.0299 - acc: 0.9909 - val_loss: 0.0177 - val_acc: 0.9937\n",
            "\n",
            "Epoch 00064: val_acc did not improve from 0.99470\n",
            "Epoch 65/100\n",
            "60000/60000 [==============================] - 20s 340us/step - loss: 0.0289 - acc: 0.9910 - val_loss: 0.0185 - val_acc: 0.9938\n",
            "\n",
            "Epoch 00065: val_acc did not improve from 0.99470\n",
            "Epoch 66/100\n",
            "60000/60000 [==============================] - 20s 341us/step - loss: 0.0272 - acc: 0.9910 - val_loss: 0.0209 - val_acc: 0.9929\n",
            "\n",
            "Epoch 00066: val_acc did not improve from 0.99470\n",
            "Epoch 67/100\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0282 - acc: 0.9910 - val_loss: 0.0200 - val_acc: 0.9933\n",
            "\n",
            "Epoch 00067: val_acc did not improve from 0.99470\n",
            "Epoch 68/100\n",
            "60000/60000 [==============================] - 20s 341us/step - loss: 0.0275 - acc: 0.9912 - val_loss: 0.0207 - val_acc: 0.9939\n",
            "\n",
            "Epoch 00068: val_acc did not improve from 0.99470\n",
            "Epoch 69/100\n",
            "60000/60000 [==============================] - 20s 341us/step - loss: 0.0259 - acc: 0.9917 - val_loss: 0.0197 - val_acc: 0.9933\n",
            "\n",
            "Epoch 00069: val_acc did not improve from 0.99470\n",
            "Epoch 70/100\n",
            "60000/60000 [==============================] - 20s 341us/step - loss: 0.0284 - acc: 0.9911 - val_loss: 0.0185 - val_acc: 0.9939\n",
            "\n",
            "Epoch 00070: val_acc did not improve from 0.99470\n",
            "Epoch 71/100\n",
            "60000/60000 [==============================] - 23s 382us/step - loss: 0.0267 - acc: 0.9911 - val_loss: 0.0190 - val_acc: 0.9946\n",
            "\n",
            "Epoch 00071: val_acc did not improve from 0.99470\n",
            "Epoch 72/100\n",
            "60000/60000 [==============================] - 20s 342us/step - loss: 0.0275 - acc: 0.9911 - val_loss: 0.0196 - val_acc: 0.9942\n",
            "\n",
            "Epoch 00072: val_acc did not improve from 0.99470\n",
            "Epoch 73/100\n",
            "60000/60000 [==============================] - 21s 342us/step - loss: 0.0285 - acc: 0.9908 - val_loss: 0.0182 - val_acc: 0.9938\n",
            "\n",
            "Epoch 00073: val_acc did not improve from 0.99470\n",
            "Epoch 74/100\n",
            "60000/60000 [==============================] - 21s 342us/step - loss: 0.0264 - acc: 0.9915 - val_loss: 0.0189 - val_acc: 0.9946\n",
            "\n",
            "Epoch 00074: val_acc did not improve from 0.99470\n",
            "Epoch 75/100\n",
            "60000/60000 [==============================] - 22s 364us/step - loss: 0.0269 - acc: 0.9919 - val_loss: 0.0182 - val_acc: 0.9940\n",
            "\n",
            "Epoch 00075: val_acc did not improve from 0.99470\n",
            "Epoch 76/100\n",
            "60000/60000 [==============================] - 20s 341us/step - loss: 0.0283 - acc: 0.9908 - val_loss: 0.0215 - val_acc: 0.9931\n",
            "\n",
            "Epoch 00076: val_acc did not improve from 0.99470\n",
            "Epoch 77/100\n",
            "60000/60000 [==============================] - 20s 341us/step - loss: 0.0267 - acc: 0.9919 - val_loss: 0.0204 - val_acc: 0.9934\n",
            "\n",
            "Epoch 00077: val_acc did not improve from 0.99470\n",
            "Epoch 78/100\n",
            "60000/60000 [==============================] - 21s 346us/step - loss: 0.0260 - acc: 0.9912 - val_loss: 0.0194 - val_acc: 0.9947\n",
            "\n",
            "Epoch 00078: val_acc did not improve from 0.99470\n",
            "Epoch 79/100\n",
            "60000/60000 [==============================] - 22s 364us/step - loss: 0.0277 - acc: 0.9912 - val_loss: 0.0204 - val_acc: 0.9933\n",
            "\n",
            "Epoch 00079: val_acc did not improve from 0.99470\n",
            "Epoch 80/100\n",
            "60000/60000 [==============================] - 21s 342us/step - loss: 0.0269 - acc: 0.9913 - val_loss: 0.0174 - val_acc: 0.9946\n",
            "\n",
            "Epoch 00080: val_acc did not improve from 0.99470\n",
            "Epoch 81/100\n",
            "60000/60000 [==============================] - 21s 342us/step - loss: 0.0259 - acc: 0.9918 - val_loss: 0.0199 - val_acc: 0.9937\n",
            "\n",
            "Epoch 00081: val_acc did not improve from 0.99470\n",
            "Epoch 82/100\n",
            "60000/60000 [==============================] - 21s 348us/step - loss: 0.0261 - acc: 0.9914 - val_loss: 0.0182 - val_acc: 0.9943\n",
            "\n",
            "Epoch 00082: val_acc did not improve from 0.99470\n",
            "Epoch 83/100\n",
            "60000/60000 [==============================] - 21s 356us/step - loss: 0.0262 - acc: 0.9912 - val_loss: 0.0195 - val_acc: 0.9938\n",
            "\n",
            "Epoch 00083: val_acc did not improve from 0.99470\n",
            "Epoch 84/100\n",
            "60000/60000 [==============================] - 20s 341us/step - loss: 0.0250 - acc: 0.9920 - val_loss: 0.0185 - val_acc: 0.9940\n",
            "\n",
            "Epoch 00084: val_acc did not improve from 0.99470\n",
            "Epoch 85/100\n",
            "60000/60000 [==============================] - 21s 349us/step - loss: 0.0260 - acc: 0.9913 - val_loss: 0.0192 - val_acc: 0.9944\n",
            "\n",
            "Epoch 00085: val_acc did not improve from 0.99470\n",
            "Epoch 86/100\n",
            "60000/60000 [==============================] - 23s 378us/step - loss: 0.0253 - acc: 0.9917 - val_loss: 0.0184 - val_acc: 0.9949\n",
            "\n",
            "Epoch 00086: val_acc improved from 0.99470 to 0.99490, saving model to /gdrive/My Drive/EVA/Session3/model_customv3_mnist_best.h5\n",
            "Epoch 87/100\n",
            "60000/60000 [==============================] - 21s 345us/step - loss: 0.0249 - acc: 0.9920 - val_loss: 0.0191 - val_acc: 0.9940\n",
            "\n",
            "Epoch 00087: val_acc did not improve from 0.99490\n",
            "Epoch 88/100\n",
            "60000/60000 [==============================] - 20s 340us/step - loss: 0.0265 - acc: 0.9914 - val_loss: 0.0180 - val_acc: 0.9941\n",
            "\n",
            "Epoch 00088: val_acc did not improve from 0.99490\n",
            "Epoch 89/100\n",
            "60000/60000 [==============================] - 20s 341us/step - loss: 0.0254 - acc: 0.9917 - val_loss: 0.0183 - val_acc: 0.9945\n",
            "\n",
            "Epoch 00089: val_acc did not improve from 0.99490\n",
            "Epoch 90/100\n",
            "60000/60000 [==============================] - 22s 364us/step - loss: 0.0249 - acc: 0.9920 - val_loss: 0.0194 - val_acc: 0.9944\n",
            "\n",
            "Epoch 00090: val_acc did not improve from 0.99490\n",
            "Epoch 91/100\n",
            "60000/60000 [==============================] - 20s 340us/step - loss: 0.0274 - acc: 0.9913 - val_loss: 0.0187 - val_acc: 0.9944\n",
            "\n",
            "Epoch 00091: val_acc did not improve from 0.99490\n",
            "Epoch 92/100\n",
            "60000/60000 [==============================] - 21s 342us/step - loss: 0.0251 - acc: 0.9922 - val_loss: 0.0173 - val_acc: 0.9954\n",
            "\n",
            "Epoch 00092: val_acc improved from 0.99490 to 0.99540, saving model to /gdrive/My Drive/EVA/Session3/model_customv3_mnist_best.h5\n",
            "Epoch 93/100\n",
            "60000/60000 [==============================] - 21s 355us/step - loss: 0.0244 - acc: 0.9919 - val_loss: 0.0184 - val_acc: 0.9943\n",
            "\n",
            "Epoch 00093: val_acc did not improve from 0.99540\n",
            "Epoch 94/100\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0255 - acc: 0.9915 - val_loss: 0.0177 - val_acc: 0.9946\n",
            "\n",
            "Epoch 00094: val_acc did not improve from 0.99540\n",
            "Epoch 95/100\n",
            "60000/60000 [==============================] - 20s 342us/step - loss: 0.0245 - acc: 0.9922 - val_loss: 0.0185 - val_acc: 0.9942\n",
            "\n",
            "Epoch 00095: val_acc did not improve from 0.99540\n",
            "Epoch 96/100\n",
            "60000/60000 [==============================] - 20s 341us/step - loss: 0.0235 - acc: 0.9922 - val_loss: 0.0191 - val_acc: 0.9941\n",
            "\n",
            "Epoch 00096: val_acc did not improve from 0.99540\n",
            "Epoch 97/100\n",
            "60000/60000 [==============================] - 20s 341us/step - loss: 0.0246 - acc: 0.9919 - val_loss: 0.0183 - val_acc: 0.9946\n",
            "\n",
            "Epoch 00097: val_acc did not improve from 0.99540\n",
            "Epoch 98/100\n",
            "60000/60000 [==============================] - 22s 364us/step - loss: 0.0234 - acc: 0.9921 - val_loss: 0.0184 - val_acc: 0.9945\n",
            "\n",
            "Epoch 00098: val_acc did not improve from 0.99540\n",
            "Epoch 99/100\n",
            "60000/60000 [==============================] - 20s 340us/step - loss: 0.0245 - acc: 0.9918 - val_loss: 0.0194 - val_acc: 0.9942\n",
            "\n",
            "Epoch 00099: val_acc did not improve from 0.99540\n",
            "Epoch 100/100\n",
            "60000/60000 [==============================] - 22s 365us/step - loss: 0.0238 - acc: 0.9920 - val_loss: 0.0185 - val_acc: 0.9944\n",
            "\n",
            "Epoch 00100: val_acc did not improve from 0.99540\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2e40b7d0f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ze0tePutr4P7",
        "colab_type": "text"
      },
      "source": [
        "**Using Dropouts in addition to Batch Normalization we obtained a validation accuracy of 99.54 after 100 epochs**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7fa3y3MszKQ",
        "colab_type": "text"
      },
      "source": [
        "### Let us load the Model with best validation accuracy and print the evaluation score "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvdXCXK2l9KQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model=load_model(\"/gdrive/My Drive/EVA/Session3/model_customv3_mnist_best.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtsH-lLk-eLb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score = model.evaluate(X_test, Y_test, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkX8JMv79q9r",
        "colab_type": "code",
        "outputId": "cc1f21e1-8ad8-47b4-ab80-361c9823d59e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(score)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.017284836861339862, 0.9954]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Acfgy31CtWYc",
        "colab_type": "text"
      },
      "source": [
        "### Predict the classes using model.predict and print predicted probabilities and categorical array for True test classes "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCWoJkwE9suh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym7iCFBm9uBs",
        "colab_type": "code",
        "outputId": "8bebbce4-7afe-48cd-dba5-4c3fbf621b08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "print(y_pred[:9])\n",
        "print(Y_test[:9])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3.0716782e-07 2.1349823e-07 5.6653794e-06 8.9690921e-08 8.2904198e-06\n",
            "  1.3050574e-07 2.7220999e-08 9.9998474e-01 3.9293749e-08 4.9592177e-07]\n",
            " [3.6056872e-07 3.4937703e-08 9.9999952e-01 5.1584973e-08 8.4285473e-10\n",
            "  2.7829195e-10 3.0795448e-08 3.4916052e-08 5.7991127e-08 7.0215789e-10]\n",
            " [1.4904915e-09 9.9998069e-01 1.2089476e-08 2.5599768e-08 7.1812960e-06\n",
            "  9.1240530e-08 1.8259593e-08 1.0810454e-05 3.6627611e-08 1.1785403e-06]\n",
            " [9.9999166e-01 2.7834920e-10 9.7693462e-09 2.7358913e-08 1.2389631e-07\n",
            "  2.0722428e-07 4.4559479e-06 5.3542628e-09 4.0721434e-07 3.1675609e-06]\n",
            " [7.1032065e-09 1.0459565e-08 1.8377417e-09 1.4293353e-10 9.9989951e-01\n",
            "  5.6989091e-08 3.0328856e-08 3.2459763e-08 1.2197431e-06 9.9176425e-05]\n",
            " [1.1293455e-09 9.9999213e-01 4.2469468e-09 1.4581947e-11 2.1753046e-06\n",
            "  3.3279002e-10 1.9187743e-09 4.9808086e-06 2.7484033e-08 7.0956656e-07]\n",
            " [1.6467759e-09 3.9426591e-06 1.1972082e-06 5.6160115e-10 9.9989212e-01\n",
            "  5.6623026e-07 1.3464309e-08 5.1959367e-05 1.9502442e-05 3.0702846e-05]\n",
            " [4.6084692e-09 2.1926196e-07 7.8449466e-07 1.7262514e-06 4.0305997e-03\n",
            "  3.4001064e-06 1.3687441e-08 7.0421208e-08 3.3065226e-05 9.9593002e-01]\n",
            " [2.0777386e-06 1.2228865e-07 1.1665722e-08 2.3172583e-07 7.3847106e-09\n",
            "  9.9211955e-01 7.6240925e-03 2.2470273e-08 2.4945117e-04 4.3746563e-06]]\n",
            "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYPrkzLvt2do",
        "colab_type": "text"
      },
      "source": [
        "### Let us visualize some of the filters in the first convolution layer 'conv2d_19'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CT--y98_dr2T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# form a layer dictionary {name : layer} of all layers in the model \n",
        "\n",
        "layer_dict = dict([(layer.name, layer) for layer in model.layers])  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GY4Upv4dsUR",
        "colab_type": "code",
        "outputId": "4cd05ae8-1fff-4bb1-af89-ae67465c6803",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 771
        }
      },
      "source": [
        " # use matplotlib to visualize the filter arrays \n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from keras import backend as K\n",
        "%matplotlib inline\n",
        "# util function to convert a tensor into a valid image\n",
        "def deprocess_image(x):\n",
        "    # normalize tensor: center on 0., ensure std is 0.1\n",
        "    x -= x.mean()\n",
        "    x /= (x.std() + 1e-5)\n",
        "    x *= 0.1\n",
        "\n",
        "    # clip to [0, 1]\n",
        "    x += 0.5\n",
        "    x = np.clip(x, 0, 1)\n",
        "\n",
        "    # convert to RGB array\n",
        "    x *= 255\n",
        "    #x = x.transpose((1, 2, 0))\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x\n",
        "\n",
        "def vis_img_in_filter(img = np.array(X_train[2]).reshape((1, 28, 28, 1)).astype(np.float64), \n",
        "                      layer_name = 'conv2d_19'):\n",
        "    layer_output = layer_dict[layer_name].output\n",
        "    img_ascs = list()\n",
        "    for filter_index in range(layer_output.shape[3]):\n",
        "        # build a loss function that maximizes the activation\n",
        "        # of the nth filter of the layer considered\n",
        "        loss = K.mean(layer_output[:, :, :, filter_index])\n",
        "\n",
        "        # compute the gradient of the input picture wrt this loss\n",
        "        grads = K.gradients(loss, model.input)[0]\n",
        "\n",
        "        # normalization trick: we normalize the gradient\n",
        "        grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)\n",
        "\n",
        "        # this function returns the loss and grads given the input picture\n",
        "        iterate = K.function([model.input], [loss, grads])\n",
        "\n",
        "        # step size for gradient ascent\n",
        "        step = 5.\n",
        "\n",
        "        img_asc = np.array(img)\n",
        "        # run gradient ascent for 20 steps\n",
        "        for i in range(20):\n",
        "            loss_value, grads_value = iterate([img_asc])\n",
        "            img_asc += grads_value * step\n",
        "\n",
        "        img_asc = img_asc[0]\n",
        "        img_ascs.append(deprocess_image(img_asc).reshape((28, 28)))\n",
        "        \n",
        "    if layer_output.shape[3] >= 35:\n",
        "        plot_x, plot_y = 6, 6\n",
        "    elif layer_output.shape[3] >= 23:\n",
        "        plot_x, plot_y = 4, 6\n",
        "    elif layer_output.shape[3] >= 11:\n",
        "        plot_x, plot_y = 2, 6\n",
        "    elif layer_output.shape[3] >= 8:\n",
        "        plot_x, plot_y = 2, 4   \n",
        "    else:\n",
        "        \n",
        "        plot_x, plot_y = 2, 2\n",
        "    fig, ax = plt.subplots(plot_x, plot_y, figsize = (12, 12))\n",
        "    \n",
        "    ax[0,0].imshow(img.reshape((28, 28)), cmap = 'gray')\n",
        "    ax[0,0].set_title('Input image')\n",
        "    fig.suptitle('Input image and %s filters' % (layer_name,))\n",
        "    fig.tight_layout(pad = 0.3, rect = [0, 0, 0.9, 0.9])\n",
        "    for (x, y) in [(i, j) for i in range(plot_x) for j in range(plot_y)]:\n",
        "        if x == 0 and y == 0:\n",
        "            continue\n",
        "        ax[x,y].imshow(img_ascs[x * plot_y + y - 1], cmap = 'gray')\n",
        "        ax[x,y].set_title('filter %d' % (x * plot_y + y - 1))\n",
        "\n",
        "vis_img_in_filter()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwcAAALyCAYAAACPcKhRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYjgAAAAAEGE4AAAAACBJ+j/5ok2/biDYhwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 864x864 with 8 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2q6fJ67qvlsY",
        "colab_type": "text"
      },
      "source": [
        "### We built a model with about 12000 parameters and achieved a max validation accuracy of 99.54 \n",
        "\n",
        "Using Batch Normalization and Dropouts helped in training faster and avoiding overfitting . \n",
        "\n",
        "Using these techniques also allows us to use higher learning rates than usual although we did not tweak the default learning rate in this exercise.\n",
        "\n",
        "Using Image Augmentation , Global Average pooling , scheduling learning rate adjustments , cyclic learning rate , etc we could build even more efficient models with better accuracy "
      ]
    }
  ]
}